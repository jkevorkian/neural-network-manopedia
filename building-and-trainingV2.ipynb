{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T23:58:47.730402Z",
     "start_time": "2024-10-20T23:58:47.640041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          DATA SHAPE DEFINITION           ##\n",
    "import os, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define the parameters for the data shape\n",
    "videos_per_handsign = 72  # Adjust if necessary\n",
    "frames_per_video = 8\n",
    "num_landmarks = 51\n",
    "num_coordinates = 3\n",
    "\n",
    "num_additional_samples = 2      #how many different samples of frames_per_video frames will be taken from each video (carefull to not add too many if vids are short)\n",
    "\n",
    "data_augmentation = True        #Add noise and transformation to generate, from each video extracted, a num_augmented_versions number of new ones (also keep between 1-4)\n",
    "num_augmented_versions = 2\n",
    "\n",
    "# Define the root directory containing handsign folders\n",
    "root_path = \"NomenclatedDataset\"\n",
    "\n",
    "def get_handsign_folders(root_path):\n",
    "    handsign_names = {}\n",
    "    handsign_video_counts = {}  # To store the video count for each handsign\n",
    "    handsign_count = 0\n",
    "    \n",
    "    # Walk through the root directory\n",
    "    for folder in os.listdir(root_path):\n",
    "        folder_path = os.path.join(root_path, folder)\n",
    "        \n",
    "        # Ignore non-directories\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        # If the folder starts with \"#\", check its subdirectories\n",
    "        if folder.startswith(\"#\"):\n",
    "            for subfolder in os.listdir(folder_path):\n",
    "                subfolder_path = os.path.join(folder_path, subfolder)\n",
    "                if os.path.isdir(subfolder_path):\n",
    "                    handsign_names[handsign_count] = subfolder\n",
    "                    # Count videos in the subfolder\n",
    "                    videos = [f for f in os.listdir(subfolder_path) if f.endswith(('.mp4', '.avi', '.MOV'))]\n",
    "                    handsign_video_counts[subfolder] = len(videos)\n",
    "                    handsign_count += 1\n",
    "        else:\n",
    "            # Directly add the folder as a handsign\n",
    "            handsign_names[handsign_count] = folder\n",
    "            # Count videos in the folder\n",
    "            videos = [f for f in os.listdir(folder_path) if f.endswith(('.mp4', '.avi', '.MOV'))]\n",
    "            handsign_video_counts[folder] = len(videos)\n",
    "            handsign_count += 1\n",
    "    \n",
    "    return handsign_names, handsign_video_counts\n",
    "\n",
    "# Get the handsign names and video counts\n",
    "handsign_names, handsign_video_counts = get_handsign_folders(root_path)\n",
    "num_handsigns = len(handsign_names)\n",
    "\n",
    "# Sort handsigns by the number of available videos in descending order\n",
    "sorted_handsign_video_counts = dict(sorted(handsign_video_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Output the handsign names, number of handsigns, and sorted video counts\n",
    "print(f\"Number of handsigns: {num_handsigns}\")\n",
    "print(\"Handsign names:\")\n",
    "print(json.dumps(handsign_names, indent=4))\n",
    "\n",
    "print(\"\\nVideo counts per handsign (sorted by number of videos):\")\n",
    "print(json.dumps(sorted_handsign_video_counts, indent=4))\n"
   ],
   "id": "823e51359a2141fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of handsigns: 29\n",
      "Handsign names:\n",
      "{\n",
      "    \"0\": \"A\",\n",
      "    \"1\": \"C\",\n",
      "    \"2\": \"G\",\n",
      "    \"3\": \"I\",\n",
      "    \"4\": \"J\",\n",
      "    \"5\": \"K\",\n",
      "    \"6\": \"L\",\n",
      "    \"7\": \"U\",\n",
      "    \"8\": \"V\",\n",
      "    \"9\": \"Y\",\n",
      "    \"10\": \"bien\",\n",
      "    \"11\": \"buen dia\",\n",
      "    \"12\": \"buenas noches\",\n",
      "    \"13\": \"buenas tardes\",\n",
      "    \"14\": \"chau\",\n",
      "    \"15\": \"como estas\",\n",
      "    \"16\": \"hola\",\n",
      "    \"17\": \"mal\",\n",
      "    \"18\": \"muy bien\",\n",
      "    \"19\": \"nos vemos\",\n",
      "    \"20\": \"apellido\",\n",
      "    \"21\": \"aprender\",\n",
      "    \"22\": \"argentina\",\n",
      "    \"23\": \"ayuda\",\n",
      "    \"24\": \"comprar\",\n",
      "    \"25\": \"cumplea\\u00f1os\",\n",
      "    \"26\": \"hijo\",\n",
      "    \"27\": \"mujer\",\n",
      "    \"28\": \"sordo\"\n",
      "}\n",
      "\n",
      "Video counts per handsign (sorted by number of videos):\n",
      "{\n",
      "    \"C\": 163,\n",
      "    \"cumplea\\u00f1os\": 143,\n",
      "    \"mujer\": 137,\n",
      "    \"apellido\": 136,\n",
      "    \"V\": 100,\n",
      "    \"I\": 97,\n",
      "    \"comprar\": 84,\n",
      "    \"aprender\": 83,\n",
      "    \"hijo\": 83,\n",
      "    \"argentina\": 81,\n",
      "    \"ayuda\": 74,\n",
      "    \"K\": 57,\n",
      "    \"sordo\": 47,\n",
      "    \"A\": 42,\n",
      "    \"G\": 42,\n",
      "    \"J\": 32,\n",
      "    \"L\": 32,\n",
      "    \"U\": 32,\n",
      "    \"Y\": 31,\n",
      "    \"como estas\": 30,\n",
      "    \"nos vemos\": 29,\n",
      "    \"chau\": 28,\n",
      "    \"buenas tardes\": 26,\n",
      "    \"buenas noches\": 25,\n",
      "    \"hola\": 24,\n",
      "    \"bien\": 22,\n",
      "    \"buen dia\": 19,\n",
      "    \"mal\": 17,\n",
      "    \"muy bien\": 11\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T14:08:40.928689Z",
     "start_time": "2024-10-18T14:08:39.942630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "#redefine videos per handsign to accomodate for additional samples to be taken before generating the dummy array (this is done so there's no shaping/dimensions size incompatibility issue)\n",
    "videos_per_handsign = videos_per_handsign * (1+num_additional_samples)\n",
    "\n",
    "# Generate dummy data\n",
    "data = [np.random.rand(videos_per_handsign, frames_per_video, num_landmarks, num_coordinates) for _ in range(num_handsigns)]\n",
    "\n",
    "# Convert the list to a numpy array with shape (num_handsigns, videos_per_handsign, frames_per_video, num_landmarks, num_coordinates)\n",
    "data_array = np.array(data)\n",
    "\n",
    "# Save the data array to a .npy file\n",
    "np.save('handsigns_data.npy', data_array)"
   ],
   "id": "fa59d6b8b575718a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T23:05:56.530599Z",
     "start_time": "2024-10-20T23:05:39.087271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          PROCESS VIDEO DATASET FUNC DEFINITIONS         ##\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_landmarks(hands_results, pose_results, last_handedness):\n",
    "    landmarks = []\n",
    "\n",
    "    try:\n",
    "        nose_landmark = pose_results.pose_landmarks.landmark[0]\n",
    "    except:\n",
    "        class nose_landmark:\n",
    "            x = 0\n",
    "            y = 0\n",
    "            z = 0\n",
    "        nose_landmark = nose_landmark()\n",
    "\n",
    "    left_hand_landmarks = [(0, 0, 0)] * 21\n",
    "    right_hand_landmarks = [(0, 0, 0)] * 21\n",
    "\n",
    "    if hands_results.multi_hand_landmarks and hands_results.multi_handedness:\n",
    "        for i, hand_landmarks in enumerate(hands_results.multi_hand_landmarks):\n",
    "            label = hands_results.multi_handedness[i].classification[0].label\n",
    "            if label == 'Left':\n",
    "                left_hand_landmarks = [(lm.x - nose_landmark.x, lm.y - nose_landmark.y, lm.z - nose_landmark.z) for lm in hand_landmarks.landmark]\n",
    "                last_handedness['Left'] = True\n",
    "            elif label == 'Right':\n",
    "                right_hand_landmarks = [(lm.x - nose_landmark.x, lm.y - nose_landmark.y, lm.z - nose_landmark.z) for lm in hand_landmarks.landmark]\n",
    "                last_handedness['Right'] = True\n",
    "\n",
    "    if not hands_results.multi_hand_landmarks or len(hands_results.multi_hand_landmarks) < 2:\n",
    "        if not last_handedness.get('Left'):\n",
    "            left_hand_landmarks = [(0, 0, 0)] * 21\n",
    "        if not last_handedness.get('Right'):\n",
    "            right_hand_landmarks = [(0, 0, 0)] * 21\n",
    "\n",
    "    landmarks.extend(left_hand_landmarks)\n",
    "    landmarks.extend(right_hand_landmarks)\n",
    "\n",
    "    selected_body_landmarks = [0, 11, 12, 13, 14, 15, 16, 23, 24]\n",
    "    if pose_results.pose_landmarks:\n",
    "        for idx in selected_body_landmarks:\n",
    "            lm = pose_results.pose_landmarks.landmark[idx]\n",
    "            landmarks.append((lm.x - nose_landmark.x, lm.y - nose_landmark.y, lm.z - nose_landmark.z))\n",
    "    else:\n",
    "        landmarks.extend([(0, 0, 0)] * 9)\n",
    "\n",
    "    return np.array(landmarks), last_handedness\n",
    "\n",
    "\n",
    "def process_frame(frame, hands, pose, last_handedness):\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    hands_results = hands.process(image)\n",
    "    pose_results = pose.process(image)\n",
    "\n",
    "    landmarks, last_handedness = extract_landmarks(hands_results, pose_results, last_handedness)\n",
    "\n",
    "    return landmarks, last_handedness\n",
    "\n",
    "\n",
    "def process_video_with_samples(video_path, frames_per_video, num_landmarks, num_coordinates, num_additional_samples=0):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if the video was opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video {video_path}.\")\n",
    "        return np.zeros((1 + num_additional_samples, frames_per_video, num_landmarks, num_coordinates))\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Handle case where the video contains zero frames\n",
    "    if total_frames == 0:\n",
    "        print(f\"Warning: Video {video_path} contains zero frames. Filling this video's samples with zeros.\")\n",
    "        return np.zeros((1 + num_additional_samples, frames_per_video, num_landmarks, num_coordinates))\n",
    "    \n",
    "    # Select evenly spaced frames\n",
    "    indices = np.linspace(0, total_frames - 1, frames_per_video, dtype=int)\n",
    "    if total_frames < frames_per_video:\n",
    "        print(f\"Warning: Video {video_path} has fewer frames ({total_frames}) than required ({frames_per_video}). Duplicating frames to match.\")\n",
    "\n",
    "    # Initialize frame sets and track used frames\n",
    "    frame_sets = [indices]\n",
    "    used_frames = set(indices)\n",
    "    \n",
    "    # Create additional samples\n",
    "    for i in range(1, num_additional_samples + 1):\n",
    "        available_frames = list(set(range(total_frames)) - used_frames)\n",
    "\n",
    "        if len(available_frames) >= frames_per_video:\n",
    "            # Select from available frames without duplication\n",
    "            additional_indices = np.linspace(0, len(available_frames) - 1, frames_per_video, dtype=int)\n",
    "            additional_indices = [available_frames[idx] for idx in additional_indices]\n",
    "        else:\n",
    "            print(f\"Warning: Not enough unique frames left for additional sample {i}. Duplicating frames.\")\n",
    "            additional_indices = np.linspace(0, total_frames - 1, frames_per_video, dtype=int)\n",
    "        \n",
    "        used_frames.update(additional_indices)\n",
    "        frame_sets.append(additional_indices)\n",
    "\n",
    "    frames_data = []\n",
    "    last_handedness = {'Left': False, 'Right': False}\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    # Initialize the MediaPipe solutions\n",
    "    with mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5) as hands, \\\n",
    "         mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "        frame_count = 0\n",
    "        frame_buffer = {}\n",
    "\n",
    "        # Read and store all frames once\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_buffer[frame_count] = frame  # Buffer the frame for later processing\n",
    "            frame_count += 1\n",
    "\n",
    "        # Process the buffered frames for each sample set\n",
    "        for frame_set in frame_sets:\n",
    "            for frame_idx in frame_set:\n",
    "                if frame_idx in frame_buffer:\n",
    "                    landmarks, last_handedness = process_frame(frame_buffer[frame_idx], hands, pose, last_handedness)\n",
    "                    frames_data.append(landmarks)\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    # Ensure the correct number of frames is collected, padding if necessary\n",
    "    required_frames = frames_per_video * (1 + num_additional_samples)\n",
    "    if len(frames_data) < required_frames:\n",
    "        if len(frames_data) > 0:\n",
    "            # Duplicate the last valid frame to fill the gap\n",
    "            frames_data.extend([frames_data[-1]] * (required_frames - len(frames_data)))\n",
    "        else:\n",
    "            # Fill with zeros if no frames were processed\n",
    "            frames_data.extend([np.zeros((num_landmarks, num_coordinates))] * required_frames)\n",
    "\n",
    "    # Reshape the data into the expected format\n",
    "    reshaped_data = np.array(frames_data).reshape((1 + num_additional_samples, frames_per_video, num_landmarks, num_coordinates))\n",
    "    \n",
    "    return reshaped_data\n",
    "\n",
    "\n",
    "def process_dataset_with_samples(root_path, handsign_names, frames_per_video, num_landmarks, num_coordinates, videos_per_handsign, num_additional_samples=0):\n",
    "    data = []\n",
    "\n",
    "    for handsign_index in tqdm(range(len(handsign_names)), desc=\"Processing handsigns\"):\n",
    "        handsign_folder = handsign_names[handsign_index]\n",
    "        handsign_path = os.path.join(root_path, handsign_folder)\n",
    "        \n",
    "        if not os.path.exists(handsign_path):\n",
    "            print(f\"Warning: Directory {handsign_path} does not exist. Skipping.\")\n",
    "            data.append(np.zeros((videos_per_handsign, frames_per_video, num_landmarks, num_coordinates)))\n",
    "            continue\n",
    "\n",
    "        videos = [f for f in os.listdir(handsign_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        handsign_data = []\n",
    "\n",
    "        # Calculate how many videos to process based on videos_per_handsign and num_additional_samples\n",
    "        max_videos_to_process = videos_per_handsign // (1 + num_additional_samples)\n",
    "        \n",
    "        # Process the required number of videos\n",
    "        for video in tqdm(videos[:max_videos_to_process], desc=f\"Processing videos for handsign {handsign_index}\", leave=False):\n",
    "            video_path = os.path.join(handsign_path, video)\n",
    "            \n",
    "            # Call the updated process_video_with_samples function\n",
    "            video_data = process_video_with_samples(video_path, frames_per_video, num_landmarks, num_coordinates, num_additional_samples)\n",
    "            \n",
    "            # Append all generated samples (original + additional) from the video\n",
    "            for sample in video_data:\n",
    "                handsign_data.append(sample)\n",
    "\n",
    "        handsign_data = np.array(handsign_data)\n",
    "        total_video_samples = handsign_data.shape[0]  # Total samples generated from the videos processed\n",
    "\n",
    "        # Ensure total number of video samples matches the predefined videos_per_handsign\n",
    "        if total_video_samples < videos_per_handsign:\n",
    "            padding_needed = videos_per_handsign - total_video_samples\n",
    "            print(f\"Warning: Handsign {handsign_index} ('{handsign_folder}') has only {total_video_samples} samples. \"\n",
    "                  f\"Padding with {padding_needed} empty samples.\")\n",
    "            handsign_data = np.pad(handsign_data, ((0, padding_needed), (0, 0), (0, 0), (0, 0)), mode='constant')\n",
    "        elif total_video_samples > videos_per_handsign:\n",
    "            handsign_data = handsign_data[:videos_per_handsign]\n",
    "            print(f\"Warning: More samples generated ({total_video_samples}) than expected ({videos_per_handsign}). Trimming excess samples.\")\n",
    "\n",
    "        data.append(handsign_data)\n",
    "\n",
    "    final_data = np.array(data)\n",
    "    print(f\"Final dataset shape: {final_data.shape}\")\n",
    "\n",
    "    return final_data\n",
    "\n",
    "\n"
   ],
   "id": "c6b1d3592b943838",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "##          PROCESS VIDEOS DATASET FUNC CALLING         ##\n",
    "data_array = process_dataset_with_samples(root_path, handsign_names, frames_per_video, num_landmarks, num_coordinates, videos_per_handsign, num_additional_samples)\n",
    "\n",
    "# Save the data array to a .npy file\n",
    "np.save('handsigns_data.npy', data_array)\n",
    "print(\"Data saved to handsigns_data.npy\")\n",
    "    "
   ],
   "id": "38b563f5e5342b66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T23:45:14.648202Z",
     "start_time": "2024-10-20T23:45:12.669079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##        DATA AUGMENTATION (OPTIONAL)        ##\n",
    "\n",
    "def apply_rotation(landmarks, angle_degrees_x, angle_degrees_y, angle_degrees_z):\n",
    "    \"\"\"Rotate the landmarks in 3D space by given angles around X, Y, and Z axes.\"\"\"\n",
    "    angle_radians_x = np.radians(angle_degrees_x)\n",
    "    angle_radians_y = np.radians(angle_degrees_y)\n",
    "    angle_radians_z = np.radians(angle_degrees_z)\n",
    "\n",
    "    # Rotation matrix around the X-axis\n",
    "    rotation_matrix_x = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(angle_radians_x), -np.sin(angle_radians_x)],\n",
    "        [0, np.sin(angle_radians_x), np.cos(angle_radians_x)]\n",
    "    ])\n",
    "\n",
    "    # Rotation matrix around the Y-axis\n",
    "    rotation_matrix_y = np.array([\n",
    "        [np.cos(angle_radians_y), 0, np.sin(angle_radians_y)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(angle_radians_y), 0, np.cos(angle_radians_y)]\n",
    "    ])\n",
    "\n",
    "    # Rotation matrix around the Z-axis\n",
    "    rotation_matrix_z = np.array([\n",
    "        [np.cos(angle_radians_z), -np.sin(angle_radians_z), 0],\n",
    "        [np.sin(angle_radians_z), np.cos(angle_radians_z), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    # Combine rotations by multiplying the matrices (Z * Y * X)\n",
    "    combined_rotation_matrix = np.dot(np.dot(rotation_matrix_z, rotation_matrix_y), rotation_matrix_x)\n",
    "\n",
    "    return np.dot(landmarks, combined_rotation_matrix)\n",
    "\n",
    "def apply_scaling(landmarks, scale_factor):\n",
    "    \"\"\"Scale the landmarks by a given factor.\"\"\"\n",
    "    return landmarks * scale_factor\n",
    "\n",
    "def apply_translation(landmarks, translation_vector):\n",
    "    \"\"\"Translate the landmarks by a given vector (x, y, z).\"\"\"\n",
    "    return landmarks + translation_vector\n",
    "\n",
    "def add_noise(landmarks, noise_level=0.001):\n",
    "    \"\"\"Add random noise to the landmarks.\"\"\"\n",
    "    noise = np.random.normal(0, noise_level, landmarks.shape)\n",
    "    return landmarks + noise\n",
    "\n",
    "def augment_data(data_array, num_augmented_versions=5):\n",
    "    \"\"\"\n",
    "    Augment the data array by applying transformations.\n",
    "    Creates `num_augmented_versions` augmented copies of each handsign video.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Augmentation parameters\n",
    "    noise_level = 0.005  # for the add_noise() function\n",
    "    translation_vector = np.random.uniform(-0.05, 0.05, 3)  # for the apply_translation() function\n",
    "    scale_factor = np.random.uniform(0.6, 1.6)  # for the apply_scaling() function\n",
    "    angle_degrees_x = np.random.uniform(-25, 25)  # for rotation around X-axis\n",
    "    angle_degrees_y = np.random.uniform(-25, 25)  # for rotation around Y-axis\n",
    "    angle_degrees_z = np.random.uniform(-25, 25)  # for rotation around Z-axis\n",
    "\n",
    "    augmented_data = []\n",
    "    for handsign_data in data_array:\n",
    "        augmented_handsign_data = []\n",
    "        for video_data in handsign_data:\n",
    "            augmented_videos = [video_data]  # Start with the original video data\n",
    "\n",
    "            for _ in range(num_augmented_versions):\n",
    "                augmented_video = []\n",
    "                for frame in video_data:\n",
    "                    # Apply a combination of augmentations\n",
    "                    rotated_frame = apply_rotation(frame, angle_degrees_x, angle_degrees_y, angle_degrees_z)\n",
    "                    scaled_frame = apply_scaling(rotated_frame, scale_factor)\n",
    "                    translated_frame = apply_translation(scaled_frame, translation_vector)\n",
    "                    noisy_frame = add_noise(translated_frame, noise_level)\n",
    "\n",
    "                    augmented_video.append(noisy_frame)\n",
    "                \n",
    "                augmented_videos.append(np.array(augmented_video))\n",
    "\n",
    "            # Flatten the augmented videos for each original video\n",
    "            augmented_handsign_data.extend(augmented_videos)\n",
    "\n",
    "        augmented_data.append(np.array(augmented_handsign_data))\n",
    "    \n",
    "    return np.array(augmented_data)\n",
    "\n",
    "# Load original handsigns data\n",
    "handsigns_data = np.load('handsigns_data.npy')\n",
    "\n",
    "# Apply augmentation\n",
    "if data_augmentation:\n",
    "    augmented_data = augment_data(handsigns_data, num_augmented_versions)\n",
    "    # Save the augmented data to a new .npy file\n",
    "    np.save('handsigns_data_augmented.npy', augmented_data)\n",
    "    # Update the videos per handsign value to match the videos generated by the augmentation\n",
    "    data_array = np.load('handsigns_data_augmented.npy')\n",
    "    videos_per_handsign = data_array.shape[1]\n",
    "    \n",
    "    print(\"Augmented data saved to handsigns_data_augmented.npy, videos_per_handsign augmented by \"+str(num_augmented_versions)+\" per existing video for a total of \"+str(videos_per_handsign)+\" videos per handsign\")\n",
    "else:\n",
    "    print(\"no data augmentation was performed\")"
   ],
   "id": "d0c17cfc27c8ccac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented data saved to handsigns_data_augmented.npy, videos_per_handsign augmented by 2 per existing video for a total of 648 videos per handsign\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T13:44:34.047614Z",
     "start_time": "2024-10-18T13:44:33.773696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          MODEL DEFINITION            ##\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Reshape, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Reshape input to (frames_per_video, num_landmarks * num_coordinates)\n",
    "new_input_shape = (frames_per_video, num_landmarks * num_coordinates)\n",
    "\n",
    "model = Sequential([\n",
    "    # Reshape layer\n",
    "    Reshape((frames_per_video, num_landmarks * num_coordinates), input_shape=(frames_per_video, num_landmarks, num_coordinates)),\n",
    "    \n",
    "    # LSTM layers with Dropout and Batch Normalization to reduce overfitting\n",
    "    LSTM(64, return_sequences=True, kernel_regularizer=l2(0.05)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.7),\n",
    "    \n",
    "    LSTM(128, return_sequences=False, kernel_regularizer=l2(0.05)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.7),\n",
    "    \n",
    "    # Fully connected layer\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.05)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.7),\n",
    "    \n",
    "    # Output layer for multi-class classification\n",
    "    Dense(num_handsigns, activation='softmax')  # Softmax for multi-class classification\n",
    "])\n",
    "\n",
    "# Specify a learning rate\n",
    "learning_rate = 0.0005\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#model.summary() # Uncomment if you want to see the model summary\n"
   ],
   "id": "c1f9d72562ce0728",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-18T13:44:40.400553Z",
     "start_time": "2024-10-18T13:44:38.773883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##        DATA PREPROCESSING FOR TRAINING            ##\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "# Load the data from the .npy file, making a copy to use for training as to not modify the original extracted data\n",
    "if data_augmentation:\n",
    "    shutil.copy('handsigns_data_augmented.npy', 'handsigns_data_training_copy.npy')\n",
    "    data_array = np.load('handsigns_data_training_copy.npy')\n",
    "    \n",
    "    # Update videos_per_handsign based on augmentation\n",
    "    #videos_per_handsign = data_array.shape[1]  # Dynamically update based on the new augmented shape\n",
    "    print(\"using handsigns_data_augmented.npy. After augmentation videos per handsign updated to: \" + str(data_array.shape[1]))\n",
    "else:\n",
    "    shutil.copy('handsigns_data.npy', 'handsigns_data_training_copy.npy')\n",
    "    data_array = np.load('handsigns_data_training_copy.npy')\n",
    "\n",
    "print('Array used shape: ',data_array.shape)\n",
    "# X remains unchanged\n",
    "X = data_array \n",
    "\n",
    "# Create labels for each handsign (0 to num_handsigns-1)\n",
    "# This creates a label for each hand sign, repeated for each video\n",
    "y = np.repeat(np.arange(num_handsigns), videos_per_handsign)\n",
    "y = y.reshape(num_handsigns, videos_per_handsign)\n",
    "\n",
    "# Initialize lists to hold training and validation data\n",
    "X_train_list = []\n",
    "X_val_list = []\n",
    "y_train_list = []\n",
    "y_val_list = []\n",
    "\n",
    "# Split videos and labels for each handsign\n",
    "for handsign_index in range(num_handsigns):\n",
    "    # Split the videos within each handsign\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(videos_per_handsign), \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Select training and validation data for this handsign\n",
    "    X_train_list.append(data_array[handsign_index, train_indices])\n",
    "    X_val_list.append(data_array[handsign_index, val_indices])\n",
    "    \n",
    "    # Select corresponding labels\n",
    "    y_train_list.append(y[handsign_index, train_indices])\n",
    "    y_val_list.append(y[handsign_index, val_indices])\n",
    "\n",
    "# Concatenate lists to form the final training and validation sets\n",
    "X_train = np.concatenate(X_train_list, axis=0)\n",
    "X_val = np.concatenate(X_val_list, axis=0)\n",
    "y_train = np.concatenate(y_train_list, axis=0)\n",
    "y_val = np.concatenate(y_val_list, axis=0)\n",
    "\n",
    "# Reshape X_train and X_val to fit the model's expected input shape\n",
    "X_train = X_train.reshape(-1, frames_per_video, num_landmarks, num_coordinates)\n",
    "X_val = X_val.reshape(-1, frames_per_video, num_landmarks, num_coordinates)\n",
    "\n",
    "# Flatten y_train and y_val\n",
    "y_train = y_train.flatten()\n",
    "y_val = y_val.flatten()"
   ],
   "id": "664f881d08e116f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using handsigns_data_augmented.npy. After augmentation videos per handsign updated to: 648\n",
      "Array used shape:  (10, 648, 8, 51, 3)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T13:53:25.708557Z",
     "start_time": "2024-10-18T13:44:42.616656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          MODEL TRAINING          ##\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Callback helpers for model training #\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "# Model checkpointing to save the best model during training\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)  \n",
    "checkpoint = ModelCheckpoint('best_handsigns_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)  \n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val), \n",
    "    callbacks=[early_stopping, checkpoint, reduce_lr]\n",
    ")\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model.save('handsigns_model.h5')\n",
    "\n",
    "# Optionally, save the training history\n",
    "import pickle\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "    \n",
    "\n",
    "    \n",
    "##          TRAINING HISTORY ANALYSIS           ##\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# If you want to save the plot instead of displaying it:\n",
    "# plt.savefig('training_history.png')"
   ],
   "id": "febb1178fa7e41a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001B[1m156/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.1382 - loss: 20.3259\n",
      "Epoch 1: val_loss improved from inf to 12.51744, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 13ms/step - accuracy: 0.1400 - loss: 20.2042 - val_accuracy: 0.1869 - val_loss: 12.5174 - learning_rate: 5.0000e-04\n",
      "Epoch 2/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.3009 - loss: 11.5788\n",
      "Epoch 2: val_loss improved from 12.51744 to 8.08181, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.3013 - loss: 11.5578 - val_accuracy: 0.4523 - val_loss: 8.0818 - learning_rate: 5.0000e-04\n",
      "Epoch 3/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.3740 - loss: 7.8086\n",
      "Epoch 3: val_loss improved from 8.08181 to 5.61952, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - accuracy: 0.3744 - loss: 7.7936 - val_accuracy: 0.6008 - val_loss: 5.6195 - learning_rate: 5.0000e-04\n",
      "Epoch 4/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.4488 - loss: 5.6174\n",
      "Epoch 4: val_loss improved from 5.61952 to 3.98745, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.4493 - loss: 5.6098 - val_accuracy: 0.7477 - val_loss: 3.9875 - learning_rate: 5.0000e-04\n",
      "Epoch 5/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.5155 - loss: 4.2211\n",
      "Epoch 5: val_loss improved from 3.98745 to 2.98220, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - accuracy: 0.5165 - loss: 4.2117 - val_accuracy: 0.8077 - val_loss: 2.9822 - learning_rate: 5.0000e-04\n",
      "Epoch 6/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.5982 - loss: 3.2283\n",
      "Epoch 6: val_loss improved from 2.98220 to 2.32025, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.5983 - loss: 3.2260 - val_accuracy: 0.8108 - val_loss: 2.3203 - learning_rate: 5.0000e-04\n",
      "Epoch 7/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.6476 - loss: 2.6015\n",
      "Epoch 7: val_loss improved from 2.32025 to 1.74886, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 11ms/step - accuracy: 0.6476 - loss: 2.5999 - val_accuracy: 0.8485 - val_loss: 1.7489 - learning_rate: 5.0000e-04\n",
      "Epoch 8/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.6942 - loss: 2.0863 \n",
      "Epoch 8: val_loss improved from 1.74886 to 1.50058, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 11ms/step - accuracy: 0.6943 - loss: 2.0853 - val_accuracy: 0.8462 - val_loss: 1.5006 - learning_rate: 5.0000e-04\n",
      "Epoch 9/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.7232 - loss: 1.7742\n",
      "Epoch 9: val_loss improved from 1.50058 to 1.19801, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.7231 - loss: 1.7739 - val_accuracy: 0.8762 - val_loss: 1.1980 - learning_rate: 5.0000e-04\n",
      "Epoch 10/500\n",
      "\u001B[1m157/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.7281 - loss: 1.5668\n",
      "Epoch 10: val_loss improved from 1.19801 to 1.07970, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.7287 - loss: 1.5642 - val_accuracy: 0.8800 - val_loss: 1.0797 - learning_rate: 5.0000e-04\n",
      "Epoch 11/500\n",
      "\u001B[1m157/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.7570 - loss: 1.3901\n",
      "Epoch 11: val_loss did not improve from 1.07970\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.7569 - loss: 1.3886 - val_accuracy: 0.8362 - val_loss: 1.0975 - learning_rate: 5.0000e-04\n",
      "Epoch 12/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.7684 - loss: 1.2479\n",
      "Epoch 12: val_loss improved from 1.07970 to 0.79950, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.7684 - loss: 1.2474 - val_accuracy: 0.9085 - val_loss: 0.7995 - learning_rate: 5.0000e-04\n",
      "Epoch 13/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.7685 - loss: 1.2041\n",
      "Epoch 13: val_loss improved from 0.79950 to 0.76074, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.7685 - loss: 1.2039 - val_accuracy: 0.9108 - val_loss: 0.7607 - learning_rate: 5.0000e-04\n",
      "Epoch 14/500\n",
      "\u001B[1m157/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.7930 - loss: 1.0893\n",
      "Epoch 14: val_loss improved from 0.76074 to 0.71715, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.7929 - loss: 1.0899 - val_accuracy: 0.9085 - val_loss: 0.7172 - learning_rate: 5.0000e-04\n",
      "Epoch 15/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8053 - loss: 1.0440\n",
      "Epoch 15: val_loss did not improve from 0.71715\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.8052 - loss: 1.0442 - val_accuracy: 0.8877 - val_loss: 0.7726 - learning_rate: 5.0000e-04\n",
      "Epoch 16/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8019 - loss: 1.0049\n",
      "Epoch 16: val_loss did not improve from 0.71715\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.8018 - loss: 1.0052 - val_accuracy: 0.8862 - val_loss: 0.7239 - learning_rate: 5.0000e-04\n",
      "Epoch 17/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.8165 - loss: 0.9675\n",
      "Epoch 17: val_loss improved from 0.71715 to 0.61197, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.8165 - loss: 0.9675 - val_accuracy: 0.9231 - val_loss: 0.6120 - learning_rate: 5.0000e-04\n",
      "Epoch 18/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.8127 - loss: 0.9223\n",
      "Epoch 18: val_loss did not improve from 0.61197\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 11ms/step - accuracy: 0.8127 - loss: 0.9223 - val_accuracy: 0.8877 - val_loss: 0.6997 - learning_rate: 5.0000e-04\n",
      "Epoch 19/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8275 - loss: 0.9054\n",
      "Epoch 19: val_loss did not improve from 0.61197\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.8275 - loss: 0.9054 - val_accuracy: 0.8038 - val_loss: 0.9122 - learning_rate: 5.0000e-04\n",
      "Epoch 20/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8285 - loss: 0.9115\n",
      "Epoch 20: val_loss did not improve from 0.61197\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.8285 - loss: 0.9117 - val_accuracy: 0.9108 - val_loss: 0.6505 - learning_rate: 5.0000e-04\n",
      "Epoch 21/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.8283 - loss: 0.8967\n",
      "Epoch 21: val_loss improved from 0.61197 to 0.57113, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.8283 - loss: 0.8965 - val_accuracy: 0.9131 - val_loss: 0.5711 - learning_rate: 5.0000e-04\n",
      "Epoch 22/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.8371 - loss: 0.8611\n",
      "Epoch 22: val_loss did not improve from 0.57113\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.8368 - loss: 0.8621 - val_accuracy: 0.8946 - val_loss: 0.6604 - learning_rate: 5.0000e-04\n",
      "Epoch 23/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8329 - loss: 0.8757\n",
      "Epoch 23: val_loss did not improve from 0.57113\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.8330 - loss: 0.8756 - val_accuracy: 0.8569 - val_loss: 0.7304 - learning_rate: 5.0000e-04\n",
      "Epoch 24/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8381 - loss: 0.8761\n",
      "Epoch 24: val_loss improved from 0.57113 to 0.54518, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.8381 - loss: 0.8760 - val_accuracy: 0.9200 - val_loss: 0.5452 - learning_rate: 5.0000e-04\n",
      "Epoch 25/500\n",
      "\u001B[1m156/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.8642 - loss: 0.7852\n",
      "Epoch 25: val_loss did not improve from 0.54518\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.8638 - loss: 0.7860 - val_accuracy: 0.9185 - val_loss: 0.5484 - learning_rate: 5.0000e-04\n",
      "Epoch 26/500\n",
      "\u001B[1m156/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8574 - loss: 0.7723\n",
      "Epoch 26: val_loss did not improve from 0.54518\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.8571 - loss: 0.7734 - val_accuracy: 0.9223 - val_loss: 0.5495 - learning_rate: 5.0000e-04\n",
      "Epoch 27/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.8505 - loss: 0.7845\n",
      "Epoch 27: val_loss improved from 0.54518 to 0.46197, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.8506 - loss: 0.7843 - val_accuracy: 0.9515 - val_loss: 0.4620 - learning_rate: 5.0000e-04\n",
      "Epoch 28/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.8719 - loss: 0.7271\n",
      "Epoch 28: val_loss did not improve from 0.46197\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.8719 - loss: 0.7273 - val_accuracy: 0.9446 - val_loss: 0.4681 - learning_rate: 5.0000e-04\n",
      "Epoch 29/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8697 - loss: 0.7213\n",
      "Epoch 29: val_loss did not improve from 0.46197\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.8696 - loss: 0.7218 - val_accuracy: 0.9046 - val_loss: 0.5897 - learning_rate: 5.0000e-04\n",
      "Epoch 30/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8716 - loss: 0.7225\n",
      "Epoch 30: val_loss did not improve from 0.46197\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.8711 - loss: 0.7239 - val_accuracy: 0.9508 - val_loss: 0.4828 - learning_rate: 5.0000e-04\n",
      "Epoch 31/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.8700 - loss: 0.7262\n",
      "Epoch 31: val_loss did not improve from 0.46197\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.8700 - loss: 0.7264 - val_accuracy: 0.9208 - val_loss: 0.5387 - learning_rate: 5.0000e-04\n",
      "Epoch 32/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.8716 - loss: 0.7032\n",
      "Epoch 32: val_loss did not improve from 0.46197\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.8715 - loss: 0.7036 - val_accuracy: 0.9423 - val_loss: 0.4867 - learning_rate: 5.0000e-04\n",
      "Epoch 33/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8958 - loss: 0.6565\n",
      "Epoch 33: val_loss improved from 0.46197 to 0.39054, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.8958 - loss: 0.6563 - val_accuracy: 0.9577 - val_loss: 0.3905 - learning_rate: 2.5000e-04\n",
      "Epoch 34/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8999 - loss: 0.6187\n",
      "Epoch 34: val_loss did not improve from 0.39054\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.8999 - loss: 0.6186 - val_accuracy: 0.9538 - val_loss: 0.3908 - learning_rate: 2.5000e-04\n",
      "Epoch 35/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8930 - loss: 0.5880\n",
      "Epoch 35: val_loss did not improve from 0.39054\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.8932 - loss: 0.5879 - val_accuracy: 0.9477 - val_loss: 0.4071 - learning_rate: 2.5000e-04\n",
      "Epoch 36/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9001 - loss: 0.5755\n",
      "Epoch 36: val_loss improved from 0.39054 to 0.34617, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9002 - loss: 0.5752 - val_accuracy: 0.9708 - val_loss: 0.3462 - learning_rate: 2.5000e-04\n",
      "Epoch 37/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9001 - loss: 0.5794\n",
      "Epoch 37: val_loss improved from 0.34617 to 0.33310, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 12ms/step - accuracy: 0.9001 - loss: 0.5792 - val_accuracy: 0.9662 - val_loss: 0.3331 - learning_rate: 2.5000e-04\n",
      "Epoch 38/500\n",
      "\u001B[1m157/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9084 - loss: 0.5460\n",
      "Epoch 38: val_loss improved from 0.33310 to 0.33121, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9085 - loss: 0.5454 - val_accuracy: 0.9638 - val_loss: 0.3312 - learning_rate: 2.5000e-04\n",
      "Epoch 39/500\n",
      "\u001B[1m156/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.8989 - loss: 0.5537\n",
      "Epoch 39: val_loss did not improve from 0.33121\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.8990 - loss: 0.5532 - val_accuracy: 0.9385 - val_loss: 0.4140 - learning_rate: 2.5000e-04\n",
      "Epoch 40/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9021 - loss: 0.5462\n",
      "Epoch 40: val_loss did not improve from 0.33121\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9023 - loss: 0.5457 - val_accuracy: 0.9323 - val_loss: 0.4072 - learning_rate: 2.5000e-04\n",
      "Epoch 41/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8987 - loss: 0.5645\n",
      "Epoch 41: val_loss did not improve from 0.33121\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.8989 - loss: 0.5638 - val_accuracy: 0.9562 - val_loss: 0.3486 - learning_rate: 2.5000e-04\n",
      "Epoch 42/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9173 - loss: 0.5193\n",
      "Epoch 42: val_loss improved from 0.33121 to 0.30262, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9172 - loss: 0.5193 - val_accuracy: 0.9715 - val_loss: 0.3026 - learning_rate: 2.5000e-04\n",
      "Epoch 43/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9002 - loss: 0.5247\n",
      "Epoch 43: val_loss did not improve from 0.30262\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9002 - loss: 0.5246 - val_accuracy: 0.9446 - val_loss: 0.3612 - learning_rate: 2.5000e-04\n",
      "Epoch 44/500\n",
      "\u001B[1m157/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9091 - loss: 0.5034\n",
      "Epoch 44: val_loss improved from 0.30262 to 0.30027, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9089 - loss: 0.5040 - val_accuracy: 0.9662 - val_loss: 0.3003 - learning_rate: 2.5000e-04\n",
      "Epoch 45/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9086 - loss: 0.4936\n",
      "Epoch 45: val_loss did not improve from 0.30027\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9086 - loss: 0.4937 - val_accuracy: 0.9500 - val_loss: 0.3357 - learning_rate: 2.5000e-04\n",
      "Epoch 46/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9069 - loss: 0.5245\n",
      "Epoch 46: val_loss did not improve from 0.30027\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9069 - loss: 0.5245 - val_accuracy: 0.9623 - val_loss: 0.3232 - learning_rate: 2.5000e-04\n",
      "Epoch 47/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9104 - loss: 0.4997\n",
      "Epoch 47: val_loss did not improve from 0.30027\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9103 - loss: 0.5002 - val_accuracy: 0.9638 - val_loss: 0.3138 - learning_rate: 2.5000e-04\n",
      "Epoch 48/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9182 - loss: 0.4965\n",
      "Epoch 48: val_loss improved from 0.30027 to 0.28273, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9182 - loss: 0.4964 - val_accuracy: 0.9738 - val_loss: 0.2827 - learning_rate: 2.5000e-04\n",
      "Epoch 49/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9213 - loss: 0.4792\n",
      "Epoch 49: val_loss did not improve from 0.28273\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - accuracy: 0.9213 - loss: 0.4792 - val_accuracy: 0.9485 - val_loss: 0.3358 - learning_rate: 2.5000e-04\n",
      "Epoch 50/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9022 - loss: 0.5416\n",
      "Epoch 50: val_loss did not improve from 0.28273\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9022 - loss: 0.5415 - val_accuracy: 0.9692 - val_loss: 0.2995 - learning_rate: 2.5000e-04\n",
      "Epoch 51/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9167 - loss: 0.4763\n",
      "Epoch 51: val_loss did not improve from 0.28273\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9167 - loss: 0.4765 - val_accuracy: 0.9715 - val_loss: 0.2902 - learning_rate: 2.5000e-04\n",
      "Epoch 52/500\n",
      "\u001B[1m156/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9178 - loss: 0.4749\n",
      "Epoch 52: val_loss did not improve from 0.28273\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9176 - loss: 0.4752 - val_accuracy: 0.9577 - val_loss: 0.3346 - learning_rate: 2.5000e-04\n",
      "Epoch 53/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9208 - loss: 0.4749\n",
      "Epoch 53: val_loss did not improve from 0.28273\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9208 - loss: 0.4747 - val_accuracy: 0.9569 - val_loss: 0.3205 - learning_rate: 2.5000e-04\n",
      "Epoch 54/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9239 - loss: 0.4650\n",
      "Epoch 54: val_loss improved from 0.28273 to 0.27311, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9239 - loss: 0.4647 - val_accuracy: 0.9677 - val_loss: 0.2731 - learning_rate: 1.2500e-04\n",
      "Epoch 55/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9353 - loss: 0.4056\n",
      "Epoch 55: val_loss did not improve from 0.27311\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - accuracy: 0.9353 - loss: 0.4057 - val_accuracy: 0.9623 - val_loss: 0.2864 - learning_rate: 1.2500e-04\n",
      "Epoch 56/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9342 - loss: 0.3978\n",
      "Epoch 56: val_loss improved from 0.27311 to 0.24693, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9342 - loss: 0.3978 - val_accuracy: 0.9785 - val_loss: 0.2469 - learning_rate: 1.2500e-04\n",
      "Epoch 57/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9342 - loss: 0.3997\n",
      "Epoch 57: val_loss did not improve from 0.24693\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9342 - loss: 0.3997 - val_accuracy: 0.9662 - val_loss: 0.2571 - learning_rate: 1.2500e-04\n",
      "Epoch 58/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9425 - loss: 0.3714\n",
      "Epoch 58: val_loss improved from 0.24693 to 0.24493, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9425 - loss: 0.3715 - val_accuracy: 0.9723 - val_loss: 0.2449 - learning_rate: 1.2500e-04\n",
      "Epoch 59/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9437 - loss: 0.3791\n",
      "Epoch 59: val_loss improved from 0.24493 to 0.23366, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9436 - loss: 0.3794 - val_accuracy: 0.9785 - val_loss: 0.2337 - learning_rate: 1.2500e-04\n",
      "Epoch 60/500\n",
      "\u001B[1m157/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9433 - loss: 0.3666\n",
      "Epoch 60: val_loss did not improve from 0.23366\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9432 - loss: 0.3669 - val_accuracy: 0.9777 - val_loss: 0.2377 - learning_rate: 1.2500e-04\n",
      "Epoch 61/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9420 - loss: 0.3694\n",
      "Epoch 61: val_loss did not improve from 0.23366\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9419 - loss: 0.3695 - val_accuracy: 0.9708 - val_loss: 0.2514 - learning_rate: 1.2500e-04\n",
      "Epoch 62/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9389 - loss: 0.3658\n",
      "Epoch 62: val_loss did not improve from 0.23366\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9388 - loss: 0.3658 - val_accuracy: 0.9700 - val_loss: 0.2444 - learning_rate: 1.2500e-04\n",
      "Epoch 63/500\n",
      "\u001B[1m157/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9402 - loss: 0.3705\n",
      "Epoch 63: val_loss did not improve from 0.23366\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9402 - loss: 0.3704 - val_accuracy: 0.9715 - val_loss: 0.2369 - learning_rate: 1.2500e-04\n",
      "Epoch 64/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9394 - loss: 0.3692\n",
      "Epoch 64: val_loss improved from 0.23366 to 0.22171, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9394 - loss: 0.3694 - val_accuracy: 0.9785 - val_loss: 0.2217 - learning_rate: 1.2500e-04\n",
      "Epoch 65/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9431 - loss: 0.3614\n",
      "Epoch 65: val_loss did not improve from 0.22171\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9432 - loss: 0.3613 - val_accuracy: 0.9777 - val_loss: 0.2225 - learning_rate: 1.2500e-04\n",
      "Epoch 66/500\n",
      "\u001B[1m157/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9297 - loss: 0.3738\n",
      "Epoch 66: val_loss did not improve from 0.22171\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9296 - loss: 0.3743 - val_accuracy: 0.9692 - val_loss: 0.2366 - learning_rate: 1.2500e-04\n",
      "Epoch 67/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9363 - loss: 0.3559\n",
      "Epoch 67: val_loss did not improve from 0.22171\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9362 - loss: 0.3563 - val_accuracy: 0.9715 - val_loss: 0.2344 - learning_rate: 1.2500e-04\n",
      "Epoch 68/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9384 - loss: 0.3680\n",
      "Epoch 68: val_loss improved from 0.22171 to 0.22138, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9384 - loss: 0.3679 - val_accuracy: 0.9777 - val_loss: 0.2214 - learning_rate: 1.2500e-04\n",
      "Epoch 69/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9309 - loss: 0.3880\n",
      "Epoch 69: val_loss did not improve from 0.22138\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9310 - loss: 0.3875 - val_accuracy: 0.9777 - val_loss: 0.2236 - learning_rate: 1.2500e-04\n",
      "Epoch 70/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9493 - loss: 0.3403\n",
      "Epoch 70: val_loss improved from 0.22138 to 0.22018, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9493 - loss: 0.3404 - val_accuracy: 0.9808 - val_loss: 0.2202 - learning_rate: 1.2500e-04\n",
      "Epoch 71/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9434 - loss: 0.3494\n",
      "Epoch 71: val_loss did not improve from 0.22018\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9434 - loss: 0.3495 - val_accuracy: 0.9700 - val_loss: 0.2361 - learning_rate: 1.2500e-04\n",
      "Epoch 72/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9267 - loss: 0.3941\n",
      "Epoch 72: val_loss improved from 0.22018 to 0.21239, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9268 - loss: 0.3938 - val_accuracy: 0.9808 - val_loss: 0.2124 - learning_rate: 1.2500e-04\n",
      "Epoch 73/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9367 - loss: 0.3503\n",
      "Epoch 73: val_loss did not improve from 0.21239\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9367 - loss: 0.3504 - val_accuracy: 0.9777 - val_loss: 0.2216 - learning_rate: 1.2500e-04\n",
      "Epoch 74/500\n",
      "\u001B[1m156/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9336 - loss: 0.3729\n",
      "Epoch 74: val_loss did not improve from 0.21239\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9337 - loss: 0.3723 - val_accuracy: 0.9769 - val_loss: 0.2293 - learning_rate: 1.2500e-04\n",
      "Epoch 75/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9412 - loss: 0.3611\n",
      "Epoch 75: val_loss did not improve from 0.21239\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9412 - loss: 0.3611 - val_accuracy: 0.9746 - val_loss: 0.2187 - learning_rate: 1.2500e-04\n",
      "Epoch 76/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9419 - loss: 0.3465\n",
      "Epoch 76: val_loss did not improve from 0.21239\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9419 - loss: 0.3467 - val_accuracy: 0.9738 - val_loss: 0.2158 - learning_rate: 1.2500e-04\n",
      "Epoch 77/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9404 - loss: 0.3473\n",
      "Epoch 77: val_loss improved from 0.21239 to 0.20959, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9404 - loss: 0.3474 - val_accuracy: 0.9823 - val_loss: 0.2096 - learning_rate: 1.2500e-04\n",
      "Epoch 78/500\n",
      "\u001B[1m157/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9437 - loss: 0.3637\n",
      "Epoch 78: val_loss did not improve from 0.20959\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9439 - loss: 0.3634 - val_accuracy: 0.9800 - val_loss: 0.2115 - learning_rate: 1.2500e-04\n",
      "Epoch 79/500\n",
      "\u001B[1m157/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9330 - loss: 0.3598\n",
      "Epoch 79: val_loss did not improve from 0.20959\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9332 - loss: 0.3592 - val_accuracy: 0.9723 - val_loss: 0.2251 - learning_rate: 1.2500e-04\n",
      "Epoch 80/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9438 - loss: 0.3319\n",
      "Epoch 80: val_loss improved from 0.20959 to 0.20857, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9437 - loss: 0.3325 - val_accuracy: 0.9808 - val_loss: 0.2086 - learning_rate: 1.2500e-04\n",
      "Epoch 81/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9356 - loss: 0.3469\n",
      "Epoch 81: val_loss did not improve from 0.20857\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - accuracy: 0.9356 - loss: 0.3470 - val_accuracy: 0.9554 - val_loss: 0.2733 - learning_rate: 1.2500e-04\n",
      "Epoch 82/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9377 - loss: 0.3524\n",
      "Epoch 82: val_loss did not improve from 0.20857\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9377 - loss: 0.3523 - val_accuracy: 0.9754 - val_loss: 0.2116 - learning_rate: 1.2500e-04\n",
      "Epoch 83/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9484 - loss: 0.3198\n",
      "Epoch 83: val_loss did not improve from 0.20857\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 11ms/step - accuracy: 0.9484 - loss: 0.3199 - val_accuracy: 0.9769 - val_loss: 0.2161 - learning_rate: 1.2500e-04\n",
      "Epoch 84/500\n",
      "\u001B[1m156/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9517 - loss: 0.3133\n",
      "Epoch 84: val_loss did not improve from 0.20857\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9515 - loss: 0.3142 - val_accuracy: 0.9746 - val_loss: 0.2122 - learning_rate: 1.2500e-04\n",
      "Epoch 85/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9436 - loss: 0.3227\n",
      "Epoch 85: val_loss did not improve from 0.20857\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9437 - loss: 0.3227 - val_accuracy: 0.9800 - val_loss: 0.2109 - learning_rate: 1.2500e-04\n",
      "Epoch 86/500\n",
      "\u001B[1m156/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9490 - loss: 0.3032\n",
      "Epoch 86: val_loss improved from 0.20857 to 0.19533, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9492 - loss: 0.3029 - val_accuracy: 0.9800 - val_loss: 0.1953 - learning_rate: 6.2500e-05\n",
      "Epoch 87/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9489 - loss: 0.3062\n",
      "Epoch 87: val_loss did not improve from 0.19533\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9489 - loss: 0.3062 - val_accuracy: 0.9762 - val_loss: 0.1988 - learning_rate: 6.2500e-05\n",
      "Epoch 88/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9513 - loss: 0.3089\n",
      "Epoch 88: val_loss improved from 0.19533 to 0.18732, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9514 - loss: 0.3087 - val_accuracy: 0.9862 - val_loss: 0.1873 - learning_rate: 6.2500e-05\n",
      "Epoch 89/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9508 - loss: 0.2945\n",
      "Epoch 89: val_loss did not improve from 0.18732\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9508 - loss: 0.2946 - val_accuracy: 0.9831 - val_loss: 0.1889 - learning_rate: 6.2500e-05\n",
      "Epoch 90/500\n",
      "\u001B[1m156/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9527 - loss: 0.3014\n",
      "Epoch 90: val_loss improved from 0.18732 to 0.18519, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9527 - loss: 0.3013 - val_accuracy: 0.9854 - val_loss: 0.1852 - learning_rate: 6.2500e-05\n",
      "Epoch 91/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9478 - loss: 0.2987\n",
      "Epoch 91: val_loss did not improve from 0.18519\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9479 - loss: 0.2986 - val_accuracy: 0.9800 - val_loss: 0.1916 - learning_rate: 6.2500e-05\n",
      "Epoch 92/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9590 - loss: 0.2829\n",
      "Epoch 92: val_loss improved from 0.18519 to 0.17779, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9589 - loss: 0.2830 - val_accuracy: 0.9877 - val_loss: 0.1778 - learning_rate: 6.2500e-05\n",
      "Epoch 93/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9577 - loss: 0.2785\n",
      "Epoch 93: val_loss did not improve from 0.17779\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9577 - loss: 0.2785 - val_accuracy: 0.9846 - val_loss: 0.1799 - learning_rate: 6.2500e-05\n",
      "Epoch 94/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9535 - loss: 0.2776\n",
      "Epoch 94: val_loss did not improve from 0.17779\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9536 - loss: 0.2777 - val_accuracy: 0.9854 - val_loss: 0.1818 - learning_rate: 6.2500e-05\n",
      "Epoch 95/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9570 - loss: 0.2757\n",
      "Epoch 95: val_loss did not improve from 0.17779\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9570 - loss: 0.2758 - val_accuracy: 0.9831 - val_loss: 0.1842 - learning_rate: 6.2500e-05\n",
      "Epoch 96/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9562 - loss: 0.2824\n",
      "Epoch 96: val_loss improved from 0.17779 to 0.17726, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9562 - loss: 0.2824 - val_accuracy: 0.9892 - val_loss: 0.1773 - learning_rate: 6.2500e-05\n",
      "Epoch 97/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9528 - loss: 0.2920\n",
      "Epoch 97: val_loss did not improve from 0.17726\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9529 - loss: 0.2918 - val_accuracy: 0.9854 - val_loss: 0.1807 - learning_rate: 6.2500e-05\n",
      "Epoch 98/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9503 - loss: 0.2890\n",
      "Epoch 98: val_loss improved from 0.17726 to 0.17486, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9503 - loss: 0.2889 - val_accuracy: 0.9862 - val_loss: 0.1749 - learning_rate: 6.2500e-05\n",
      "Epoch 99/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9534 - loss: 0.2939\n",
      "Epoch 99: val_loss did not improve from 0.17486\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 12ms/step - accuracy: 0.9534 - loss: 0.2938 - val_accuracy: 0.9823 - val_loss: 0.1866 - learning_rate: 6.2500e-05\n",
      "Epoch 100/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9531 - loss: 0.2929\n",
      "Epoch 100: val_loss did not improve from 0.17486\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9531 - loss: 0.2928 - val_accuracy: 0.9731 - val_loss: 0.1891 - learning_rate: 6.2500e-05\n",
      "Epoch 101/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9519 - loss: 0.2885\n",
      "Epoch 101: val_loss did not improve from 0.17486\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9519 - loss: 0.2885 - val_accuracy: 0.9785 - val_loss: 0.1820 - learning_rate: 6.2500e-05\n",
      "Epoch 102/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9539 - loss: 0.2785\n",
      "Epoch 102: val_loss did not improve from 0.17486\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9539 - loss: 0.2788 - val_accuracy: 0.9815 - val_loss: 0.1767 - learning_rate: 6.2500e-05\n",
      "Epoch 103/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9557 - loss: 0.2765\n",
      "Epoch 103: val_loss improved from 0.17486 to 0.17431, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - accuracy: 0.9557 - loss: 0.2765 - val_accuracy: 0.9862 - val_loss: 0.1743 - learning_rate: 6.2500e-05\n",
      "Epoch 104/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9576 - loss: 0.2673\n",
      "Epoch 104: val_loss improved from 0.17431 to 0.16824, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9575 - loss: 0.2673 - val_accuracy: 0.9885 - val_loss: 0.1682 - learning_rate: 6.2500e-05\n",
      "Epoch 105/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9598 - loss: 0.2753\n",
      "Epoch 105: val_loss improved from 0.16824 to 0.16746, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9598 - loss: 0.2753 - val_accuracy: 0.9862 - val_loss: 0.1675 - learning_rate: 6.2500e-05\n",
      "Epoch 106/500\n",
      "\u001B[1m157/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9565 - loss: 0.2669\n",
      "Epoch 106: val_loss did not improve from 0.16746\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9564 - loss: 0.2673 - val_accuracy: 0.9762 - val_loss: 0.1880 - learning_rate: 6.2500e-05\n",
      "Epoch 107/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9555 - loss: 0.2708\n",
      "Epoch 107: val_loss did not improve from 0.16746\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9554 - loss: 0.2707 - val_accuracy: 0.9823 - val_loss: 0.1764 - learning_rate: 6.2500e-05\n",
      "Epoch 108/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9514 - loss: 0.2746\n",
      "Epoch 108: val_loss did not improve from 0.16746\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9514 - loss: 0.2746 - val_accuracy: 0.9777 - val_loss: 0.1824 - learning_rate: 6.2500e-05\n",
      "Epoch 109/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9517 - loss: 0.2844\n",
      "Epoch 109: val_loss did not improve from 0.16746\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9519 - loss: 0.2841 - val_accuracy: 0.9869 - val_loss: 0.1682 - learning_rate: 6.2500e-05\n",
      "Epoch 110/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9590 - loss: 0.2633\n",
      "Epoch 110: val_loss did not improve from 0.16746\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9589 - loss: 0.2633 - val_accuracy: 0.9854 - val_loss: 0.1706 - learning_rate: 6.2500e-05\n",
      "Epoch 111/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9566 - loss: 0.2678\n",
      "Epoch 111: val_loss did not improve from 0.16746\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9566 - loss: 0.2678 - val_accuracy: 0.9854 - val_loss: 0.1686 - learning_rate: 3.1250e-05\n",
      "Epoch 112/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9602 - loss: 0.2582\n",
      "Epoch 112: val_loss improved from 0.16746 to 0.16399, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9602 - loss: 0.2582 - val_accuracy: 0.9877 - val_loss: 0.1640 - learning_rate: 3.1250e-05\n",
      "Epoch 113/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9547 - loss: 0.2713\n",
      "Epoch 113: val_loss improved from 0.16399 to 0.16248, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9549 - loss: 0.2710 - val_accuracy: 0.9877 - val_loss: 0.1625 - learning_rate: 3.1250e-05\n",
      "Epoch 114/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9613 - loss: 0.2573\n",
      "Epoch 114: val_loss improved from 0.16248 to 0.15939, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9613 - loss: 0.2573 - val_accuracy: 0.9877 - val_loss: 0.1594 - learning_rate: 3.1250e-05\n",
      "Epoch 115/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9612 - loss: 0.2448\n",
      "Epoch 115: val_loss did not improve from 0.15939\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9612 - loss: 0.2449 - val_accuracy: 0.9854 - val_loss: 0.1630 - learning_rate: 3.1250e-05\n",
      "Epoch 116/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9576 - loss: 0.2434\n",
      "Epoch 116: val_loss improved from 0.15939 to 0.15925, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9575 - loss: 0.2434 - val_accuracy: 0.9877 - val_loss: 0.1593 - learning_rate: 3.1250e-05\n",
      "Epoch 117/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9592 - loss: 0.2505\n",
      "Epoch 117: val_loss improved from 0.15925 to 0.15777, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9592 - loss: 0.2507 - val_accuracy: 0.9877 - val_loss: 0.1578 - learning_rate: 3.1250e-05\n",
      "Epoch 118/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9605 - loss: 0.2502\n",
      "Epoch 118: val_loss did not improve from 0.15777\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9605 - loss: 0.2501 - val_accuracy: 0.9877 - val_loss: 0.1579 - learning_rate: 3.1250e-05\n",
      "Epoch 119/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9621 - loss: 0.2483\n",
      "Epoch 119: val_loss did not improve from 0.15777\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9621 - loss: 0.2482 - val_accuracy: 0.9862 - val_loss: 0.1587 - learning_rate: 3.1250e-05\n",
      "Epoch 120/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9573 - loss: 0.2487\n",
      "Epoch 120: val_loss improved from 0.15777 to 0.15762, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 12ms/step - accuracy: 0.9574 - loss: 0.2486 - val_accuracy: 0.9862 - val_loss: 0.1576 - learning_rate: 3.1250e-05\n",
      "Epoch 121/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9538 - loss: 0.2587\n",
      "Epoch 121: val_loss improved from 0.15762 to 0.15604, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 12ms/step - accuracy: 0.9538 - loss: 0.2586 - val_accuracy: 0.9869 - val_loss: 0.1560 - learning_rate: 3.1250e-05\n",
      "Epoch 122/500\n",
      "\u001B[1m156/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9619 - loss: 0.2383\n",
      "Epoch 122: val_loss did not improve from 0.15604\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9618 - loss: 0.2384 - val_accuracy: 0.9862 - val_loss: 0.1568 - learning_rate: 3.1250e-05\n",
      "Epoch 123/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9638 - loss: 0.2403\n",
      "Epoch 123: val_loss did not improve from 0.15604\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9638 - loss: 0.2402 - val_accuracy: 0.9846 - val_loss: 0.1574 - learning_rate: 3.1250e-05\n",
      "Epoch 124/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9581 - loss: 0.2723\n",
      "Epoch 124: val_loss improved from 0.15604 to 0.15545, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9581 - loss: 0.2721 - val_accuracy: 0.9854 - val_loss: 0.1555 - learning_rate: 3.1250e-05\n",
      "Epoch 125/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9682 - loss: 0.2313\n",
      "Epoch 125: val_loss did not improve from 0.15545\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 12ms/step - accuracy: 0.9681 - loss: 0.2317 - val_accuracy: 0.9854 - val_loss: 0.1570 - learning_rate: 3.1250e-05\n",
      "Epoch 126/500\n",
      "\u001B[1m157/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9621 - loss: 0.2383\n",
      "Epoch 126: val_loss did not improve from 0.15545\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9620 - loss: 0.2386 - val_accuracy: 0.9808 - val_loss: 0.1657 - learning_rate: 3.1250e-05\n",
      "Epoch 127/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9560 - loss: 0.2592\n",
      "Epoch 127: val_loss did not improve from 0.15545\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 11ms/step - accuracy: 0.9560 - loss: 0.2591 - val_accuracy: 0.9869 - val_loss: 0.1561 - learning_rate: 3.1250e-05\n",
      "Epoch 128/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9621 - loss: 0.2397\n",
      "Epoch 128: val_loss did not improve from 0.15545\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9620 - loss: 0.2399 - val_accuracy: 0.9869 - val_loss: 0.1555 - learning_rate: 3.1250e-05\n",
      "Epoch 129/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9670 - loss: 0.2374\n",
      "Epoch 129: val_loss improved from 0.15545 to 0.15382, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 12ms/step - accuracy: 0.9669 - loss: 0.2376 - val_accuracy: 0.9869 - val_loss: 0.1538 - learning_rate: 3.1250e-05\n",
      "Epoch 130/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9612 - loss: 0.2342\n",
      "Epoch 130: val_loss did not improve from 0.15382\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9613 - loss: 0.2342 - val_accuracy: 0.9808 - val_loss: 0.1648 - learning_rate: 3.1250e-05\n",
      "Epoch 131/500\n",
      "\u001B[1m158/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9623 - loss: 0.2471\n",
      "Epoch 131: val_loss improved from 0.15382 to 0.15153, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9623 - loss: 0.2470 - val_accuracy: 0.9877 - val_loss: 0.1515 - learning_rate: 3.1250e-05\n",
      "Epoch 132/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9653 - loss: 0.2267\n",
      "Epoch 132: val_loss improved from 0.15153 to 0.15103, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.9652 - loss: 0.2268 - val_accuracy: 0.9877 - val_loss: 0.1510 - learning_rate: 3.1250e-05\n",
      "Epoch 133/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9643 - loss: 0.2320 \n",
      "Epoch 133: val_loss did not improve from 0.15103\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.9643 - loss: 0.2320 - val_accuracy: 0.9877 - val_loss: 0.1515 - learning_rate: 3.1250e-05\n",
      "Epoch 134/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9620 - loss: 0.2452\n",
      "Epoch 134: val_loss improved from 0.15103 to 0.15061, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.9620 - loss: 0.2452 - val_accuracy: 0.9885 - val_loss: 0.1506 - learning_rate: 3.1250e-05\n",
      "Epoch 135/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9653 - loss: 0.2317\n",
      "Epoch 135: val_loss did not improve from 0.15061\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 9ms/step - accuracy: 0.9652 - loss: 0.2318 - val_accuracy: 0.9877 - val_loss: 0.1532 - learning_rate: 3.1250e-05\n",
      "Epoch 136/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.9575 - loss: 0.2479\n",
      "Epoch 136: val_loss improved from 0.15061 to 0.14968, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - accuracy: 0.9575 - loss: 0.2478 - val_accuracy: 0.9892 - val_loss: 0.1497 - learning_rate: 3.1250e-05\n",
      "Epoch 137/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 20ms/step - accuracy: 0.9561 - loss: 0.2480\n",
      "Epoch 137: val_loss improved from 0.14968 to 0.14945, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 23ms/step - accuracy: 0.9562 - loss: 0.2480 - val_accuracy: 0.9885 - val_loss: 0.1494 - learning_rate: 3.1250e-05\n",
      "Epoch 138/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9685 - loss: 0.2195\n",
      "Epoch 138: val_loss did not improve from 0.14945\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 21ms/step - accuracy: 0.9684 - loss: 0.2197 - val_accuracy: 0.9846 - val_loss: 0.1499 - learning_rate: 3.1250e-05\n",
      "Epoch 139/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9642 - loss: 0.2325\n",
      "Epoch 139: val_loss did not improve from 0.14945\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9642 - loss: 0.2324 - val_accuracy: 0.9862 - val_loss: 0.1499 - learning_rate: 3.1250e-05\n",
      "Epoch 140/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9590 - loss: 0.2395\n",
      "Epoch 140: val_loss did not improve from 0.14945\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 21ms/step - accuracy: 0.9590 - loss: 0.2395 - val_accuracy: 0.9838 - val_loss: 0.1538 - learning_rate: 3.1250e-05\n",
      "Epoch 141/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9615 - loss: 0.2387\n",
      "Epoch 141: val_loss did not improve from 0.14945\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9615 - loss: 0.2387 - val_accuracy: 0.9869 - val_loss: 0.1505 - learning_rate: 3.1250e-05\n",
      "Epoch 142/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9629 - loss: 0.2218\n",
      "Epoch 142: val_loss did not improve from 0.14945\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9629 - loss: 0.2220 - val_accuracy: 0.9869 - val_loss: 0.1534 - learning_rate: 3.1250e-05\n",
      "Epoch 143/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9536 - loss: 0.2512\n",
      "Epoch 143: val_loss improved from 0.14945 to 0.14684, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 21ms/step - accuracy: 0.9536 - loss: 0.2512 - val_accuracy: 0.9877 - val_loss: 0.1468 - learning_rate: 1.5625e-05\n",
      "Epoch 144/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9607 - loss: 0.2347\n",
      "Epoch 144: val_loss improved from 0.14684 to 0.14453, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 22ms/step - accuracy: 0.9607 - loss: 0.2346 - val_accuracy: 0.9885 - val_loss: 0.1445 - learning_rate: 1.5625e-05\n",
      "Epoch 145/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9684 - loss: 0.2165\n",
      "Epoch 145: val_loss improved from 0.14453 to 0.14426, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 20ms/step - accuracy: 0.9683 - loss: 0.2166 - val_accuracy: 0.9892 - val_loss: 0.1443 - learning_rate: 1.5625e-05\n",
      "Epoch 146/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9629 - loss: 0.2252\n",
      "Epoch 146: val_loss did not improve from 0.14426\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 21ms/step - accuracy: 0.9629 - loss: 0.2253 - val_accuracy: 0.9892 - val_loss: 0.1446 - learning_rate: 1.5625e-05\n",
      "Epoch 147/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9603 - loss: 0.2345\n",
      "Epoch 147: val_loss did not improve from 0.14426\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 20ms/step - accuracy: 0.9603 - loss: 0.2344 - val_accuracy: 0.9892 - val_loss: 0.1443 - learning_rate: 1.5625e-05\n",
      "Epoch 148/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9571 - loss: 0.2389\n",
      "Epoch 148: val_loss did not improve from 0.14426\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 21ms/step - accuracy: 0.9572 - loss: 0.2388 - val_accuracy: 0.9869 - val_loss: 0.1444 - learning_rate: 1.5625e-05\n",
      "Epoch 149/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9627 - loss: 0.2213\n",
      "Epoch 149: val_loss did not improve from 0.14426\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9627 - loss: 0.2214 - val_accuracy: 0.9885 - val_loss: 0.1452 - learning_rate: 1.5625e-05\n",
      "Epoch 150/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9710 - loss: 0.2197\n",
      "Epoch 150: val_loss improved from 0.14426 to 0.14239, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9709 - loss: 0.2196 - val_accuracy: 0.9885 - val_loss: 0.1424 - learning_rate: 1.5625e-05\n",
      "Epoch 151/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9566 - loss: 0.2383\n",
      "Epoch 151: val_loss did not improve from 0.14239\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - accuracy: 0.9567 - loss: 0.2379 - val_accuracy: 0.9885 - val_loss: 0.1434 - learning_rate: 1.5625e-05\n",
      "Epoch 152/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9686 - loss: 0.2125\n",
      "Epoch 152: val_loss did not improve from 0.14239\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9685 - loss: 0.2127 - val_accuracy: 0.9877 - val_loss: 0.1430 - learning_rate: 1.5625e-05\n",
      "Epoch 153/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9663 - loss: 0.2220\n",
      "Epoch 153: val_loss did not improve from 0.14239\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 21ms/step - accuracy: 0.9663 - loss: 0.2220 - val_accuracy: 0.9885 - val_loss: 0.1448 - learning_rate: 1.5625e-05\n",
      "Epoch 154/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9680 - loss: 0.2178\n",
      "Epoch 154: val_loss did not improve from 0.14239\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9680 - loss: 0.2179 - val_accuracy: 0.9885 - val_loss: 0.1454 - learning_rate: 1.5625e-05\n",
      "Epoch 155/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9664 - loss: 0.2172\n",
      "Epoch 155: val_loss improved from 0.14239 to 0.14212, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9664 - loss: 0.2173 - val_accuracy: 0.9885 - val_loss: 0.1421 - learning_rate: 1.5625e-05\n",
      "Epoch 156/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9630 - loss: 0.2263\n",
      "Epoch 156: val_loss improved from 0.14212 to 0.14119, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 22ms/step - accuracy: 0.9631 - loss: 0.2261 - val_accuracy: 0.9877 - val_loss: 0.1412 - learning_rate: 1.5625e-05\n",
      "Epoch 157/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9734 - loss: 0.1954\n",
      "Epoch 157: val_loss did not improve from 0.14119\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9733 - loss: 0.1957 - val_accuracy: 0.9877 - val_loss: 0.1438 - learning_rate: 1.5625e-05\n",
      "Epoch 158/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.9642 - loss: 0.2148\n",
      "Epoch 158: val_loss did not improve from 0.14119\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - accuracy: 0.9641 - loss: 0.2149 - val_accuracy: 0.9877 - val_loss: 0.1416 - learning_rate: 1.5625e-05\n",
      "Epoch 159/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9656 - loss: 0.2309\n",
      "Epoch 159: val_loss did not improve from 0.14119\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9656 - loss: 0.2309 - val_accuracy: 0.9885 - val_loss: 0.1420 - learning_rate: 1.5625e-05\n",
      "Epoch 160/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9649 - loss: 0.2254\n",
      "Epoch 160: val_loss improved from 0.14119 to 0.14093, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9649 - loss: 0.2254 - val_accuracy: 0.9892 - val_loss: 0.1409 - learning_rate: 1.5625e-05\n",
      "Epoch 161/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9671 - loss: 0.2200\n",
      "Epoch 161: val_loss did not improve from 0.14093\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 21ms/step - accuracy: 0.9671 - loss: 0.2202 - val_accuracy: 0.9862 - val_loss: 0.1431 - learning_rate: 1.5625e-05\n",
      "Epoch 162/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9583 - loss: 0.2428\n",
      "Epoch 162: val_loss improved from 0.14093 to 0.14076, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 22ms/step - accuracy: 0.9584 - loss: 0.2426 - val_accuracy: 0.9892 - val_loss: 0.1408 - learning_rate: 1.5625e-05\n",
      "Epoch 163/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9602 - loss: 0.2209\n",
      "Epoch 163: val_loss did not improve from 0.14076\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9603 - loss: 0.2209 - val_accuracy: 0.9877 - val_loss: 0.1433 - learning_rate: 1.5625e-05\n",
      "Epoch 164/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9672 - loss: 0.2152\n",
      "Epoch 164: val_loss improved from 0.14076 to 0.13973, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9673 - loss: 0.2152 - val_accuracy: 0.9892 - val_loss: 0.1397 - learning_rate: 1.5625e-05\n",
      "Epoch 165/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9711 - loss: 0.2090\n",
      "Epoch 165: val_loss did not improve from 0.13973\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9712 - loss: 0.2089 - val_accuracy: 0.9892 - val_loss: 0.1417 - learning_rate: 1.5625e-05\n",
      "Epoch 166/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9655 - loss: 0.2160\n",
      "Epoch 166: val_loss did not improve from 0.13973\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 21ms/step - accuracy: 0.9655 - loss: 0.2160 - val_accuracy: 0.9862 - val_loss: 0.1413 - learning_rate: 1.5625e-05\n",
      "Epoch 167/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9645 - loss: 0.2240\n",
      "Epoch 167: val_loss improved from 0.13973 to 0.13845, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 21ms/step - accuracy: 0.9644 - loss: 0.2240 - val_accuracy: 0.9885 - val_loss: 0.1384 - learning_rate: 1.5625e-05\n",
      "Epoch 168/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9670 - loss: 0.2188\n",
      "Epoch 168: val_loss did not improve from 0.13845\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9669 - loss: 0.2190 - val_accuracy: 0.9885 - val_loss: 0.1395 - learning_rate: 1.5625e-05\n",
      "Epoch 169/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9680 - loss: 0.2169\n",
      "Epoch 169: val_loss did not improve from 0.13845\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9679 - loss: 0.2170 - val_accuracy: 0.9885 - val_loss: 0.1391 - learning_rate: 1.5625e-05\n",
      "Epoch 170/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9667 - loss: 0.2090\n",
      "Epoch 170: val_loss did not improve from 0.13845\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 21ms/step - accuracy: 0.9667 - loss: 0.2091 - val_accuracy: 0.9892 - val_loss: 0.1397 - learning_rate: 1.5625e-05\n",
      "Epoch 171/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9655 - loss: 0.2164\n",
      "Epoch 171: val_loss improved from 0.13845 to 0.13767, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 22ms/step - accuracy: 0.9655 - loss: 0.2164 - val_accuracy: 0.9885 - val_loss: 0.1377 - learning_rate: 1.5625e-05\n",
      "Epoch 172/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9695 - loss: 0.2086\n",
      "Epoch 172: val_loss did not improve from 0.13767\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 20ms/step - accuracy: 0.9695 - loss: 0.2086 - val_accuracy: 0.9900 - val_loss: 0.1377 - learning_rate: 1.5625e-05\n",
      "Epoch 173/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9716 - loss: 0.2038\n",
      "Epoch 173: val_loss did not improve from 0.13767\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 22ms/step - accuracy: 0.9715 - loss: 0.2039 - val_accuracy: 0.9885 - val_loss: 0.1383 - learning_rate: 1.5625e-05\n",
      "Epoch 174/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9576 - loss: 0.2366\n",
      "Epoch 174: val_loss did not improve from 0.13767\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 22ms/step - accuracy: 0.9576 - loss: 0.2364 - val_accuracy: 0.9885 - val_loss: 0.1386 - learning_rate: 1.5625e-05\n",
      "Epoch 175/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9622 - loss: 0.2274\n",
      "Epoch 175: val_loss did not improve from 0.13767\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 22ms/step - accuracy: 0.9622 - loss: 0.2272 - val_accuracy: 0.9877 - val_loss: 0.1388 - learning_rate: 1.5625e-05\n",
      "Epoch 176/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9623 - loss: 0.2293\n",
      "Epoch 176: val_loss improved from 0.13767 to 0.13697, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 21ms/step - accuracy: 0.9624 - loss: 0.2291 - val_accuracy: 0.9885 - val_loss: 0.1370 - learning_rate: 1.5625e-05\n",
      "Epoch 177/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9685 - loss: 0.2068\n",
      "Epoch 177: val_loss improved from 0.13697 to 0.13664, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 22ms/step - accuracy: 0.9685 - loss: 0.2068 - val_accuracy: 0.9885 - val_loss: 0.1366 - learning_rate: 1.5625e-05\n",
      "Epoch 178/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9602 - loss: 0.2329\n",
      "Epoch 178: val_loss improved from 0.13664 to 0.13565, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9603 - loss: 0.2327 - val_accuracy: 0.9892 - val_loss: 0.1357 - learning_rate: 1.5625e-05\n",
      "Epoch 179/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9656 - loss: 0.2131\n",
      "Epoch 179: val_loss did not improve from 0.13565\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9656 - loss: 0.2131 - val_accuracy: 0.9892 - val_loss: 0.1372 - learning_rate: 1.5625e-05\n",
      "Epoch 180/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9647 - loss: 0.2165\n",
      "Epoch 180: val_loss did not improve from 0.13565\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9648 - loss: 0.2165 - val_accuracy: 0.9885 - val_loss: 0.1368 - learning_rate: 1.5625e-05\n",
      "Epoch 181/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.9649 - loss: 0.2065\n",
      "Epoch 181: val_loss did not improve from 0.13565\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - accuracy: 0.9648 - loss: 0.2067 - val_accuracy: 0.9885 - val_loss: 0.1377 - learning_rate: 1.5625e-05\n",
      "Epoch 182/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9688 - loss: 0.2014\n",
      "Epoch 182: val_loss did not improve from 0.13565\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 21ms/step - accuracy: 0.9688 - loss: 0.2016 - val_accuracy: 0.9885 - val_loss: 0.1367 - learning_rate: 1.5625e-05\n",
      "Epoch 183/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9615 - loss: 0.2135\n",
      "Epoch 183: val_loss did not improve from 0.13565\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9615 - loss: 0.2136 - val_accuracy: 0.9877 - val_loss: 0.1396 - learning_rate: 1.5625e-05\n",
      "Epoch 184/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9629 - loss: 0.2191\n",
      "Epoch 184: val_loss improved from 0.13565 to 0.13510, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9629 - loss: 0.2191 - val_accuracy: 0.9892 - val_loss: 0.1351 - learning_rate: 7.8125e-06\n",
      "Epoch 185/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9729 - loss: 0.2028\n",
      "Epoch 185: val_loss did not improve from 0.13510\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9729 - loss: 0.2028 - val_accuracy: 0.9892 - val_loss: 0.1357 - learning_rate: 7.8125e-06\n",
      "Epoch 186/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 0.9511 - loss: 0.2479\n",
      "Epoch 186: val_loss did not improve from 0.13510\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - accuracy: 0.9513 - loss: 0.2474 - val_accuracy: 0.9892 - val_loss: 0.1357 - learning_rate: 7.8125e-06\n",
      "Epoch 187/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9730 - loss: 0.2034\n",
      "Epoch 187: val_loss did not improve from 0.13510\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9730 - loss: 0.2034 - val_accuracy: 0.9885 - val_loss: 0.1354 - learning_rate: 7.8125e-06\n",
      "Epoch 188/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9672 - loss: 0.2127\n",
      "Epoch 188: val_loss did not improve from 0.13510\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9672 - loss: 0.2127 - val_accuracy: 0.9885 - val_loss: 0.1359 - learning_rate: 7.8125e-06\n",
      "Epoch 189/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9675 - loss: 0.2093\n",
      "Epoch 189: val_loss did not improve from 0.13510\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9675 - loss: 0.2093 - val_accuracy: 0.9885 - val_loss: 0.1357 - learning_rate: 7.8125e-06\n",
      "Epoch 190/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9658 - loss: 0.2096\n",
      "Epoch 190: val_loss improved from 0.13510 to 0.13479, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9658 - loss: 0.2096 - val_accuracy: 0.9885 - val_loss: 0.1348 - learning_rate: 3.9063e-06\n",
      "Epoch 191/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9598 - loss: 0.2244\n",
      "Epoch 191: val_loss did not improve from 0.13479\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9599 - loss: 0.2241 - val_accuracy: 0.9885 - val_loss: 0.1350 - learning_rate: 3.9063e-06\n",
      "Epoch 192/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9704 - loss: 0.2019\n",
      "Epoch 192: val_loss did not improve from 0.13479\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 21ms/step - accuracy: 0.9704 - loss: 0.2019 - val_accuracy: 0.9885 - val_loss: 0.1349 - learning_rate: 3.9063e-06\n",
      "Epoch 193/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9663 - loss: 0.2105\n",
      "Epoch 193: val_loss improved from 0.13479 to 0.13457, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9663 - loss: 0.2103 - val_accuracy: 0.9892 - val_loss: 0.1346 - learning_rate: 3.9063e-06\n",
      "Epoch 194/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9657 - loss: 0.2197\n",
      "Epoch 194: val_loss improved from 0.13457 to 0.13443, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 22ms/step - accuracy: 0.9657 - loss: 0.2196 - val_accuracy: 0.9885 - val_loss: 0.1344 - learning_rate: 3.9063e-06\n",
      "Epoch 195/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9675 - loss: 0.2087\n",
      "Epoch 195: val_loss did not improve from 0.13443\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9675 - loss: 0.2087 - val_accuracy: 0.9892 - val_loss: 0.1352 - learning_rate: 3.9063e-06\n",
      "Epoch 196/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.9725 - loss: 0.2002\n",
      "Epoch 196: val_loss did not improve from 0.13443\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - accuracy: 0.9724 - loss: 0.2003 - val_accuracy: 0.9892 - val_loss: 0.1345 - learning_rate: 3.9063e-06\n",
      "Epoch 197/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9721 - loss: 0.1979\n",
      "Epoch 197: val_loss improved from 0.13443 to 0.13379, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9721 - loss: 0.1980 - val_accuracy: 0.9885 - val_loss: 0.1338 - learning_rate: 3.9063e-06\n",
      "Epoch 198/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9654 - loss: 0.2119\n",
      "Epoch 198: val_loss did not improve from 0.13379\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9655 - loss: 0.2117 - val_accuracy: 0.9885 - val_loss: 0.1344 - learning_rate: 3.9063e-06\n",
      "Epoch 199/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9672 - loss: 0.2152\n",
      "Epoch 199: val_loss did not improve from 0.13379\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9672 - loss: 0.2150 - val_accuracy: 0.9892 - val_loss: 0.1342 - learning_rate: 3.9063e-06\n",
      "Epoch 200/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.9662 - loss: 0.2069\n",
      "Epoch 200: val_loss improved from 0.13379 to 0.13376, saving model to best_handsigns_model.keras\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9663 - loss: 0.2069 - val_accuracy: 0.9892 - val_loss: 0.1338 - learning_rate: 3.9063e-06\n",
      "Epoch 201/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9635 - loss: 0.2235\n",
      "Epoch 201: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9636 - loss: 0.2233 - val_accuracy: 0.9892 - val_loss: 0.1343 - learning_rate: 3.9063e-06\n",
      "Epoch 202/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9631 - loss: 0.2282\n",
      "Epoch 202: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 20ms/step - accuracy: 0.9632 - loss: 0.2279 - val_accuracy: 0.9892 - val_loss: 0.1346 - learning_rate: 3.9063e-06\n",
      "Epoch 203/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9664 - loss: 0.2136\n",
      "Epoch 203: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9665 - loss: 0.2135 - val_accuracy: 0.9892 - val_loss: 0.1348 - learning_rate: 1.9531e-06\n",
      "Epoch 204/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.9731 - loss: 0.1957\n",
      "Epoch 204: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - accuracy: 0.9730 - loss: 0.1959 - val_accuracy: 0.9892 - val_loss: 0.1342 - learning_rate: 1.9531e-06\n",
      "Epoch 205/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9680 - loss: 0.2043\n",
      "Epoch 205: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9680 - loss: 0.2043 - val_accuracy: 0.9892 - val_loss: 0.1342 - learning_rate: 1.9531e-06\n",
      "Epoch 206/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9611 - loss: 0.2247\n",
      "Epoch 206: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9612 - loss: 0.2246 - val_accuracy: 0.9892 - val_loss: 0.1342 - learning_rate: 1.9531e-06\n",
      "Epoch 207/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9635 - loss: 0.2136\n",
      "Epoch 207: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9635 - loss: 0.2136 - val_accuracy: 0.9892 - val_loss: 0.1341 - learning_rate: 1.9531e-06\n",
      "Epoch 208/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9639 - loss: 0.2042\n",
      "Epoch 208: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9639 - loss: 0.2042 - val_accuracy: 0.9892 - val_loss: 0.1339 - learning_rate: 1.0000e-06\n",
      "Epoch 209/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9674 - loss: 0.2139\n",
      "Epoch 209: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 21ms/step - accuracy: 0.9673 - loss: 0.2140 - val_accuracy: 0.9892 - val_loss: 0.1342 - learning_rate: 1.0000e-06\n",
      "Epoch 210/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9693 - loss: 0.1996\n",
      "Epoch 210: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - accuracy: 0.9693 - loss: 0.1998 - val_accuracy: 0.9892 - val_loss: 0.1344 - learning_rate: 1.0000e-06\n",
      "Epoch 211/500\n",
      "\u001B[1m161/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9720 - loss: 0.1933\n",
      "Epoch 211: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9720 - loss: 0.1933 - val_accuracy: 0.9892 - val_loss: 0.1342 - learning_rate: 1.0000e-06\n",
      "Epoch 212/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 0.9653 - loss: 0.2072\n",
      "Epoch 212: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - accuracy: 0.9654 - loss: 0.2070 - val_accuracy: 0.9892 - val_loss: 0.1345 - learning_rate: 1.0000e-06\n",
      "Epoch 213/500\n",
      "\u001B[1m159/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.9654 - loss: 0.2131\n",
      "Epoch 213: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9656 - loss: 0.2129 - val_accuracy: 0.9892 - val_loss: 0.1341 - learning_rate: 1.0000e-06\n",
      "Epoch 214/500\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9659 - loss: 0.2172\n",
      "Epoch 214: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 20ms/step - accuracy: 0.9659 - loss: 0.2172 - val_accuracy: 0.9892 - val_loss: 0.1342 - learning_rate: 1.0000e-06\n",
      "Epoch 215/500\n",
      "\u001B[1m160/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.9735 - loss: 0.1979\n",
      "Epoch 215: val_loss did not improve from 0.13376\n",
      "\u001B[1m162/162\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 21ms/step - accuracy: 0.9735 - loss: 0.1980 - val_accuracy: 0.9892 - val_loss: 0.1339 - learning_rate: 1.0000e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAGGCAYAAACqvTJ0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAADI3UlEQVR4nOzdd3hU1dbA4d+ZnmTSE5IQOhEISG8WFJEiIjYU6yeglwsq6hWxYUWvqIgNFRVQrCiKIIh6UVEUKwhSBKSEDiEhvU4/5/tjkoEhCSQhbZL1Po+PmTPn7LNn9gA7a9ZeW9E0TUMIIYQQQgghhBBCiDqkq+8OCCGEEEIIIYQQQoimR4JSQgghhBBCCCGEEKLOSVBKCCGEEEIIIYQQQtQ5CUoJIYQQQgghhBBCiDonQSkhhBBCCCGEEEIIUeckKCWEEEIIIYQQQggh6pwEpYQQQgghhBBCCCFEnZOglBBCCCGEEEIIIYSocxKUEkIIIYQQQgghhBB1ToJSQogK3XTTTXTs2JHrrruuwnMmT55Mx44defDBB0/7fmvWrKFjx46sWbOmVq8RQgghhAhEjWVu1rFjR1599dXT7p8QIvBJUEoIcVI6nY6NGzeSlpZW5rni4mJWrVpVD70SQgghhGiaZG4mhGhMJCglhDipzp07YzabWbFiRZnnVq1aRVBQEHFxcfXQMyGEEEKIpkfmZkKIxkSCUkKIkwoODmbgwIHlTny+/vprLrroIgwGg99xh8PB7NmzGT58OF27dmXYsGHMnTsXVVX9zlu4cCEXXXQR3bp14//+7/9ITU0tc4/U1FTuuece+vXrR/fu3Rk7dizbtm2r0muw2+288MILDBs2jDPPPJNevXpx8803888///id99NPP3HdddfRo0cPBgwYwGOPPUZ+fr7v+T179nDHHXfQr18/+vbty8SJE9m9ezdQcar6TTfdxE033eR7fOGFF/L0008zduxYunXrxsMPPwzA9u3bueOOOzjrrLPo0qUL5513Hk899RR2u913rdPp5OWXX2bw4MF069aNkSNH8vnnnwOwYMECOnbsyN69e/3uv2zZMpKTkzly5EiV3jMhhBBCNEyNYW52oqNHjzJ16lQGDhxIt27duPrqq/n+++/9zvn111+55ppr6NmzJ3379uW2227zzcMADhw4wK233kr//v3p3r071157LT/99NNp9UsIUfskKCWEOKURI0aUSRMvLCxk9erVjBw50u9cTdO49dZbeeuttxg9ejRvvvkmw4cP5+WXX+bxxx/3nffhhx/y+OOPM3DgQF5//XW6d+/Oo48+6tdWdnY21113HVu3buXRRx/lhRdeQFVVbrzxRr9JyKncf//9LF68mAkTJjB//nymTp3Krl27mDJlCpqmAd5vFidOnEh0dDQvv/wy9957LytXrmTy5MkApKenc+2117Jv3z6mTZvGzJkzyczMZOzYseTm5lbp/VywYAFdu3bl9ddf5+qrr+bo0aPceOON2Gw2nn32WebNm8cll1zCBx98wPvvv++77t577+Wdd95h9OjRzJkzhwEDBvDggw/y5Zdfcumll2I2m1m2bJnfvZYuXcrZZ59NQkJClfoohBBCiIYr0Odmx8vMzOTqq69m3bp1TJ48mVdffZXExEQmTZrEF198AcDBgwe5/fbbOfPMM3njjTeYPn06e/fuZcKECaiqiqqqTJw4EZvNxnPPPcfrr79OREQEt912G/v3769Wv4QQdcNw6lOEEE3dBRdcQFBQECtWrGDcuHEAfPfdd0RHR9O7d2+/c1evXs1vv/3Giy++yCWXXALAueeei8ViYdasWYwZM4akpCRef/11RowYwUMPPQTAgAEDKCwsZOHChb623nvvPXJzc/n4449JTEwE4Pzzz2fEiBHMmjWLV1555ZR9dzqdFBUV8cgjjzBixAgA+vXrR2FhIc8++yyZmZnExsby6quvkpyczGuvvYaiKACYTCZmzZpFZmYm7777Lk6nk3feeYfY2FgAOnXqxPXXX8+mTZuwWCyVfj+bN2/Ovffe63v8yy+/kJyczKxZs7BarQCcc845/Prrr6xZs4YJEyawc+dOvvnmGx566CHGjh0LwNlnn83hw4dZs2YNI0eOZOjQoXzxxRf85z//QVEU0tLS+OOPP5g5c2al+yaEEEKIhi+Q52Yneuedd8jOzuabb77xtTlw4EDGjRvHc889x8iRI9m8eTN2u52JEyf6libGx8fz/fffU1xcjM1mY8+ePdx+++0MHDgQgG7duvHaa6/hdDqr3CchRN2RTCkhxClZLBYuvPBCvzTxr776iosvvtgXwCm1du1aDAYDw4cP9zt+2WWX+Z7fs2cPWVlZDBo0yO+ciy++2O/x77//TnJyMnFxcbjdbtxuNzqdjvPPP5/ffvutUn03mUy8/fbbjBgxgvT0dP744w8WLlzoKwLqdDqx2+1s27aNIUOG+L2eESNG8M033xATE8P69evp0aOHLyAF3snQqlWrfJOfykpOTvZ7PGDAAD788EPMZjMpKSl8//33vPHGG2RnZ/smUuvXrwdg2LBhfte++uqr/Pe//wXg6quv5vDhw6xbtw7wZkmFhIQwdOjQKvVPCCGEEA1bIM/NTrR27Vp69uzpC0gd37+MjAz27NlD9+7dMZvNXH311UyfPp2ff/6ZTp06MXnyZKxWKzExMSQlJfHoo4/ywAMPsHz5clRVZerUqZxxxhnV6pcQom5IppQQolIuvvhi7rjjDtLS0jCbzfz+++/cfffdZc7Ly8sjMjISvV7vd7w0mFNQUEBeXh4AkZGR5Z5TKjc3l/3799OlS5dy+2Sz2SrV959//pmnn36aPXv2EBISQqdOnQgODga8Ke15eXlomkZ0dHSFbeTm5tKiRYtK3e9USu9dSlVVXnzxRRYsWEBxcTEJCQl069YNs9nsd3/gpH0866yzaNGiBUuXLqVv374sXbqUESNG+LUjhBBCiMYhkOdmJ/avZcuWZY7HxMQAkJ+fT1JSEh9++CFz587ls88+4/333ycsLIwbbriBu+++G0VRmD9/Pm+88QbfffcdS5cuxWg0MmTIEJ544gnCw8Or3C8hRN2QoJQQolLOP/98QkJCWLFiBcHBwbRo0YIzzzyzzHnh4eHk5OTg8Xj8Jj9Hjx4FvJOd0glPVlaW37Un1mYKDQ2lX79+3H///eX2yWQynbLfBw4cYNKkSQwZMoQ5c+bQsmVLFEVhwYIF/PzzzwBYrVYURSE7O9vvWofDwR9//EH37t0JDQ0t8zx4vzFs0aKF71vJEwuGFhUVERISctI+zp07l3fffZcnnniCYcOGERoaCngzn0qFhYUB3loO8fHxvuO7d+8mNzeX3r17oygKV155JR988AHXX389e/fuZcaMGad8j4QQQggReAJ1blZe/zIyMsocLz1W2rfjl+OtX7+eTz75hDfffJNOnTpx8cUXExcXx7Rp03j88cfZvn07K1asYN68eURGRvrVzhJCNCyyfE8IUSkmk4khQ4bwzTff8L///c9Xk+BE/fr1w+12l9kRprRQZe/evWnTpg0JCQllzildUnd8W3v37qVt27Z07drV99+yZcv47LPPynzjV54tW7bgcDiYMGECrVq18gWPSgNSmqYREhJCcnJymfuvXr2aCRMmcPToUfr06cOmTZv8AlNZWVmMHz+en376yVcL6viCo3l5eZUq+rl+/XqSkpK46qqrfAGp9PR0du7c6QtyldaH+OGHH/yuff7555k+fbrv8ahRo8jPz2fGjBm0b9+e7t27n/L+QgghhAg8gTo3O1Hfvn3ZsGEDhw8fLtO/2NhYWrduzbvvvsugQYNwOp2YTCbOPvtsX/mC1NRUNmzYwDnnnMPmzZtRFIXk5GQmT55Mhw4dyt1BUAjRcEimlBCi0kaMGMHEiRPR6XQ88sgj5Z5z/vnn079/fx555BHS09Pp1KkTa9euZd68eVx55ZUkJSUB3p3kpkyZwiOPPMLw4cPZuHEjH3/8sV9b48aNY9myZYwbN45bbrmFyMhIvv76az799FOmTp1aqT536dIFg8HAzJkzueWWW3A6nSxZsoQff/wRgOLiYgDuuusubrvtNu655x6uuOIKMjMzefHFFxkyZAgdOnRg3LhxLF26lPHjxzNx4kSMRiNvvPEG8fHxXHrppVitVhISEpg9e7Yv82rOnDkEBQWdso/dunXj9ddfZ+7cufTo0YP9+/czZ84cnE6nLw2+U6dODB8+nJkzZ2K320lOTmb16tWsWrWK1157zddW8+bNOeecc/jll1/8iqkLIYQQovEJxLnZiW6++Wa++OILxo0bxx133EFERARLly7ljz/+4Omnn0an03HWWWfx/PPPM2nSJP7v//4PvV7PwoULMZlMDBo0iMTERCwWC/fffz933nknMTEx/Pbbb/zzzz+MGTOmWv0SQtQNCUoJISrtnHPOISwsjISEBNq3b1/uOaXBmFdeeYV3332X7OxsWrRowT333MPNN9/sO2/kyJHodDpef/11li1bRocOHXjyySe55557fOfExcWxcOFCXnjhBaZNm4bD4aBNmzZMnz7db2nbybRu3ZoXXniB1157jdtuu43w8HB69OjBBx98wE033cS6devo2LEjgwYN4s033+S1115j0qRJREVFcemll3LnnXcCkJCQwEcffcTMmTN58MEHMZlM9O/fn5deeslXp+CVV17h6aef5p577iEmJoaxY8eyZ88e9u7de9I+Tpw4kZycHN5//31mz55NQkICl19+ue+9zM/PJywsjJkzZ/Laa6/x3nvvkZOTQ/v27XnllVcYMmSIX3sXXHABv//+O5dffnml3iMhhBBCBKZAnJudKDY2lo8//pgXXniBp556CpfLRadOnXj99dcZPHgw4P1y7s0332T27Nncc889eDwezjzzTObPn0+7du0AmD9/Pi+88ALTp08nPz+fNm3a8OSTTzJq1Khq9UsIUTcUTdO0+u6EEEKImjN+/HjMZjOzZ8+u764IIYQQQgghRIUkU0oIIRqJ2bNns3fvXn755Rc++uij+u6OEEIIIYQQQpyUBKWEEKKR+OGHHzhw4AD3338/vXr1qu/uCCGEEEIIIcRJyfI9IYQQQgghhBBCCFHndPXdASGEEEIIIYQQQgjR9EhQSgghhBBCCCGEEELUOQlKCSGEEEIIIYQQQog6J0EpIYQQQgghhBBCCFHnJCglhBBCCCGEEEIIIeqcob47UNeysgqojf0GFQWio0NrrX1Re2TsApuMX+CSsQtsjXn8Sl+bqJjMp8SJZOwCm4xf4JKxC2yNefwqO59qckEpTaNWB7u22xe1R8YusMn4BS4Zu8Am49c0yXxKVETGLrDJ+AUuGbvA1pTHT5bvCSGEEEIIIYQQQog6J0EpIYQQQgghhBBCCFHnGkRQyul0MnLkSNasWVPhOdu2bWP06NF0796dq666ii1bttRhD4UQQgghhBBCCCFETar3mlIOh4MpU6awa9euCs8pLi5mwoQJXHrppTz77LN8/PHHTJw4ke+++47g4OAa64umabjdrmpdqyhgt9txuZxNdi1oXdLrDeh0DSKmKoQQQogTqKqKx+Ou8nUyn6pbMp8SQghR3+o1KJWSksKUKVPQTjHr+PrrrzGbzdx///0oisLDDz/M6tWrWbFiBaNGjaqRvrjdLrKy0tA0tdptZGfrUNXqXy+qJijISlhYFIqi1HdXhBBCCIH3C778/GxstsJqtyHzqbol8ykhhBD1qV6DUmvXrqV///5MnjyZHj16VHjepk2b6N27t+8fS0VR6NWrFxs3bqyRoJSmaeTlZaPT6QgPj0VRqveNkV6v4PHI13q1TdM0nE4HhYU5AISHR9dzj4QQQoi64XQ6GTVqFI8++ij9+/fnwQcf5PPPPy9zXv/+/Xn//ffLHM/Ly6Nfv35+xyIiIk5aQqEqSgNSVmskJpO5WoEOmU/VDZlPCSGEaAjqNSh1ww03VOq8jIwMkpKS/I5FR0efdMlfVaiqB5fLTnh4DCaTpdrtGAw63G75Zq8umExmAAoLcwgNjZTUcyGEEI1eeSUPHn74YaZMmeJ7fPjwYW666SbGjBlTbhspKSlERETw5Zdf+o7V1L+hqurxBaSs1rBqtyPzqboj8ykhhBD1rd5rSlWGzWbDZDL5HTOZTDidziq3Vd4XdqVL9vT6gHg7RInSiZSqutHrTac4u2KlnwnJWg9MMn6BS8YusDXm8WuIr6mikgehoaGEhob6Hj/44IMMHz6cIUOGlNvOnj17aNu2LbGxsTXeR4/HAxz791kEhtLx8njc6HTVn08JIYQQ1REQURiz2VwmAOV0OrFYqp7VFB0dWuaY3W4nO1uHweD973Sc7vWi8lRVh06nIzIypFqfhROV99kQgUPGL3DJ2AU2Gb+6UZmSB7///jt//vkn33zzTYXtpKSk0KZNm9rpZAmpTRRYZLyEEELUp4AISsXFxZGZmel3LDMzk2bNmlW5raysgjK7ubhczpJdYrTTSheXdPO65fFoqKpKTk4RRmP1dk0E7zfi0dGh5X42RMMn4xe4ZOwCW2Mev9LX1pBUpuTB3LlzufLKK0lISKjwnN27d+N2u7n66qtJT0+nT58+TJ06tcpzqvLiGBLbCGyKcnpj2JizJ5sCGb/AJWMX2Brz+FX2NQVEUKp79+7MmzcPTdNQFAVN0/jrr7+49dZbq9yWplFm8hyIk+np06fxv/99WeHzr7zyJr169al0e3fcMYGePXvzr39NrInu1anyxrQ+2xGnR5e3D/O+ldjPuAItOKbS1504footG50tC09kUuP8W74yNA3FkYeuMBXF40C1NkfTmzGmrceQuRXVHIYa2gJXfB80S0TV2nbb0BceQVdwCH3BYXSFR0D13/5dDY7B2X4Eakg8OIswZP2Doh7LelUA7LHgiUQzWtEVHkFfnA4n7IKqmULxhLZAM0f4j6WmoivOQFdwuKQPh9FModg7XAnGYO9nac83eEITcSf0RSnOxHhkLbriDO/llgjsHa9GC4oCTcWQuRXFWeB3b8VVjK7gEDp7DmpwM9SgGAxZ/2BIW49msuJK6IsnqiMoCprOiBqSgBrSDJ0ty/e+o2koznz0BYdRbNklDetQQ+JRrQloxiD/16s3o4YmohmCMKT9hTFjM7gd/ucYg1GtzVGDm0Fla8B4XN4xK0oreU8TQXWjL0xFsed6z9HpUUPi8Vibg8EMmobOlo2u4FDZ90YBgkwE2Zx1/3enouBIugxPdMc6vnHDdfDgQf744w8efvjhk563Z88eoqKimDp1Kpqm8dJLL3HrrbeyaNEi9Hp9pe93ssxzvV6pdub44RwbZqOOGGvdLQF88snH+frr5RU+P3v2XHr3rvyc6rbb/k2vXr3597+rPk+tD6qqSOa58JHxC1wydoGtKY+fop1YnKCedOzYkffff5/+/fsD3uLmoaGhWCwWCgsLGTp0KJdccgnXXXcdCxcuZMWKFXz77bcEBwdX6T6ZmeVnSmVlHSE6OgGjsfpr6esyU6qwsBCHww7A999/x8KFHzJv3nu+58PCwjEajZVuLz8/D4PBWOX3sz7V1LgpCsTEhJb72RBV4LJh3v0lrub9UcNaAd7AEDo9mjkcAMuWDwlZ+zyOtsMo7nkbqjUefeER1KBoNHM4hoy/Cf/iRnT2bFRTGMX97sET1gpdURru2K6443qWue2J42fa+x2WrR9gOvATiubBE9Yae6erKe55KxiCylwPYDz0K5YtH+CJScYV3wfVHIGiOjHtX4V51zL0efv8L9AZcLYYgCPpUgw5KZh3LUNXmOp/jt6IO7YrroS+qJYo7y/39mxfoMJjbY4a1gpXXC/csV1BdaEvTPUFH9TQRF+gyHB0M8F/zUZx+W+x7kroR3GvO0CnJ2jjXILXvojith13hoainfrvJNUcTlH/+3AkXYoxbT06WzbO1oNQg5thTP0Dy9YP0TlyQQPFkYu+4BA6W9Yp2wXQFB2eiPboc/egaJ5KXXOytkpCWSUHVBTK/qFVLZG44ntj2v/DKV+/ZgjC0XYYxiPr0BcePq3+ibrlaD2Y/JHvnfrEaij9e6WhOnHOBPDWW2/x9ddfs2TJkpNea7PZUBTFF3zIyspiwIABLFiwgF69elW6DxVlnmdmVv/fZadbJSWzCINOoUMza5Wvr66mPqcqnU/FxJz+fKqxZk82BTJ+gUvGLrA15vGrbOZ5g82UGjBgAM888wyjRo3CarUyZ84cHn/8cT799FM6duzI3LlzA+Yf+9pgtVqxWq2+n3U6HdHRlc8qOVFYWHhNdU00VG47oFUYmDldoavuxbJrGZqix5E0Ep09F+Ohn9FMoeRf9AaKx4V19UMomkrQto8I2vaR71pNZ8TZaiDG1LXonPloBgs6Zz7WX6b53cPeYRRF5zyMGhJX5v6KswDrjw9j2XnsFzJNZ0Kfv5+QtS+gz9tHwZBZ3ifcNtCZQKdHV3iEsBUT0DnyYHfF2Yd+PB7M+7/HvP/7is9xezAe+RPjkT9P2ZymM6CckGWkGYIoHPA4noj2hH01Dp2rqMx1pgM/oc/agTu+N9Zfn6ywfTUoGk1vQleUjqKpuMPb4I7rieIqxpC1HX3+fkJXP0Lo6keO3V/R4QlrjSFvb8XtGkNQQ1t4A2zW5miG4zIbNA1jxt8Y09ZhyPHuFOYJiUczHbcjlwIGVwFaQRoKGpohCE9IPOiO/+XPm+2lLz5aboBJK804Cm2Bx5qA4egmDCXZdgDO5mehc+Siz9qBZgzGHd8Hd0Q7UBSMR9ZhzNiMZdeyktdjRbU2P2EcLKihzVEtkeiKjqIrPoonvC2uhD7onIUYj6xBV3DE+3I8dnSFaSiqE01nQA2JRzN4/53STCHebK+gaDRFh6K60RUeKQlS+i8/VlyFJWPlDaq6Evqgmv3/jtY5C9AVHPYGBys7g9F53ytPSLzvenR6PKEtvIFTRUHxOL0Za8dlvmmWCDyhiagnZKopQFCQCZvNWU5osJYpehwdrqzruzZoP//8M4MHDz7leUFB/v8GREdHExERQXp6epXuV5uZ52odf6BkTuUlmecCZPwCmYxdYGvK49dgglI7duw46eNu3brx+eef12WXAtaRI6mMHn0Z48ffysKFCxg2bDiTJ9/PBx+8w/LlS8nIOEp4eASXXz6KW26ZAPgv35s+fRphYWFkZGTw66+rCQ+PYMKE2xk+/JJ6fmVNmKZhOrAKQ/oGbN3HezOPNBXL3++CIci7XMlQccq9UnSUyMWXoSvOwNF2GPbk63C1Guh90llE8Oa3cbQejCe2yym7Yji6GV3+AQDU8Da4Y8/EtG+l7xd7RfP4fgZQHHmEL78J9CYUTcXefiSKuxjz/h+8L81gQXHbfUEEV0I/8kbMx5zyJUF/v4NmsKCZQr3ZTDuXYDrwIznX/A81NNF7A48TNnxIxA/PoC84hKbosHUbj73LDXhCErCkfIF11f1Ydiz2Lgk0WQn/6mY0YwgFQ14ieN2r6Bx5uKM64o48w7tUyuPNVvJEJ2PvcCWuxLNBObasRbFnY075EtO+71FDE7F3uBJ3fG9Qji1XUZwFGNLWY0zfgOL2fgNfulxO05vQF6aiz07xLidz5HqfN4WiGYNRPC509mxCf3wQDQUFDWfiOdiTr/G1rys6SsiamVhSvoCULwAo6n0X9q5j/cZLNYVB6fIwjwvFbUMzHxcYUj1Yti0g5I8Z3vchoj2ayYrx6CYMeXvR9GbsydfiivdmUGhG71I6NTTR+zk8xdJIXd5+DNk7cMd0OTZmpe9RSTZKVnoWOItP3p7b7g0cHkdD8S690x33T5nqwbz7K/RZ/+Bsfwnu2DO9x13FoDf5n6tpGA/9jOnAT7jieuJsM/j0g7aaiuLI8wbfdJVfClWG6kZxFfmyDBsiRYGgmFCKJMu03mmaxt9//33KsgaFhYUMGjSIV199lbPOOguA9PR0cnJyaNeuXV109aSO/fFvOB8omVMJIYQQtavBBKUaGk3TsFdxKZ5B1XB7qr98z2LQ1egOKJs3b+Lttz9AVVVWrPiKTz/9mGnTppOY2II1a37j+eef5dxzz6djx05lrl28+FP+/e/bmDhxEp999gkzZz7NgAEDfd8kNlWWv9/FmPYXBRc8C8ZjmXqKLRvrr0+iK0rDY03ElXgWjo5Xl/8LtseJ9efH0OekoIYm4mrWHXuXm0B/XHaIpmHetRRD5jZAw3jwZ4yZWwG8GT9DX8Wy7WNCf34MgOA1M3F0ugZ3RDs84W1wx/c69su36ibsu0noCw55X0PKciwpyyk4/ynsZ471ZjilLMe0/3tyr1p2fE9Rio6iLzyMO+ZM0OkJ+eNZgv963e8ce9JlGI+sBaC45604ki7Fsv1T1KAYHO0vIfiv17Hs+AzcNpyJ51Aw9FXQG1GKjoLeiGaOQJ+9E/OupShuO0X97wNjMPYz/w/7mf/nu4/h6CZCv78HQ/YOQr+fTN7lCzEe/JnQH++HgsPoAU9oS/KHvoI7oe+x/nW+Hn32DoI3vUXoD1PQOQu8S9wcuUQs9QZ5NIOF/OFzvPWnKiMkjuLoZIr733fSczyRSTiSrz15W5qKruAwmjn8WLBIUwnaOJeQP2agqC7vMqXhc8oEHj1RHQlbMQHF48DWdZy3Pyf7O0RvRNOfsARFp8d+5hjsydeiuIrRLJHew3n7MB7dhLP52WghVd9UopQa3hpneOuTn6Q3oVlOsWTEYEE9SeDVR6fHccZlcMZl/seN5WTWKgqulufjann+qdutLEXnew9Pi87QoANSomE5fPgwRUVFJCWV/TvMbrdTUFBAbGwsVquV3r1788wzz/Df//4XvV7P9OnTOe+88+jYsfbqc1V2TuXyqDjc3mW+Npcbv+W6VVDT8ymQOZUQQghRWyQoVQ5N0xi/cBObU/Pr9L7dm4cx77ruNTaRuuaa60lMbAFARsZRHnrocfr06QfAFVdczTvvzGPv3t3lTqCSkjpw443ejIvx4yeyaNHH7N27m65du9dI3+qc6kFXnF5maU5VKLYsrL88gaK68ES2p7jPfwDQFaUTvux6DDk7fecGbf+Eorx9ZYMWqofQ7yf7ZRJZdixGX3CYonMf9R7QVEJ+fpzgv9/xu1QzBIPbhmXn5zjaXUzI7894mzRa0RcfJfiv13zneoKb4Ui6FHfsmRjTN2I6/DuqMYSCIbMw7f2OoO2fYP35cYxHN2FJ8RZ3NaRvRHEWoJlCsWz5gKCNczCU1FJSLZF4wlpjPLoRAFdJVpAhbb03Uwdwh7ehqO8UMAZR2OzY56Rg8Eu44npiyPibonMe9gXfjg90eKI7Uhz9wEnff3ez7uRfPI/ITy7CdPg3wr66GdOBVd4lXdY4irqNx9blJjRT2Ul+Uf/7Me/5Bn3BQQCcrS7AExxH0PZPACg859HKB6RqmqJDDWtZ5pit5604W12A4ehmHB2u8Gb5nMDZZjA5o7/EkL0TR9Klp1fQXW9G0x9bfqeGt8ER3qb67Qkh6kxWlrfGW3h42UDm119/zdSpU30Z6DNmzODZZ59lwoQJOJ1OBg8ezCOPPFLmuppSH3Oqmp5PgcyphBBCiNoiQakKNIa9uhISjgVgevXqw9atW3jzzdfYv38vO3fuICsrC1Ut/5vLFi2O/ZIcEuL9Jd/tdpd77mlR3d5lUVWZOLqKS86v5NIYTSPs29sx7/6K3CsWeZdilfK4MKb+Bo7mYD7jpM1Yti9CUb21X4I2vIntzDEoziIill2LPn8/npA4ivve483K2TyfkHWz0IxWbN3/BTojurx9BP/1mrfuks5A0VlT0dmzCf5rNsEb5+CK740r8Wysv/4Xy/ZPAbB1vh7NFIYa3Ax78jWE/PEcQVs/IGzFRBQ03NGdyLlqOebdX2I88if6gsMYjm7yBqk2v+3X/8JBM3G2G46z7UUoqFi2L8KyfZH3LdIZUVQXxtS1OJufhfWXaSgeh3d5lCkUnT0HnT0HTW+mYNBMHB1HAaDP2Ir158cwZG6l8MIXji0TO56ilFlSVl2eiHYUDphG6I8P+Oo52ZOvxTJqFrY8d8VLiIzBFAx+gbD//RtnqwsoGPwi6M3e2lfFGTg6ja6R/tU0T3QnPNFlf8HxPycZT3RyHfWocUjLt/PSj3tIySzi6UuS6RgXmNkKmqbh9GiYq7nLWXnybC7259jILHTQPTGc6JDqFz2ujq1H8vl8cxptooO5rlciBl1j+Ne45p1Y4qB79+5ljpUaNWoUo0aN8j0ODw/nmWeeqdX+nagxjGJAzKmEEEKIACRBqXIoisK867pXffmeXteglu+ZTMd+mVi+fCmvvPIil156OQMHXsikSXdz110V154ob5eZmt6oUbHnos/fX7I1eUv/5WsVcTsw5KSgKTpc4ZXLbDGnfIF591fen3d/5Q1KaSrBa18gaMsH6OzZ3qDR//2EJ7SV/8Wq27sMTtOwbF0AeLdr1zkLsP78GMbUNegLU/GEtSL3so9RS5YpqcHNsP7xLNbfpxPy+9NoJiu6ku3UNRQKhrziXWJUco/gjXMI++5O0DwoqhtN0VFw4Ys4Ol3t152is+7HvPtLdPYcAArPnw7GIBydRh8LrHicmA78hGnfSvT5B9AVpeFIuvTY/RSFggueRZd3ANORNTjaj0A1hxO07WOMh38Dt827O1xYa3Ku+RrNGILx0K+YDq7GccZluI/LgvLEdiFv1GJQPadXP6cK7J1vwHj4N8y7v6LorAex95yIxRgEFJz0OlfiOWTdstmvn67Wg2q5t6Im2V0evt6WzuYjBYw/qxUtIqpW/8npVlm0MZU5v+3D5vL+XT156RbeuaEncaGV335+T1YRr/y0lxYRFu46vx2mGgwKVSSryMljX28nKTaEuwd6a/889e1OvtyazuVd47n13DZEBZtQNY0DOTa2pRVg0uvo1zoCTYNPN6by655sEsMtdI4PJTHcQlSIidaRQYQHGckudjLz+92s3Jnhu2eQUccNvVswuIO34POBHBtr9ueQZ3Nzba/m9G4ZgcPtYfXuLOwuFatZz96sYn7Zk02+3c2Izs247Mx4rOZTTzW2pRXw8o+72XD4WDbN9zszuLRLHD/vySaz0MlV3RMY2SWOL7am88GfBxnZJY5/nXWKpaGi3lVlTqVqGjuPencZPSPWir6aQcnaWL7X0OdUQgghRKCSoFQFFEUhyFi1X7INBh1ud8P8PnDp0sXcfPN4brhhDAAFBQVkZ2fV66RIKSnurDgL0GfvRLXGo5kjThrc8G5Dr3m3li8pHq0UHyVoz5fYzhxbJlNHsWVjLam7BGA8/DsApj0rCFnn3YlNQ0FRXQRtmOMN8pQwpK0n/MsxuOJ7Y0++DkPeXlSjlcJBzxH27e1YdnoL77sj2pN3+cd+SwNtvSahqG6C/5qN4rZ5l8XpTLjjulPcfTzO9scKnBad9SDG9L98u7S5YrpQ3P8+nG2GlHn9miWSwgGPE7bybmydr8fVvH+Zc9CbcLYdirPt0ArfR/Rm8i79EFPq7zhbDMC8+ytfUEpXlAaAo/3Fvpo2rlYDjxVGL08dBaQAb1Bt6GsUXPAcmEKqtmKtLvsZgDyqxrK/j7DlSAH/OrsVieGVC/r8lJLJh+sOcW7bKG7q2xK9TsFdUmPPcsLfoymZRbz84246xFq5bUAbv+eOFjj4J72AzCInmYVODuXZOZBjw+VRsZoN7M0qJtfmzVbcllbAOzf0IMRkIC3fjsOt0iIiCJ0C6QUOcmwu2kWHYDboKHS4+WFXJm//vp/UfG8R++7Nw8h3uNmbVczkz7fw3GWdKwxybU7N56eULCKCDNhdKu+uPYDT4/27c3t6Ic9d3pmoYP+MIk3T2JtdzFdbj7Lin3SMeh0392/JJZ3j2JdtY1dmIWaDnlCznjNirEQEGyl0uFm+NZ2cYifj+rUi2OR975xulQe+2Mam1HzWHsilmdVMRJCRL7Z4d0v7fHMa/9t2lGCTnkKH29c3AL0CRr3OFxDYmlbAtzuOBZ4UoFOcldQ8O3l2b+ZGM6sJi1HPgRwbb/9xgLf/OFDmPflhVybntIlkZ2YRmYXOct+3HUcLmfvbfsb2a8n1vRJ9n4XcYhe/7csmvcBBXKiZ3ZlFfLjuEKoGBp3CwKRo1uzPYcuRArYcORZsnv7dLmat3kOhw1PyeSn/vqLhqeycStM0zAbveRajDoOu9gO+1dEQ51RCCCFEoJKgVBMRHh7OunVrGTBgIMXFxcydOxu3243LVU+Tek1FcXq/DdV0JhTV6S3EXZCKGhTlDfCcGG1QVRR7tu+h4rYBELL+Naz/fIjOluWtWQToCg5j2v89lh1L0NmycEe0w5C7B0P2DpTiDMx7VgBg63wDzqRLCP/iRiz/fEJR33vQgqJBdRP64wPoHHmY9//g2ynO0eFKHEmX4to8H2PaOtzRyeRe9jFa8AlbRysKxX3vprjPf1Ds2eiKM/CEtyl/hzy9kbwR72BO+RJX8354ojqc9K1zdLyarOZno4bEV/bdLp8xCGfrCwFvFhGAIWMLhpzd3vu0G3F67dcmRQFTSJ3eUtU0nO6yQZbyaJpGVpGTYpeKzeUhNc/OoVwbneKs9G1Vtgi2W9V8y5Q0TWPN/hxUDc5pG3XKe+UUO8mzuWkdFeTLDEgvcLBmfw7rD+ZS5PBgMeoINukJMuqJCDJyfvto2sf4v3+7Mgp5+rtdviDAD7syuX9wEme3icRk0BFs1JfJPEjLt/PCqt38mOKtZ7PxcD6/7cuhYzMr/9uWTqHTQ8/EMM5qE0WryCAyCp28snoPDrfKmv25bDicx22DzmDlllTW7M8lNc9+ytfbPMyM3a2yN6uYaf/bQWSwkaWb09AAs0GHSa+jwOENrpj0Cq2jgtmTVYynZI/5WKuJiee05tIz40nLd3DzRxvYlVHElW//SavIIM5uE8k5baOICzWTVeRk2d9pfkGcUr1bhrPjaCGbUvO54f2/GN0jgXPaRvHXwTz+2J/DtrQC8u3+y3Oe+nYXz65MwX3CfvcKcEZsCIfz7BQ5vQGXn3dnM/PyzsSEmHh+1W42peZjKAn2vfrzXkx671hc3jWenUcL+Se90Bd4Mht0dGxmpaAk6OZxq5wRG8LVPZqTW+xi+9FCMgodZBY6SStw8E96aWZKCI9f1JGOcVY0TWPVrkzeWXOQo4XeQF5ksJG+rSKxuTx88Xcav+3L8b2nLSOCKHC4iQwycm67KMwGHQv/Osy+bBuv/7KPJZuO0CYqmIwiB3syi8vdW21Yx1j+M7AdzULNpOXbmfXTHo7kO7zt6XW8s/YAhQ4PISY9E89tw+juCaf8vIjAoigKCiV77zXg+E6Dm1MJIYQQAUyCUk3Ef/5zL08//QTjxt1AZGQkgwcPxWIJYufO8mtQ1DbFVYyiedB0BjzRHVGKM9HZs1E8DnS2TNSg6DIBHMWR482QKuUqBk2PMXUNgDeo1P9e9PkHiVh0CTpXEQCazkDB4JcJ/fFBDFnbMB1cjam0HlHHq/E07wvNe6Gk/kXQ5vkU97+PoL/fw5C1HdUcgWYMRl+YCoCty/+BopB38VuY9/wPxxmXnXyHLEVBC4rGExR90vdDs0T47TR3KmpoYqXPrVR7IXG4I5Mw5KSAuxhPSDzuuB41eo9A5HSr/Hkglx9TMlm9O4ucYhf920QysnMcYUEG3B6NLgmhflkyfx7IYc6v+9lUTlFfvU7htau60qdVBCv+OcpH6w/5MlSSYkLo1zqCPw/ksivD+9m9sls8UwYl+WoGZRQ6+HZ7BvkOt3cL+NR8/jqUh6pBQpiZHonhbEsrYH+O7aSv6/Vf9tEhNoR/ndWKQWfEsHp3Ng9/9Q8Ot0qISU+LiCB2HC3k8f8d+/sh1mqiZ2I4neNDaR0VxN9HCliw7hAOt4pepzA8uRk/7Mxgw6E8NhzK81237mAe6w7m+d2/Z4twUjKK2HKkgEkf/eU7rlMgKSaEhDALUSFGmodZaBUVTJBR5wtG9GsdyT9pBUz4ZJMvGAbeQIzDrfr6E2LSk293+97LNlFBjOwSz7U9m/sCi83DLbxyVVde+nE3Gw/ncyDHxoEcG59sSPXrrwIM7hCLXgd5NjeDO8Rwedd49mfbmLJsKwdybLz5637e/HW/33UmvULfVpFc1jWetHw78/84QJ7dTZBRR6dmVjwa5NpcHMixsbOkn22jgsl3uEnJLOKad9fh9mhoJe/NC1d04aut6Xy7IwObqtEzMYypQ85AUWDX0SIUBaxmA82sJgx672fmUK6NfLub5DhrucuZMgodrN2fi04HQzvE+q5TFIULO8RyYYfYcj9DV3VPYPmWNC7skkDPuBD05bR9ZbcEvt2ewaur95BW4CCtwOF77ozYEJJiQsgodODyaNzUtwUDk44F9+PDLDxzaWe/9i47M56f92RxdtsoYuq41pWoQyVRqQYck2pwcyohhBAikClaE8s1zswsKFMM2eVykpV1hOjoBIzG6k90vcv3ql9TqlHSNBRb5rFt0hXvLzy6wiPoio+iWiJRw1r5ztXn7kFxFeKxJvpnH2ka+pxdKG4bqjkSnSMHh6Ynw2XkjO/HYSn07qqWP+x1zLu/wrz7K9yRSTiSLsPRbjiemM6E/DKN4E1v+YIvalA0WeP+QtHriTn6A3w6BtUcjq3bLQRteguds4CCC57F0fYiQn98EI+1OUXn/7eu38E6Y/3pIYK2vA9Acdeba/21Hi1wEB5k9CvSrGkaOTYXeTY34UEGwi1Gv5oiHlXDrfoXdlYUiIkJLffPdk6xkw2H80nLt3PhGTHEh5WTqYa3uPMfJVkfigL7c2zsPFrI2v25FLs85V5TKsioY1y/ViSEm1m04Qh/H/EGo3QKBBn1mA064kLNaBpsP1pImMXAxcnNygQ+TmzT7lLRgLbRwXSIDcHmUvl1Txaecv7GNukVvyVbOgWS40Lp3zqCuFAztpKMLZtLZW9WEb/vy/Fl63RvHsbfR/JRNTirTSSPDutAVIiJd/44wIL1h3yZOxXp2SKc+wcnkRQTwsEcGy+s2o3FqOPSM+NpGRHEb3uz2XQ4jyP5Dgocbi47M56b+rbgSL6dx77eQY7dxVmtIjm3bRTdE8MqVX8IYNnfR3j6u120iw7h/sFJdGsexqFcG06PSuvIYIx6hQM5NnZnFnFGrJWWkSdfiljocLP2QC6/783mj305FLs8RAUbaRMVzPizW9OxWfnF0B1ule93ZrBoYyq7Moro1jyMAe2i6NUinPYxIRj1xz6rxU4PaQV2WkUE+YI/AJlFTv46mEtEkJE+rSLILHRy/xfb2JrmzVoLtxiYdF5bruyWQJHTzb8XbiKryMm7N/YkoYLPdF042Z+949ldHn7YlYmqaUQFm2gXHVzhn8WGovS1iYrV1nxqe3oBquYNUNdFvTZRc/Pgyv6dIBomGb/AJWMX2Brz+FV2PiVBKSQoVZsURz76vL0AaHoLamgimsmKPnsnituGJ6yVN1hVQleUjq4oDdUc4SsaDoCryJvFgw53TDL6rO24PCoZRU6SfrkTsz0TxW3DHd4GQ94+NBRyrvvOb+cy095vCf/6Ft9jW/K1FF74gvcPS1Qw7lf6YsjdfeyWzbqTe9UXTaYOkSnlS8K/8RZqLbNLYSW4PCqbU/PpHB9aYe2QfLuLzzenseKfo6RkFmEx6OjfOpIgk57dmUUcyLHhOO7PkF6ncE6bSK7slsCujCI+/uswBQ43vVqEc0FSNCM6xxFqMRAWEcKSNfv480AuGw7lkVHoxOlR/doyG3T8X58W9EgMw+HWaBlpoV10CDuPFnLP0q2kH5fFcbxYq4mB7aO5ICmGuDAz/9uWzq97c9A0DZvLw8Fc/yVnRr3ClV0TGNe/JbHWY8Wz7S4Pty3a7FcjZ0zfFlycHIfVrGfD4TzWHcglMTyIq7onsC29gEe/2u6r81OqZ2IYHUoCJAlhFi44I5roYBO/7cthR3oBHeNC6dMynDBLxRsH5NpcfPzXYd5fe9AXnLrszDimDu1QZrczteR1bk8vZMOhPHZnFrE/x4ZBpzCufysGJUVXu6Dw6f4jnFvsIizIgK6GCxo3BB7VW5MqOthIRJDR7z12qxqqqtX7L+wyiWraams+teNoIR5Vo31MsK++lKhdEpQSIOMXyGTsAltjHj8JSlVAglKnQXWj2HPRLBHeHen8nvOAppbZQU+fuwfFWYAvHx8FT0g8+qIjALijO/tf4yzCkJuCphjwxHT21ZXSFaahK073Bav0uXtw2QvJzC+g/e/34up9G9afHkYpSfi3dxhFwdBX/PqiOPKIfrsriuYdo7wR7+BsO9T3hyU7ZRPmXcvRFRxCcRRQ3P9ePJGV2+GvMVDsOUR9eB5qUBQ51686aTDOrWq88cte0vIdjOqeQHiQkce/3s7OjCKiQ0z866xWON0qK3dmoFMULk5uhk6n8MYv+3yFqk/Gatb7ihmfTKjZwIUdYvh9Xw5HKwgqlf5isy2t7O58Z8SGcCjXhs2lEh9qpkVkEB6PSvOIIM6ICaFHYhjJ8aEVBj1UTeOb7Ud56/cDuDwql50ZzxXdEipcWpRZ5OSWjzZwtNDJ1CFJXN715DVxMgsd/LYvhwK7G6dHZUC7KM6ILT9jpzr2ZhXz1u/76RRn5f/6tKjx3apOpTH/I9wUNObxk6DUqdXWfGrn0ULcqka76OBK1fATp0+CUgJk/AKZjF1ga8zjV9n5lNSUEpWmKz6KrjgDzZaFJ7K9X2BKl38AnasQd2QHMJRkh7gdvoCUOzIJnS0TnT3HF5DSDEFlglgYg0DRoWhu7+56JbvpedsBzeT9UKtGK9hLCqXrzdg7jca0byXm/T+g6QwU9ZtSpv+aORx3zJkYMzajGYJwthzg97wa0Y7iPv857fcpUGmWSLJv+NE7riUBKU3TOJLvIMxi8C2rcnlUHvt6Oyt3ZgLw7Y6MY4Vp8W5d/9z3KX5tbz6uvlLb6GBu6JXIhR1iOJLn4Ld92aiaxhmxVtpGBRMXasZk0OFWNfZnF/P55iN8uz2DGKuJm/q2ILlZKL/uzWbp30fYl21j2d/e3QJjrSaGdIilZ4tw2kQFYzbofP0uLdq8YP1h7C4Pep3CrowiX72hfq0ieObS5JNmF5XHG3CL4+LkuEqdHxNi4pNxfShyeipVEyfGauayM0+zoP1JtI0OZvrI5FprXwghqqoRJj4KIYQQ4iQkKCUqTXF6f4FXPHb0uXvxRLTzBi80DZ2r0LujnqsQrSQopbN5ixCrplAwBqMaWoLOgK7Yu5NVaYDJ/yY6NGMIirPA25YxyJuhVbLTnmbyZoloxmM7h7ni+4AhCFvP2zAdXE1xz9v8l/4dx9XiXIwZm3G2ugAMldvuvjbl210cyrXTOb5uvpF3e1Rmrd7L36n5PDa8A+2i/XdgO76OV77dxYPL/+HPA7kAhJj0JIRZ0NDYnVmMUa8wKCmGn3Zn4XB7s3geGJzETylZLNqYSkSQkeHJzXB6VL7Ykka+3c2NvVtwbc/mvno6YRYjHePKz/wx6BTax4Rw74VJ3Huhf8Zam+hgru+dyI+7Mvl9Xw7nJ8dxdmJohduHl1e0Oc/m4vtdmTjcKqO7J/jV+KlNQUZ9pbZGF0KIpqyRfVkshBBCiApIUEpUjqYeCwwpehR3MbrCVNSwlqC6vEv38O6qpwVFg+pBsWd7zy/deU5RUEMS0BQDOkcuqqX87e5VoxW9swDFVYRGLIqzENDQ9BbQl2SXGIMAbxChtPaRK/FsMifsBF3F2S7FvSaBomDrMuY035DTdyjXxq2fbia9wMGca7vRq0XESc/PKXay/WghXeJDT5nRs/5gLrN+2kN0iIkzYkPoEh9Kx2ZWnl2Zwq97veMyadHfzL22e7kFoA/l2rh7yRb259jQKaBqUOT0kJLpDUyaDTpmXt6Zs9tEkVvs4mCujTMTQlEUhWt7JXJtL//dAW/o3aIK70zl6EoCTYM7xlYr5TU8yMiobrKlvBBCNCRKSe5tE6suIYQQQjRZEpQSleOyARqazoAa2hJ93t6SYBEoHqfvNMVV7P2/Iw9F86Dpzf4ZUYqCFtIMT0izCm+lmaxQhLd9TfPdpzRLytuODjUoAq3IjrP5wGMfZP3Jl0RplgiKzn6o0i/7VNwelTUHcjEoCq2igggy6nG6VbKLnRzIsVHs9DCgfTQxISYyCx2s3JmJQacQF2rm2ZW7OFrofe++2JJeJii1OTWfnGIXZ8SG8OeBHF5ZvZd8uxuTXmHQGTHcem4bWkSUDSjtySri3mVbfTWZftmT7fd86W5wB3Js3L5oMzf1bckZsSF0jg/FbNCx8VAe9y7bSp7dTVyomZeu7EKLiCDS8h2kFdjJKHDSPTGM1lHBAEQEG4kIrtqyNyGEEKI8pcv3JCQlhBBCNA0SlBKVori9wSbNGIJm9AYjFNXpLXB+fFDKY/cut3N6awhp5oiqF4gwBHmzsTSPdxnfCfWkSmnBzdBCPGjmsGq+qtNT6HDzwBfbWFuyvK0i+pW76Bwfxrb0Ajyq/zQ7JsREZpGTVTszeXBwkq+o6w+7Mnnwi21lJuWhZgMFDjffbM9ga1oBH/xfL6xmA2n5dnYcLUSnKDz/QwqFDg/dm4cxrFMzdmYUsjk1n71ZxYRbDLx45ZkkhluY8MkmDuTYmPmDt/6T1azn7DZR/JiSicujkRxn5cUruhBTsoNc2+hg2kYH18h7J4QQQpSndMYgiVJCCCFE0yBBKVEpisu7bEszBIPOgKYzoKhu8DhQPI4Tzi32ZTep1QkYKQqaORzFno0+bz+gAopfHam6pGoar63ey/4cG1azHqvJgNViYHVKFimZRVgMOuLDzBzKteNWNfQ6hTCzgVaRQbhUjW1pBfx9xBuk65oQRpjFwK6MQuJCLcy8vDM3f7SBI/kOVu/OYlinZmxNK+Cxr7ejAfGhZjKLnJj0Oiac05preyWy82ghDy7fxqFcO0+s2MHApGieXZmC47idH1tEWHj+8i5+GUy5NhcGneIrWD732u4s2pjKjqOFbEsrILvYxXc7vPW+Bp0Rw5MXd5Sdj4QQQtQpKXQuhBBCNC0SlBKnpmm+ZXmUZEmht4Ba6M2M8mVKeetA6IqPepfu6YzVLiauhiaC6kHnzPN2wRji2xGuri3akMoH6w6V+1xMiImXruxCp7hQPKqGhrdA9/H2ZBWx7kAe3RPD6NisbFHvizo14921B1nxz1HaRYdwz+dbcLhVzmkbyQtXnImmaSiK4mu3c3woz4xMZvzCTfyYksWPKd6C8q0jg7AY9UQFG5kyqH2ZJXURQf6Po0NM3HpuG8AbeNtwKI/vdmSQEGbhpr4t0MlvBkIIIeqYUpIrJTWlhBBCiKahbrabEjXu9tvH88QTj5T73Lff/o/hwwfhdDrLff7IkVQGDOjDkSOpAAwY0Ie//lpX7rl//bWOAef1RVFdgIJWEmQq3WFPcR/LlNJMIXz/25/kZBwBYN6nX3HHnROr9wIVHWp4K1RzJACqObx67Zymw3k2Xvt5LwCjezTnrvPbcnP/lozu0Zwbeifyzg096BTnXVao1yllAlIA7aJDuKZn83IDUgAXd/bW1/ptXw7jPtpAdkkdqadHJmPQKRj1ujLtdkkI4+6B7QBvKHDCOa359OY+fHhTL165qquv3lNl6RSF3i0jeHDIGYzt11ICUkIIIepFXdeUqtP51IA+le7XDz+sJCfHWxPy7bfncMcdEyp9rRBCCBFIJFMqQA0ZchFz587G5XJhNPpnwPzww3dccMGFmEwnL/pdatmyFYSFnTrooxksvmwlTe8NSuFx+DKlUnOdPPz8myx5cwYA198whqt1lsq+pLIUHWpYS1Q1/qQ76tUWTdN46ttd2N0qvVqEc++F7WslWNMuOoQOsSHszCjCo2qc1TqS/47oRIjp5H88r+nZnBiriWZWM12b109dLSGEEKI21FWiVH3Mp04lLe0Ijz32IIsWfQHA9dffxOjR1512u0IIIURDJJlSAWrQoCHYbDbWrVvjd7yoqJC1a35n2IB+lZ7RRUfHlJmIlcevppPBG2xSnEUomgdQUI3HZwLpCAqLPf3JmaJ4d9Srh8yd99YeZN2BXMwGHY8M61Cr2UM39W1JiEnPv85qxcujzqzUbnaKojC4Q6wEpIQQQjQadZ0pddL51No/GDp0eKXbqux86lROXLoYHBxcI8EuIYQQoiGSTKkAFRkZSZ8+/fnpp1WcffYA3/Gff/6JsFArbaLNPPLQFNZt2IDDYadt23bcffd9dOvWo0xbAwb04ZVX3qRXrz4UFRYw89lp/LZmLdHR0Vx20RDfeZoxmM2bN/LGG6+yc+d2FDR6dunIw7ePIzqmGaOvvRKAUbc+wMN3305q3m9s2LCe116bC8CWLZuZPXsWu3btIDIyihtvHMMVV1wNwPTp0wgLCyMjI4Nff11NeHgEEybczvDhl9To+1bkdHMgx8YZsdZyl9qVWrUrk9m/7ANg8gXtaBlZvdpYlTU8uRkXdYpFkWVzQgghmrDSmlJ1lSp10vlUWDitWrXmkUfuZ926P6s2nyoq5Lnnnua3334hOjqGyy67wu9cv/mUotCjRy8efPAxYmJiGD36MgBGj76Mhx56nCNHUhvcfEoIIYSoKZIpVRFNA1dx3f5XxQnYkCHD+OWXn/B4PL5jP/zwLUPO7cO0WfNQPS7mzHmH+fMXEBvbjBdeePaUbc587ikO7N/D60/ex5SbR/Px4iWAt6ZTgUvP/fffTb9+Z/HB+5/w8uP3cejIUd5b8jXoTcyb9x4A82c8wuBhI/3a3bdvL3fddRs9evRi/vwPueWWCbz22sv89NMq3zmLF39Kx46deP/9Txg48EJmznyawsLCKr0nFVl3IJcxH/7Fha/9xpgPN/DSqt2+5zYcyuPzDYc4WuCg0OHmi7/TeOzr7YC3jtRV3ZvXSB9ORQJSQgghGqUqzKn0nmJ07oYwn1rJ4MFDefLJR/F41KrPp2Y+w4ED+3jttblMnnwfCxcu8D1XWFh4bD71wae8+OJrHDp0iA8/fAfAN5+aN+89Bg8e6tdufc+nhBBCiJommVLl0TQillyJMa38YpW1xZXQl9wrl1R6qdrAgYOYOfMZNm3aQK9efSgsLOTPP9cy4ekHSYiNZuDAC4lp3QaAUaOu4b77/uPfgMft97CwsJBVP65i9pP30TGpPSg6brkul+fffAc1vA2OrEzGjh3PddfdiKIotAyyMejs3mzbtQdNbyYiwptNFNqmF6awZn5tL1/+OR06dGTixEkAtGrVhn379vLRR+8zcOAgAJKSOnDjjWMBGD9+IosWfczevbvp2rW7X1uaplUprT/f7uLB5dvIsx97vYs3pXJ1j+YUuzzc+skmPCUNGnQKbtX7oH/rCO4Z1L4KdxJCCCGEnyrOqWJr4JY1M5/6g1tumUBcXAIXXHAhzZrFARXMp05QWFjIqlUreeWVN+nYsRMA48aN58UXvTU3HQ6733yqefNELrjgQv75ZysAERGRvv+bzf61OWtyPiWEEEI0BPUalHI4HDzxxBN8++23WCwWbrnlFm655ZZyz/3ll1947rnnOHjwIN27d+exxx6jXbt2tde5hpi1omkoriJQnWjGUIKDQzjnnAH8+OP39OrVh59//pGEuDg6tW9D+1Yt+Pa3v9i8ZAX79+9jx47tqKqKYs9Dn78fAH3ebkiI9zV/8OB+PKrKGW1boplDUcNa0bG3Cni/uYuOjuHii0fyyScL2LVrJ/t372DX3n1065SEpjcBJUEfXdmP1b59++jcuYvfsa5du7Fs2WLf4xYtWvp+Dgnx1qdyu/0DZ0UON4fz7Bh0Cs1DK/fxfev3A+TZ3bSNDubVq7oy8/sUftqdxcs/7SY1z45HgxirmaxCB25Vo01UECM6x3Fdr8STLvETQgghRCU0xDnVccqdTyU0p1OnZNq3T2Llym/YsmWz33zqZA4e3I/H4+GMMzr4jiUnd/b9fOJ8at++vaSk7KxU0Kim5lNCCCFEQ1GvQannnnuOLVu28N5775GamsoDDzxA8+bNGT7cv6jkrl27mDhxIhMmTODSSy/ls88+Y+zYsaxYsYKQkJAKWj8NiuL9hs1tq9JlBoMOt/vkE5WTNxBU4cRNsWWhKzqKopZuS6ygmawMHTyEl195icmT7+eHH75j6AUDUFWVu554gYKiYi4cdinnnns+LpeLhx++D33+Pm9qO4Cmothzy95Mw7fbncFwrGBnRsZRxo+/iY4dk+nTpz+XDb2AP35dxZade0BvxheUKkd5O9d4PCoez7H3y2AwkF7gQNM04kK9u/sVOdykZBRhMeow6XVkFTvRNHCrGjk2Fye+W3aXhzX7c1m7P4eIICOd40P5dKN3q+YpF7QnLtTMHee15Zc9Wfy2NweA6BAT300+n+zsQvJsblpEWGQpnRBCCFETqjinSsu3k2tzEWs1ER1irt49TzKfqsjQocN5+eWZvvnUkCEXoaoqkydPoqCggMGDh/rNpyrj+ILlJ51PXXYlv/32C1u3/n3KNisznyqv2PqJxdOFEEKIhqLeglLFxcUsWrSIefPm0aVLF7p06cKuXbtYsGBBmaDUxx9/TM+ePfnPf7zp0vfddx8//vgjy5cv57rrammLXEUBY3DVrjHoQDmNoFRFNBV9wWFAQ1P0oDehuG0ozgLO7daOZ2zF/PXXOtav/5PJ465m78FUNmzbyf/eeZnQNr3AGMySJYu8TWkaKHpf0zpHru/nVq1aYzAY2Jayl17NvMvWdu3a4Xt+9epVhIaG89xzLwOgOPJZsuRT0DQ0vQlFKa7wJbRq1ZqNG//yO7Z162ZatWrt61ex00NWkTfoVhoUyixyEuVRcR432Qo26Sl2esizuQj2qCzaeJhFmzPIs7kpdLjLXdp3Xrso+rfxpsO3iQ7mim4JLN50BIBHL+pAZIgJj81ImOX0d80RQgghxHGqMKfSjDpUlwvVYAJjNYNS1XD22efyzDNP+OZTd901hX379rBx418sX/4dkZHeOYTffKoCpfOpf/7ZRp8+/YCTz6cAPvvsE9/PJ/ti7FTzKSGEECLQ1Fuh8+3bt+N2u+nZs6fvWO/evdm0aVOZtOiDBw/SrVs332NFUejQoQMbN26sq+7WL9WDN31JwRPdGU9UBzzhbQEwq3bOP38Qr732Eu3atadls0hCQ4LR6RS++2Ut6Yf3s2rVSubPnwOA0+X2ZUGBgnLcN5chIVYuHnQeL7z1EVt3pPDXX+uYP3+u7/mwsHDS09NYt24thw8f4oNPFrHqj/XeNvUmLBZvTamUlJ0UF/sHqK68cjS7du3kzTdf48CB/Xz99XIWL1nEhRdfweE8G0VODy5V832xWRqc0jRvECrWaiLYpCcu1EzryCDCLAY0IKvIxacbUjmUa6egJCCVEGbmqu4JnNM2Er0CwUY9d1/gXxtq4jmt6d0ynH+f3YoB7aJqZJiEEEIIcXrqK0/ZZDIdN59KomXLVlitoeh0Or7//hvS0o74z6eczgrbCgmxMnz4Jbz88ky2bt1yyvnUhx++y08//eBrszLzqTlzZnPgwH7+978vWbJkEaNGja7pt0QIIYSoE/WWKZWRkUFkZKRfGnJMTAwOh4Pc3FyioqL8jqenp/tdn5aWRnh4eJXvW96XTw1+pZbqXRan6Qyg88YRNVMommJA0dwMvWAgX3+9nDsn3YWieWgWE829k27jnQUf88aCJbRs1Yb//OdennrqcXbu3U90bEJJG9Yyt7rn3//Hi3Pe5e4HHiA0NIyrr76O2bNfBuDCC4eyadMGHnnkARRFITm5M3f++1+89cFHON0eIiIiuOiii3nssancdtudvjbtLg+ERHDXQ0/z6XtvsnDhAqJjm3H1mNvoOWAYeTY3blVDAVpFBFF4XMaUXgeJ4RaMep1f8dO4UDNFNjsAUcFGJp6XROe4UKwWA9HBRt+3jLnFLjyaRnSIf7p7VIiJOdd6azeUjn+D/xyIcsn4BS4Zu8DWmMevMb6mgFHy3tfHarOhQy/yzqfunAxAs2ZxTJnyIO+++xZz5symZcvWvvnUrl07iI6OqbCtyZPv46WXZjJ58iRCQ0NPOZ+64467efvtOTidzgrnUwDx8fE899xLvP76LBYu/JC4uHjuuGMyl1xyWa29L0IIIURtUrR6WmS+dOlSZs2axapVx7awPXjwIEOGDOGnn34iPv5YAe7Vq1dz++238+qrr3LeeeexfPlyHnnkEfr378/8+fNPuy92u53du/cQExOPyVR3qeKVZs+H7N3eGgnNOh07nnsAirMgpBmEJ/qfZ20GufvBFAoxSd7zi7Ig7wBYwiGqHdhyIGeftx5UXEkBztRNgArNuoChbN2CqnJ5NLan5Zc7udTrFCKDjZj0OlQgzGLEYtShabA/q4hCh4fW0cGEWsqPneYV2jiakUpSUjuiwsoG2IQQQghROzIzC8r82+5yOcnKOkJ0dAJGY/XmEOkFDrKKnESHGIkLtZz6AnHaamLcwBvMjYkJLfezIRo+Gb/AJWMX2Brz+JW+tlOpt0wps9lcJvX5WNqy/yTk/PPPZ9KkSdx55514PB769+/P5ZdfTmFhYZXvm5VV/iRKVVU8Hu20CpWfdqHzCiguJ3q8mVKe49pXjKHoyUKz5eIJiUfnKEYHqHozqmLGAGiuYjxuD6Cg87i8z6NDdatgsGJQdOBx4LEVoOnNGPC279Z0UAOvpcDuQtPAqFeID7NgMehwuFU8qobVbEB/wu52pe9fiwgLqgo6nVLhe2rSKwQZdTiKHGQ6q/8nWFEgOjq03M+GaPhk/AKXjF1ga8zjV/raRN0rnRU0ts+UEEIIIcpXb0GpuLg4cnJycLvdGAzebmRkZGCxWAgLCytz/m233ca//vUvCgoKiI6O5j//+Q+JiYlVvq+mlZ3oNPiJz/HL946jGUNB0Xl35HPZweNdzobBAgYzoKBoHvC4QG+C0lpdpYXOFT2qKRSdIw+cRWAuWRqoGECpmXJjNpf3nlazgVCzt/9GfWXaVkpXKp5SeWNaHTXVjqgfMn6BS8YusMn4iZpUunRSPlJCCCFE01Bvhc6Tk5MxGAx+xcrXr19P165d0Z0Qjfjyyy+ZPn06JpOJ6Oho7HY7a9asoX///nXc6/qhlASlOCEohU6HavJ+k6uzZaK4vEXLNUMQKDo0vXcpouIuCVZpnpIGj+2+h8FbTFPx2FE8JffR11ys0u7y3tNiqLePmhBCCNFoOJ1ORo4cyZo1a3zHnnrqKTp27Oj334cfflhhG++++y7nnXcePXv25KGHHsJms1V4bl2TTCkhhBCiaam3SEFQUBBXXHEF06ZNY/PmzaxcuZL58+czZswYwJs1Zbd7gylt2rRh4cKFfPvtt+zbt48pU6aQkJDA+eefX1/dr1taSaaUUjZYpJm8WWU6ezZKSaaUZrD4/Z8TglLacUG/0nMUtx1Ul/egb3e+0+y2pmEvWXpnMepPcbYQQgghTsbhcHDPPfewa9cuv+O7d+9mypQp/PLLL77/rrrqqnLb+Oabb3jttdd48sknee+999i0aRMzZ86si+5XiuKrMi9RKSGEEKIpqNf0lalTp9KlSxfGjh3LE088wZ133smwYcMAGDBgAF9//TUAZ555JtOmTePZZ59l1KhRAMyZM6dMRlVjVWGmFKBZIlCDm3mzo1C8/y8NKum9/1dKglqoZTOlNH1pUMqBUhKU0mooKOXyaHhUDUUBs2RKCSGEENWWkpLCNddcw4EDB8o8t3v3bjp37kxsbKzvv6CgoHLbef/99xk7diyDBg2iW7duPPHEEyxevLhBZUuBZEoJIYQQTUW91ZQCb7bUjBkzmDFjRpnnduzY4ff4qquuqvBbv5pSTxsRntgJFEcuOlsWakgcminUV1Oq3AwmRYdqTQASvEEnRecryODLrCoJRinlLd/Tm7zXaCqKq6ji+1SD3X1s6Z6uFvbX1rSaLyovhBBCNERr166lf//+TJ48mR49eviOFxYWkp6eTps2bU7Zhsfj4e+//+aOO+7wHevRowcul4vt27fTs2fPGunr6fz7LDWl6p7Mp4QQQtSneg1KNRR6vQFQKCzMw2oNPy51vGpUVcHjOY1plMeJrvAIituGB1C1TLRQMx63G0VV8KgauJynasT3k6KCR1VQXS40lxOPx4OiKqiqhnZcOx7F7F36Zy8GFFRN8Xu+uorsThTNjVlnwFUD7ZXSNA2Px01BQS6KosNgqJkgmhBCCNFQ3XDDDeUe3717N4qi8Oabb7J69WoiIiK4+eabufLKK8ucm5+fj8PhoFmzZr5jBoOBiIgI0tLSqtSf8qZKRqMRRdGRl5eF1RqBXm+o8pxKdXvnDqpHw+WSpf+16cT5lHf8qt9e6bW18D2kqAMyfoFLxi6wNebxq+xrkqAUoNPpiIyMJScng+zs6qev63Q6VPU0vh20ZfmKlQOgt6M6dOgKc0HTUNVM0FVhguayobMVoOmdaC69tx3VjerJBH3+cfctPpYlBahuExTZq/UaNM377aZOgewiF063itllIMtR8xNLk8lCWFhUtYOIQgghRKDbs2cPiqLQrl07/u///o8///yTRx99FKvVytChQ/3OLa3VaTKZ/I6bTCaczqp9eRQdHVru8fDwJFJTUykoyK5Se6WKnG7sRS4w6sjxmKvVhqgaqzWEhISEMp+L6qrosyECg4xf4JKxC2xNefwkKFXCbA6iWbMWeEp3oKsiRYHIyBBycoqqVwdB04hYPBGdM4/C/g9iXfMsms5E7uULifx2CgDZ131fbl2pihiObiLs93txW1uQf9kCIn74FzpXEbkjP0QNS/CdZ972EyEb3/A9zrtoDp7odtV4EbBo42E+3ZDKFV0T+G5HFkVON89d2pno6JBqtVcRnU6HTqeXgJQQQogm7YorrmDQoEFEREQA0KlTJ/bt28fHH39cJihlNnuDPCcGoJxOZ4U1qCqSlVVQ4XwnJCSKoKDwan1RtyUlk1dWp9KteRiPDW9V5etF1ZTOp/LzHYDjtNpSFO8vVSf7bIiGS8YvcMnYBbbGPH6lr+1UJCh1HO8/zNX7lkhRwGKxYDS6qvVh0uXtIzh7C5rOREH7YZhX3Y2iunBmb8VSeBDVEonRHFylNvVBEd5rXfnYDAaCcnagoGEICkMzHnud+oiWWAoP+h4XhTZDZ6z4fTha4OChL//BajbQr3UEQzrE0izUjN3lYf7aI+TZPcz+7RAARr1C22bhGPVS6FwIIYSoaYqi+AJSpdq1a8cff/xR5tyIiAjMZjOZmZm0b98eALfbTW5uLrGxsVW6r6adrBi5gk5noDr70XgUA4cLPMQXqxgMNZO5I06tJn8ROvlnQzR0Mn6BS8YusDXl8ZNIQQNhTN8AgDu2CxiD8IS38R5PXQOAGlS1yaL3migAFEc+iiMPpaRsqGbyj1Z6ojv6ftYUHWpQzEnbnff7fjal5vPr3mxe+nEPN37wF1lFTn7YlUme3U2o2YC+JIEpKSZEAlJCCCFELZk1axbjxo3zO7Z9+3batSub8azT6ejatSvr16/3Hdu4cSMGg4FOnTrVdlcrxVASyXKrTXRmLoQQQjQxEi1oIAwlQSlXnHfnG094WwCMR9YCoAafPFBUHs0cDoCChj7fu4W0pjeDweJ3nhqSgFoSqFKDYk+6RPBIvp0vt6YDcGPvFrSIsJBrc/HyT3v4bOMRAP6vTwteuPJM2kYFc3X35lXutxBCCCEqZ9CgQfz555+8/fbbHDhwgI8++oilS5dyyy23AN46UhkZGb7zb7jhBt5++21WrlzJ5s2bmTZtGtdcc02Vl+/VFkPJt1ru09k4RgghhBABQ5bvNRC+TKnSoFSENyhlOLoZADW46plS6Ayo5nB0jjz0efsB0ExhZc9TFDxRHdGlrUO1xp+0yffWHsStavRpFcHdF7RjaKdYbl6wgRX/HAVAr1O4rGs8MSEmzm0bVfU+CyGEEKLSunXrxqxZs3jllVeYNWsWiYmJvPDCC/Ts6Z1PfP3110ydOpUdO3YAcMkll3D48GEee+wxnE4nw4YN47777qvPl+DHoCsJSkmmlBBCCNEkSFCqIfA4MGRsBY7LlIrwpt0rqgvglEvqKqJaovyCUqq5/EJj7qiOGNPWoQbHVdhWeoGDL7Z4t4wef5a3+GiX+FBG92jOpxtTARiUFE1MiNSAEEIIIWpLaYCp1JAhQxgyZEi5544aNYpRo0b5HZswYQITJkyotf6djtKglMtT/d2MhRBCCBE4ZPleA2DI2IqiOlEtUahhrYFjQalS1cqUAjRLJAC6/H3ex+VlSgGuFgMAcMd19/5f1bh90Waue28dfx3K5WiBg/uWbcXl0ejZIpzeLSN81942oA2xVm8ganRPWa4nhBBCiOqRTCkhhBCiaZFMqQbA6Ksn1cO7jR/Hlu+V0qqbKVVS7Fyft8/bjrn8oJQjaSRZ8b1Qrd6g0vItafx5IBeAWz/ZTJjFQJ7dTbjFwJRB7f2utZoNvHVdD47k2+nVIqJa/RRCCCGEMJRsjuKRoJQQQgjRJEhQqr54HIR/OQ7FmY/icQLgjuvle1oNjkMzBKO4i0seVy8opVlKg1Ily/cqyJRCUVBDEwGwuzzM+917fnKclX/SC8mzu2kbHcyLV3ShRUTZYqjNwy00D7eUOS6EEEIIUVlGX6aULN8TQgghmgIJStUTw9HNmA797HestJ4UAIqCO6IdxswtwOnUlPIu39MXeWtBaRXUlDreoo2pZBQ6iQ8189Z1PVh/KJctqQVc3zsRq1k+MkIIIYSoHbL7nhBCCNG0SIShnhiyvEVK3RHtUINiwWDB1by/3zme44NS1awpVRqUKlVRTalSh/NsvLv2IAATz22NyaDj7DZRnN1GdtITQgghRO0y6LzL96SmlBBCCNE0SFCqnhiytwPgbDuMonMeKfec4+tKqUHR1bqPFuQfTKqophTAxkN53PfFNvLtbs6IDeHi5Ip34hNCCCGEqGlS6FwIIYRoWiQoVU/02SWZUlGdKjynNCilmsLAUL16TarFPyhVUU2pdQdyuXPx37hVjU7NrDx/RRf0JRNDIYQQQoi6IEEpIYQQommRoFR90DQMWd5MKU90xwpPc8d09Z4T0a76tzpx+d5xNaU0TUNRFFRN44VVu3GrGue3j+apSzoRZNRX+55CCCGEENVxrKaUFDoXQgghmgIJStUDxZaJzp6DhoI7MqnC8zzRHcm9cjGesJbVvteJmVJuYxgbDuXx0o+7ybe7eenKM9l+tICUzCKsZj2PXdRBAlJCCCGEqBelmVIe7diXZ0IIIYRovCQoVQ9Ki5x7wtuAIeik555Y/Lyq1BNqSk353wFW2Y8tBbxt0Wbf9stj+rYkPMh4WvcTQgghhKiu0kLn4F3CZ9RLUEoIIYRozHSnPkXUtNIi5ydbuldTTqwhdcRhQQEu7xrPGbEhZBU5SStwEBVs5LpeibXeHyGEEEKIihiOq2cpdaWEEEKIxk8yperBsSLntR+U2pFpx6SFEKEUAXD/xT0JadaGdtEh5BQ7uW3RZnZnFjPxnNaybE8IIYQQ9cpwXGaU26OBJHALIYQQjZoEpeqBb/neSXbeqylfbTtKohbqC0p1a9cSzRwCQGSwifnX92TH0UJ6JJa/K58QQgghRF3xz5SSYudCCCFEYyfL9+qaph3LlKql5Xt/Hshh7m/7OJRr45t/jpKDd8c9DQXNZPU7N9ikp2eLcCkkKoQQQoh6pygK+pLAlMsjy/eEEEKIxq5eg1IOh4OHHnqIPn36MGDAAObPn1/hud999x0XX3wxPXv25Prrr2fr1q112NOaoys4jM5VhKYz4glvW+PtO9wqU5f/w7zfDzDq7T/Jsbko0nmzoDRTKCgShxRCCCFEw1WaLSU1pYQQQojGr14jFM899xxbtmzhvffe4/HHH+e1115jxYoVZc7btWsXU6ZMYeLEiSxbtozk5GQmTpyIzWarh15Xn2LLJnjDGwB4ItuDvuYLJazenUWe3Y1ep1A6lQuJiAVKglJCCCGEEA2YUe+dnkpQSgghhGj86q2mVHFxMYsWLWLevHl06dKFLl26sGvXLhYsWMDw4cP9zv31119JSkriiiuuAOCee+5hwYIFpKSk0LVr13rofdWZ9q0k7JvbUNzeQJqzxXm1cp/lW9IAGNu3Bf3bRLLxUD5JrlaQC5pZ6kYJIYQQomEz6kszpaSmlBBCCNHY1Vum1Pbt23G73fTs2dN3rHfv3mzatAn1hElIREQEKSkprF+/HlVVWbJkCVarlVatWtV1t6vNvPNzFLcNd1RH8obPoejcR2v8HukFDv7YlwPAyC7x9GoRwS1ntUIfHAWAapKglBBCCCEaNkNpppTUlBJCCCEavXrLlMrIyCAyMhKTyeQ7FhMTg8PhIDc3l6ioKN/xESNG8MMPP3DDDTeg1+vR6XTMmTOH8PDw+uh6tegLjwBQ3Oc/ONtfUiv3+HpbOhrQMzGMlpFBvuNqaKL3/yFxtXJfIYQQQoiaYpSaUkIIIUSTUW9BKZvN5heQAnyPnU6n3/GcnBwyMjJ47LHH6N69Ox9//DFTp07l888/Jzo6ukr3ra1N5krbrah9XZF3WZ0amlDjfcgucvLLnmw+25gKwGVd4/3u4Wx/MYWOp3C2HlRrrz+QnWrsRMMm4xe4ZOwCW2Mev8b4mgKJQWpKCSGEEE1GvQWlzGZzmeBT6WOLxeJ3/Pnnn6dDhw7ceOONAPz3v//l4osvZvHixUyYMKFK942Ort1i3+W2r2lQEpSKaJkEETXXhz/2ZHHzO39ic3kAiLGauObstoSYjx/aUIi/s8bu2VjV9mdD1C4Zv8AlYxfYZPxETTNITSkhhBCiyai3oFRcXBw5OTm43W4MBm83MjIysFgshIX51z7aunUrN910k++xTqejU6dOpKamVvm+WVkFaLXwxZuieCfm5bWv2LKI9ngDbpmOEMgsqJF7Zhc5uWPBX9hcHtpFBzO4QwyXnhmPrcCGrWZu0SScbOxEwyfjF7hk7AJbYx6/0tcm6odRJzWlhBBCiKai3oJSycnJGAwGNm7cSJ8+fQBYv349Xbt2Rafzr7/erFkzdu/e7Xds79691dp5T9Oo1clzee3rC7z1pNSgWDSdCWrg/qqm8dj/dpBZ5KRtVDDv3tiTIKPe1wdRdbX92RC1S8YvcMnYBTYZP1HTjmVKyQdLCCGEaOzqbfe9oKAgrrjiCqZNm8bmzZtZuXIl8+fPZ8yYMYA3a8putwNwzTXX8Omnn7J06VL279/P888/T2pqKldeeWV9db9KdCVFzj3W+Bprc+Ffh/ljXw5mg46nL032BaSEEEIIIQKZ1JQSQgghmo56y5QCmDp1KtOmTWPs2LFYrVbuvPNOhg0bBsCAAQN45plnGDVqFCNGjKCoqIg5c+aQlpZGcnIy7733XpWLnNcXX5HzkIQaaS8t386bv+4DYPIF7UiKCamRdoUQQggh6pvsvieEEEI0HfUalAoKCmLGjBnMmDGjzHM7duzwezx69GhGjx5dV12rUaWZUmoNZUq9sGo3NpdK9+ZhXNmtZgJdQgghhBANgW/5nkcKnQshhBCNXb0t32tK9DWYKfXLnix+TMlCr8CDQ85AJ/tWCyGEEKIRMcryPSGEEKLJkKBUHThWU+r0glJ2l4eZ36cAcH3vFiTFyrI9IYQQQjQuhtLle7L7nhBCCNHoSVCqDhyrKXV6y/fe/uMAqfkO4kLN/Pvs1jXRNSGEEEIEEKfTyciRI1mzZo3v2MaNG7nuuuvo2bMnF110EYsWLTppG3369KFjx45+/xUVFdV21yvtWKFzWb4nhBBCNHb1WlOqqThWU6r6mVJ7s4r5cN0hAO4d1J5gk+y2J4QQQjQlDoeDKVOmsGvXLt+xjIwM/v3vf3P99dfz7LPPsnXrVqZOnUpsbCwXXHBBmTbS09MpKChg5cqVWCwW3/Hg4OC6eAmVYtRLoXMhhBCiqZCgVC1TnAXoXIUAeKqZKaVpGjO+34Vb1RjQLoqBSYGx66AQQgghakZKSgpTpkxB0/wDNStXriQmJoZ77rkHgDZt2rBmzRqWL19eblBq9+7dxMbG0rJly7rodrVITSkhhBCi6ZCgVC3TFZYs3TOFgal6NaBW785m/cE8zAYd912YhCLFzYUQQogmZe3atfTv35/JkyfTo0cP3/HzzjuP5OTkMucXFhaW205KSgpt27atrW7WCIOuJCglNaWEEEKIRk+CUrXsdOtJuVWN2b/sBeC6Xok0D7ec4gohhBBCNDY33HBDucdbtGhBixYtfI+zsrL46quvuPPOO8s9f/fu3dhsNm666Sb27t1LcnIyDz30UIMJVAWtn03PwmAW014ypYQQQogmQIJStex060l9vTWdvVnFhFsMjO3bcFPthRBCCFG/7HY7d955JzExMVx77bXlnrNnzx7y8vK45557sFqtzJs3j3HjxvHVV19htVorfa/aSNrWFRwm5PdnuMIQySPMxq2qtXIfUTtKx0rGLDDJ+AUuGbvA1pjHr7KvSYJStUxf5A1KeaxVz5SyuzzM+W0fADf3b0WoRYZLCCGEEGUVFRVx++23s2/fPj766COCgoLKPe/tt9/G5XIREuItKfD8888zcOBAVq1axaWXXlrp+0VHh9ZIv/0YvMXWzR7vToAmi5GYmFq4j6hVtfLZEHVGxi9wydgFtqY8fhLlqGW+mlLVWL63ZPMRjhY6iQ81c3WP5jXdNSGEEEI0AoWFhYwfP54DBw7w3nvv0aZNmwrPNZlMmEwm32Oz2UyLFi1IT0+v0j2zsgrQanh1nVLsJhowak5AI7/QQWZmQc3eRNQaRfH+UlUbnw1R+2T8ApeMXWBrzONX+tpORYJStUxXVL3lezaXh/fWHgRg/NmtMBt0Nd43IYQQQgQ2VVW54447OHToEB988AHt27ev8FxN0xg6dCi33347o0aNAqC4uJj9+/fTrl27Kt1X06j5ybPO7PvRhBu3R2t0E/SmoFY+G6LOyPgFLhm7wNaUx0+CUrXsWKZU1YJSn21MJbvYRWK4hUs6x9VG14QQQggR4D777DPWrFnDG2+8QVhYGBkZGQAYjUYiIiJwOp3k5eURFRWFXq/nggsu4NVXXyUxMZGoqChmzZpFfHw8AwcOrOdXAprhWFDKjEsKnQshhBBNgASlapnOmQ+Aaomo9DVFTrdflpRBL1lSQgghhCjrm2++QVVVJk6c6He8X79+fPDBB2zYsIExY8bw/fff06JFC+677z4MBgNTpkyhsLCQs846i7lz56LX6+vpFRxHd9yyQly4VbUeOyOEEEKIuiBBqVqmuIoB0IzBlb7m621HybO7aRUZxPBkyZISQgghxDE7duzw/fz222+f9Nz+/fv7nW82m3nwwQd58MEHa61/1aYoaHoziseBGSduj2RKCSGEEI2dpODUMl9QylD5oNS6A7kAjOwSh0HXCPeGFEIIIYQoh2awAGBWZPmeEEII0RRIUKo2aSqKu2qZUpqmsfFwHgA9E8NrrWtCCCGEEA2O3ltXyowLl2RKCSGEEI2eBKVqk9vu+7GymVIHcmxkF7sw6RU6x596+0QhhBBCiMZCOy4oJTWlhBBCiMZPglK1qHTpHgDGoEpdU5ol1SU+FJNBhkcIIYQQTUfpDnyy+54QQgjRNEjUoxb5lu4ZgkCp3Fu98bB3t77usnRPCCGEEE1NaaaU4pSglBBCCNEESFCqFimuIqBqO++VZkr1aCFBKSGEEEI0Lf7L9yQoJYQQQjR2EpSqRVXdeS+z0MGhXDsK0L15WC32TAghhBCi4Tl++Z7HIzWlhBBCiMZOglK1SHHZgMpnSm0oWbp3RmwIVrOh1volhBBCCNEgSaaUEEII0aRIUKoW+WpKVTYodahk6Z7UkxJCCCFEE+TLlFIkKCWEEEI0BfUalHI4HDz00EP06dOHAQMGMH/+/HLPu+mmm+jYsWOZ/6ZOnVrHPa6aqizfc3tUvt+ZAUC/1hG12S0hhBBCiIZJbwHAjBQ6F0IIIZqCel0j9txzz7Flyxbee+89UlNTeeCBB2jevDnDhw/3O+/VV1/F5XL5Hm/atIm7776bG264oa67XCVVKXS+ek822cUuooKNnNs2qra7JoQQQgjR4PgVOvdIUEoIIYRo7OotKFVcXMyiRYuYN28eXbp0oUuXLuzatYsFCxaUCUpFRET4fvZ4PLz00kuMHz+erl271nGvq0ZxV76m1OebjwBw2ZnxGPSyqlIIIYQQTc/xhc7dqhQ6F0IIIRq7eot+bN++HbfbTc+ePX3HevfuzaZNm1BPMglZsmQJeXl5/Pvf/66Lbp6WY8v3gk563uE8G2v25QBwedf4Wu+XEEIIIUSDpJeaUkIIIURTUm+ZUhkZGURGRmIymXzHYmJicDgc5ObmEhVVdgmbpmm89dZbjBkzhpCQkGrdV1Gq3eVKtXt8+6WFzjGFnPS+X2xJQwP6t46gZeTJA1ii5pU3diJwyPgFLhm7wNaYx68xvqZAocnue0IIIUSTUm9BKZvN5heQAnyPnU5nudesWbOGtLQ0rrnmmmrfNzo6tNrXVrl9gxuAoLAIgmIqvu+3OzIBGHNuO2JOcp6oXbX92RC1S8YvcMnYBTYZP1GTji3fc0pNKSGEEKIJqLeglNlsLhN8Kn1ssVjKveabb77h/PPP96sxVVVZWQVotTDHURTvxPz49q35OViAIpcBW2ZBudc53CqHcry1pzpGmsms4DxRe8obOxE4ZPwCl4xdYGvM41f62kQ9OC5TyiU1pYQQQohGr96CUnFxceTk5OB2uzEYvN3IyMjAYrEQFhZW7jU///wzd9xxx2ndV9Oo1cmzX/uukkLnhqAK73kkzw5AsFFPmNnQ6Cb2gaS2Pxuidsn4BS4Zu8Am4ydqki9TSpHd94QQQoimoN4KnScnJ2MwGNi4caPv2Pr16+natSs6XdluZWdnc/DgQXr37l2HvTw9pTWlTrb73pF8b1AqIdyMIkUshBBCCNGU6b3Z8lJTSgghhGga6i0oFRQUxBVXXMG0adPYvHkzK1euZP78+YwZMwbwZk3Z7Xbf+bt27cJsNtOiRYv66nKV+XbfM1ZclD013wFAQlj5SxaFEEIIIZqKEwuda5KGJ4QQQjRq9RaUApg6dSpdunRh7NixPPHEE9x5550MGzYMgAEDBvD111/7zs3KyiIsLCygsol8QSnDSTKlSpbvNZeglBBCCCGauGOFzl0AyAo+IYQQonGrt5pS4M2WmjFjBjNmzCjz3I4dO/wejxgxghEjRtRV12rEsUypyizfk6CUEEIIIZq40kwpxbv5jdujYtDp67NHQgghhKhF9Zop1dj5akoZgio8JzXPu3yveZi5TvokhBBCCNFQHb98D5C6UkIIIUQjJ0GpWlSZmlKSKSWEEEII4XXi8j0JSgkhhBCNmwSlatGpdt9zuFUyi7zp6VLoXAghhBBNnmRKCSGEEE2KBKVqi+pBcXuzoCoKSqWVZEkFG/WEW+q1vJcQQgghRL3TDN4v6cxKSVDKo9Znd4QQQghRyyQoVUsUt833c0W77x1bumcOqF0FhRBCCCFqhWRKCSGEEE1KlYNSDzzwAKtXr8bj8dRGfxqP0npSKGAof2lear63yLks3RNCCCGEOL7QeenuexKUEkIIIRqzKq8Zs1qtPPzww7hcLoYNG8aIESPo37+/ZPqcwK+eVAXvzZE8b6ZUcwlKCSGEEEL4Cp1bFBeg4ZDle0IIIUSjVuVMqUcffZTVq1fzyiuvYDAYuPfeeznvvPOYPn06GzdurIUuBqbSnfeoYOkeyM57QgghhKgap9PJyJEjWbNmje/YwYMHGTduHD169GDEiBH88ssvJ23jyy+/ZMiQIXTv3p1JkyaRnZ1d292uvJJMKQATbuwuycwXQgghGrNq1ZRSFIV+/frx2GOPsWLFCq6++mo+/fRTrr/+egYPHsycOXNwOBw13deAUhqUqqjIOUBqnvc9ah5mrvAcIYQQQggAh8PBPffcw65du3zHNE1j0qRJxMTEsHjxYi6//HLuuOMOUlNTy21j8+bNPPzww9xxxx188skn5OfnM3Xq1Lp6CadUmikF3rpSNglKCSGEEI1atbZ8KyoqYtWqVaxYsYJffvmFuLg4br75ZkaMGEFGRgbPP/88a9eu5e23367p/gaMY8v3gio8RzKlhBBCCFEZKSkpTJkyBU3zr7H0xx9/cPDgQRYuXEhwcDDt27fn999/Z/Hixdx5551l2vnwww+5+OKLueKKKwB47rnnGDRoEAcPHqRly5Z18VJOTmfy/egNSsnyPSGEEKIxq3JQ6rbbbuO3334jLCyMiy++mPfff59u3br5nu/QoQP5+fk8/PDDNdrRQOPLlKpg+Z7DrZJZ5C3iKYXOhRBCCHEya9eupX///kyePJkePXr4jm/atInOnTsTHHxsvtG7d+8KSyps2rSJf//7377HCQkJNG/enE2bNjWMoJRSskGM244Zp2RKCSGEEI1clYNSMTExzJkz56TFzfv06cOiRYtOu3OB7NjyvZByn08tKXIebNQTbqlWwpoQQgghmogbbrih3OMZGRk0a9bM71h0dDRpaWnlnn/06NEqnV8vDGZvUEpxSU0pIYQQopGrcjTkv//9LwsWLCAzM5ORI0cCMGnSJAYMGMD1118PQGxsLLGxsTXb0wDjt/teOVIyiwBoFxMsOxcKIYQQolpsNhsmk8nvmMlkwul0lnu+3W6v0vkVqa2pi6LgzZQiz7t8z63W2r1EzSodJxmvwCTjF7hk7AJbYx6/yr6mKgelXnrpJZYsWcITTzzhO9a/f39ef/11srOzmTRpUlWbbJSOLd8rv6bUroxCAJJiys+kEkIIIYQ4FbPZTG5urt8xp9OJxVJ+aQCz2VwmAOV0OgkKqrgGZnmio0OrdH6VlBQ7N+FCZzQQE1OL9xI1rlY/G6LWyfgFLhm7wNaUx6/KQanFixfz8ssv06dPH9+xMWPG0LFjR+677z4JSpU41e57uzK8mVJnxFrrrE9CCCGEaFzi4uJISUnxO5aZmVlmid7x52dmZpY5v6oZ7llZBZxQc71GKApEG7wBNTMusvJsZGYW1PyNRI1TFO8vVbX12RC1S8YvcMnYBbbGPH6lr+1UqhyUstlsWK1lAymRkZEUFMikodSplu8dC0pJppQQQgghqqd79+7MnTsXu93uy45av349vXv3rvD89evXM2rUKACOHDnCkSNH6N69e5Xuq2nU3uS5JFPKrLiwOT2NbpLe2NXqZ0PUOhm/wCVjF9ia8vjpqnrBeeedx/Tp00lNTfUdS09PZ8aMGQwYMKBGOxfITlboPM/mIr3AAUhQSgghhBDV169fPxISEpg6dSq7du1i7ty5bN68mauvvhrwLs3LyMjA4/EWDL/++utZtmwZixYtYvv27dx///1ccMEFDWPnvVLHZUrJ7ntCCCFE41bloNRjjz2Gy+Vi8ODBnHXWWZx11llccMEFqKrKY489Vht9DEwuG1B+TanSIufNw8xYzbLznhBCCCGqR6/X8/rrr5ORkcGoUaP44osvmD17Ns2bNwdgw4YNDBgwgCNHjgDQs2dPnnzySWbPns31119PeHg4zzzzTH2+hLL8glJqPXdGCCGEELWpyhGRqKgoFi5cyPbt29m3bx8Gg4E2bdqQlJRUG/0LWIrLG3gqb/le6dK9JKknJYQQQogq2rFjh9/j1q1b8+GHH5Z7bv/+/cucP2rUKN/yvQapdPkeTvLckiklhBBCNGbVStNxu91ERkYSFhYGgKZp7N27l3/++YcRI0bUaAcDVWlNKQzlBaW8O+/J0j0hhBBCiBOUZkopLuyyfE8IIYRo1KoclFq5ciWPPvpome2HAWJjYyUoVeJkNaWkyLkQQgjRdOzevZtmzZoRGhrKzz//zA8//EDnzp0ZPXp0fXetYfJlSsnyPSGEEKKxq3JNqRdeeIGhQ4fy1VdfERYWxsKFC3nzzTdJTEzk7rvvroUuBibFXVJT6oTle25VY0+WN2B1hizfE0IIIRq1Tz75hMsuu4x//vmHbdu2cdttt3Hw4EFmzZrFrFmz6rt7DZMUOhdCCCGajCoHpQ4ePMj48eNp164dZ555JhkZGQwcOJDHH3+cd955p0ptORwOHnroIfr06cOAAQOYP39+hefu2LGD66+/nm7dunHppZfyxx9/VLXrdepYTSn/QucHc2w43CoWg47EcEt9dE0IIYQQdeStt95ixowZ9OvXj8WLF5OcnMxbb73FSy+9xKJFi+q7ew3TcZlSsnxPCCGEaNyqHJQKCwvDZvNmAbVt25bt27cD0K5dOw4dOlSltp577jm2bNnCe++9x+OPP85rr73GihUrypxXUFDALbfcQlJSEsuXL2fo0KHccccdZGVlVbX7dca3fO+EmlKlO++1jwlBr1PqvF9CCCGEqDvp6en07t0bgFWrVjFkyBAA4uPjKSoqqs+uNVy+mlJOWb4nhBBCNHJVDkoNHDiQJ554gpSUFPr378+yZcvYunUrn3zyCc2aNat0O8XFxSxatIiHH36YLl26MHToUMaPH8+CBQvKnPv5558THBzMtGnTaN26NXfddRetW7dmy5YtVe1+nTlWU8o/KJWWbwegRYRkSQkhhBCNXbt27Vi+fDmfffYZqampDBkyBJfLxfz58+nUqVN9d69h8qspJZlSQgghRGNW5ULnDz/8MNOnT2fLli1cfvnlfPPNN1x99dUEBwczc+bMSrezfft23G43PXv29B3r3bs3b775JqqqotMdi5etXbuWwYMHo9frfccWL15c1a7XqWM1pfyLmWcWOQGItZrrvE9CCCGEqFsPPPAAd999N3l5edxwww20b9+eJ598ku+++44333yzvrvXMB1XU8ruVlE1DZ0i2eVCCCFEY1TloNSPP/7I/fffT2RkJADPP/8806ZNw2w2YzQaK91ORkYGkZGRmEwm37GYmBgcDge5ublERUX5jh88eJBu3brx6KOP8sMPP5CYmMgDDzzgS4dvcFQ3iscBlM2UyiwsDUqZylwmhBBCiMbl7LPP5vfff6egoIDw8HAAbr/9dqZOnVqleVOTclymFIDDrRJk1J/sCiGEEEIEqCoHpZ544gk++eQTX1AKwGqt+i5yNpvNLyAF+B47nU6/48XFxcydO5cxY8Ywb948vvrqK/71r3/xv//9j4SEhCrdt7a+aCttV1FAcRcfe8IY5HfP0kypGKup1voiqub4sROBR8YvcMnYBbbGPH41/Zp++eUXunTpAsBnn33Gt99+S+fOnbn99tvLzIUEx9WU8galbC6PBKWEEEKIRqrKQan+/fvz5Zdfcuutt57WRMpsNpcJPpU+tlj86y3p9XqSk5O56667AOjcuTO//vory5Yt49Zbb63SfaOjQ6vd50q3n1/ofaDoiImL8ZvdZtu8E6yk5hHExNRuX0TV1PZnQ9QuGb/AJWMX2GT8Tm727Nm89dZbvPvuu+zevZvHHnuM0aNH891335GXl8fjjz9e311seEoypYKOC0oJIYQQonGqclAqKyuL119/nTfffJOoqCjMZv/aSN9//32l2omLiyMnJwe3243B4O1GRkYGFouFsLAwv3NjY2Np166d37E2bdpw5MiRqnafrKwCNK3Kl52Songn5llZBSg5R4kCVEMw2VmFvnM0TSO9pNC5yeMhM7Og5jsiquz4sauNz4aoXTJ+gUvGLrA15vErfW014dNPP+XVV1+le/fuPPzww/Tt25cnnniCv//+m/Hjx0tQqjwlmVLBOjeA7MAnhBBCNGJVDkpdc801XHPNNad94+TkZAwGAxs3bqRPnz4ArF+/nq5du/oVOQfo0aMHf/75p9+xPXv2MHLkyCrfV9Oo1cmzpoHiPFbk/Ph7FTo8volVdIip0U3iA11tfzZE7ZLxC1wydoFNxu/k8vLyaNeuHZqm8eOPP/Lvf/8b8JY+8HgkA6hcJZlSlpJMKbtkSgkhhBCNVpWDUldeeWWN3DgoKIgrrriCadOm8fTTT3P06FHmz5/PM888A3izpkJDQ7FYLFx33XV8+OGHvPrqq1x22WUsXbqUgwcPcvnll9dIX2paaU0pzRjkd7y0yHmISS+1EYQQQogmoFOnTrz99ttERESQnZ3N0KFDSU9P58UXX6RHjx713b2GqSRTyiLL94QQQohGr8pBqZtuugnlJBVA33///Uq3NXXqVKZNm8bYsWOxWq3ceeedDBs2DIABAwbwzDPPMGrUKBITE3nrrbeYPn06c+fOpX379sydO5e4uLiqdr9OKK4i7w+GE3beK5Kd94QQQoimZNq0aTzwwAMcPnyYe+65h8TERKZPn87hw4eZNWtWfXevYTohU0qW7wkhhBCNV7UKnR/P7XZz8OBBfvrpJ2677bYqtRUUFMSMGTOYMWNGmed27Njh97h3794sWbKkqt2tF4qrNFPKPyiVUeQAICZEglJCCCFEU9CpUyeWLVvmd+y+++6TXfdOpnT3PWT5nhBCCNHYVTkodccdd5R7fMmSJXz77bf861//Ou1OBbpjy/dC/I6XLt+LsZrLXCOEEEKIxmnbtm28/fbb7NmzB4/HQ9u2bbnxxhvp169ffXetYSoTlJJMKSGEEKKx0p36lMrp27cvv//+e001F9AUV2mh8xNqSpUu35NMKSGEEKJJ+O6777jmmmvQNI1Ro0YxatQoFEXhlltuYeXKlfXdvYapZPmeCe+8SWpKCSGEEI1XlTOlUlNTyxwrKiri7bffJjExsUY6FehKa0ppJ9SUyvBlSklQSgghhGgKZs2axb333su4ceP8jr/77ru8+uqrDBkypH461pCVZEoZNSl0LoQQQjR2VQ5KXXjhhSiKgqZpvoLnmqaRkJDA008/XeMdDEQV1ZTKLJSaUkIIIURTcvDgQQYNGlTm+KBBg3jxxRfroUcBoCRTyqiVZEq5ZfmeEEII0VhVOSj1/fff+z1WFAWj0UhMTMxJd+VrSnw1pSrcfU9qSgkhhBBNQfv27Vm9ejU33XST3/GffvpJMswrUpIpZdCcgCaFzoUQQohGrMpBqcTERBYsWEB4eDgjR44EvMXPzz33XK6//voa72AgOlZT6lhQStO0Y8v3JFNKCCGEaBLuvPNO7rzzTjZt2kT37t0B2LhxI9988w3PPfdcPfeugSrJlNKhYcQjy/eEEEKIRqzKhc5feukl3njjDYKDjwVc+vXrx+uvv87s2bNrtHOB6tjue8feoyKnB3tJ+rnUlBJCCCGahkGDBjFv3jwcDgcff/wxS5YsQdM0PvroI0aMGFHf3WuYSjKlAMw4scnue0IIIUSjVeVMqcWLF/Pyyy/Tp08f37ExY8bQsWNH7rvvPiZNmlSjHQxEvkLnxwWlMkuypKxmPUFGfb30SwghhBB17+yzz+bss8/2O+ZwODh48CAtW7asp141YPpjX96ZccnyPSGEEKIRq3KmlM1mw2q1ljkeGRlJQUFBjXQq0PkKnR9XUyqjSIqcCyGEEMJr7dq1DBs2rL670TApCpreu4TPjEuW7wkhhBCNWJWDUueddx7Tp08nNTXVdyw9PZ0ZM2YwYMCAGu1cwCqnppSvnpQUORdCCCGEOCmtZAmfRZHle0IIIURjVuWg1GOPPYbL5eLCCy/krLPO4qyzzmLgwIF4PB4ef/zx2uhjwCmtKcVxQams0p33JFNKCCGEEOKkSr/YC8YumVJCCCFEI1blmlJRUVEsXLiQHTt2sHfvXgwGA23atCEpKak2+heQfMv3ysuUkqCUEEIIIcRJaaZQ4Aihio08CUoJIYQQjVaVg1JOp5OXX36ZxMREbrzxRgBGjRrFOeecw3/+8x+MRmONdzLQ+AqdH1dTKrOodPmeBKWEEEKIxuzPP/885Tk7duyog54ELs3orV8aSrFv92IhhBBCND5VDko99dRTrF+/nieffNJ37Pbbb+fll1/GbrfzyCOP1GgHA5HiLltTKrvYG5SKDpaglBBCCNGY3XTTTZU6T1GUWu5J4NLMoQCEYpPle0IIIUQjVuWg1Lfffss777xDcnKy79iQIUOIi4tj4sSJEpSi/N33SmtKRcvyPSGEEKJR2759e53eb8mSJUydOrXMcUVRyu3LZZddViZTa/ny5XTo0KHW+lhV3uV7YFVsUuhcCCGEaMSqHJTSNA2Hw1HucZfLVSOdCmgeF4rqfR80v0Ln3mMSlBJCCCFETRoxYgTnnXee77Hb7Wbs2LFccMEFZc71eDzs27ePDz/8kDZt2viOR0ZG1kFPK88XlMKGR9VweVSM+irvzyOEEEKIBq7KQamLLrqIRx99lMcff5zOnTsD3m8En3rqKYYMGVLjHQw0vp33OBaUcrhVChxuAKJDpOaWEEIIIWqOxWLBYrH4Hs+ZMwdN07j33nvLnHvo0CFcLhfdunXDbDbXZTerpDQoFap451U2l0eCUkIIIUQjVOWg1NSpU3n44YcZO3YsqqqiaRoGg4ErrriCSZMm1UYfA4qvyLnOAHpvVlRpPSmjXiHUXOW3XAghhBCiUnJzc5k3bx5PPfUUJlPZ7OyUlBQSEhIadEAKQDV5C52HKd46nTaXSpjlZFcIIYQQIhBVOUISFBTEiy++SH5+Pvv37/elgS9fvpwhQ4awdevW2uhnwFBcJUXOj6snlV10rMi5FDUVQgghRG35+OOPadasGcOHDy/3+d27d2M0Gpk4cSJbtmyhbdu23H///XTr1q1K96mt6YyvXXMYABE677zK7vbU2j1FzSgdHxmnwCTjF7hk7AJbYx6/yr6maqft7Nq1i6VLl7JixQoKCwtp3749Dz30UHWbazxKi5wfV08qs6SeVJTUkxJCCCFELdE0jUWLFjF+/PgKz9m7dy95eXmMHj2au+66i08//ZSxY8fy9ddfk5CQUOl7RUeH1kSXKxQSGQNAuM4OgCXEQkxM7d5T1Iza/myI2iXjF7hk7AJbUx6/KgWlDh8+zNKlS1m2bBkHDx4kLCyMwsJCXnjhBUaMGFFbfQwopTWl/IqcF5dmSkk9KSGEEELUjr///pv09HQuueSSCs/573//i91ux2r1Lo+bNm0af/31F8uWLePWW2+t9L2ysgrQtNPuchmK4p2YF7iMhOLdfQ/gSEYB8WapKdWQlY5dbX02RO2S8QtcMnaBrTGPX+lrO5VKBaUWL17M0qVLWbduHc2aNePCCy9k2LBh9O3bl+7duzeoLYTrm6+mlOH4nfdKglKSKSX+v707j5Oiuvc+/qnqffYVEEEUFcQRhwEi5oohYkTM1QQ13pjrGmM0USDPTW40YG7ERCXAjfpETVxRfPSqwWxqjHG/0bglKBBEEAaVZVh6GGbtvaueP7qnYRyWGeiFbr7v12teTFdVd/1qzjQcvn3OKRERkQx5/fXXGT9+POXl5Xs8xul0pgIpAMMwGD58OFu3bu3XuWybjHaeLVdyoXMSH/YFIvGC66wXqkz/bkhmqf3yl9ouvx3K7denUOqGG25g2LBhzJs3j6985SuZrimvGcnpe7gUSomIiEj2LF++nLFjx+71mEsuuYQJEyYwffp0ACzLYvXq1Vx00UXZKLHPbE8ilComuaZUNJ7LckRERCRD+jQO+tZbb2XIkCHMmjWLz3/+88yaNYuXX36ZcDh8QCcPh8PMnj2b8ePHM3HiRBYuXLjHY7/73e8ycuTIHl+vvvrqAZ0/E1ILnbt8qW0tgcSaUgqlREREJFPWrFnDMccc02NbPB7H7/cTiSQ+IJs8eTIPP/wwL7/8MuvWreOnP/0pHR0dnHvuubkoeY9sdzKUshMf9gWjVi7LERERkQzp00ip8847j/POO4+Wlhb+/Oc/89xzzzF9+nS8Xi+WZfHOO+8wbNgwXK7+rZk0f/58VqxYwaJFi2hqauL6669n8ODBu71jTGNjIwsWLODzn/98atvehqfnys41pYpT2zRSSkRERDKtubmZsrKyHts2b97M6aefziOPPMKECRO4/PLLCYfD3HzzzTQ3N1NfX89DDz3UY0rfwcB2JeopsgOATVckltuCREREJCP6tdB5VVUVF110ERdddBFbtmzh2Wef5bnnnuNnP/sZd955J1/96leZNWtWn14rEAiwePFi7r//furq6qirq2PNmjU89thjvUKpSCTCxo0bGT16NLW1tf0pOeu6p+/tdk0pLXQuIiIiGbJ8+fJe24YMGcLq1atTjw3D4Dvf+U6/FjXPBcuTCNdMLHyEaQ8plBIRESlE+30bk0GDBnHllVfyu9/9jueff56LL76Y119/vc/PX7VqFbFYjIaGhtS2cePGsWzZMiyr5xDtdevWYRgGQ4cO3d9ysya10HlyTSnbtjVSSkRERKQ/nD5swwFAKUGFUiIiIgWqXyOl9uTII49k+vTpqUUz+8Lv91NZWYnbvTOoqampIRwO09raSlVVVWr7unXrKCkp4brrruPdd99l0KBBzJgxg0mTJvW7VsPo91P69bpGbOeaUoYBgWicUCwRslUXuzN2ftl/qbZT2+QltV/+Utvlt0Juv0K8prxjGNjuEoxwG6VGgPZQNNcViYiISAakJZTaH8FgsEcgBaQedy/G2W3dunWEQiEmTpzIVVddxYsvvsh3v/tdnnzySUaPHt2v81ZXlx5Y4fvgcyQ6TUVllRTVlNLZnBg5Vex2cMTgioyeWw5Mpn83JLPUfvlLbZff1H6SKba7FMJtGiklIiJSwHIWSnk8nl7hU/djr9fbY/s111zDJZdcklrY/LjjjuODDz7gN7/5Tb9Dqe3bO7DtAyh8Dwwj0TEPd7ThAbqiToLNHazd2AZAVZGL5uaO9J9YDlh322Xqd0MyS+2Xv9R2+a2Q26/72iS3bHdisfMSQ6GUiIhIocpZKDVw4EB27NhBLBbD6UyU4ff78Xq9ve4cY5pmrzvtDR8+nLVr1/b7vLZNZjvPyYXOLVcRtg3Nu6wnVWid9kKT8d8NySi1X/5S2+U3tZ9kiu1O9AdLCbBJoZSIiEhB2u+Fzg/UqFGjcDqdLF26NLVtyZIljB49GtPsWdaPfvSjXnf1W7VqFcOHD89Gqf3y2bvvdS9yXlWkRc5FRERE+sraZaRUm9aUEhERKUg5C6V8Ph/Tpk1jzpw5LF++nJdeeomFCxdy6aWXAolRU6FQCIDJkyfzzDPP8Ic//IFPP/2Uu+66iyVLlnDxxRfnqvw9MmLJUMrlA9Cd90RERET2g+1OTKHsXlPK1pA8ERGRgpOzUApg1qxZ1NXVcdlll3HTTTcxY8YMpkyZAsDEiRN57rnnAJgyZQo33ngjv/71rzn77LN55ZVXeOCBBxgyZEguy9+tXiOlAolP9qqLXTmrSURERCTf7AylAsQsm2DUynFFIiIikm45W1MKEqOl5s2bx7x583rtW716dY/HF1xwARdccEG2SttvqVDKVQzsMlJK0/dERERE+qw7lCozgxCH9lCUIrcjx1WJiIhIOuV0pFRBSk7fw9VzTSlN3xMRERHpu+5QqsqRWM5Bd+ATEREpPAql0mznSKlEKNUaTEzfqyzS9D0RERGRvupe6LxSoZSIiEjBUiiVTra9y5pSiYXO24KJDlS5V6GUiIiISF/Z7jIAyszuUEp34BMRESk0CqXSKR7BsONAYk2paNwiEE08LvPmdPkuERERkbxiJ0dKlZL4wK9NI6VEREQKjkKpdIp0pb61XUWpzpMBlCqUEhEREemz7jWlSggCmr4nIiJSiBRKpVMylLJNN5jO1DDzMq8T0zByWZmIiIhIXukOpYrsxEgphVIiIiKFR6FUOqUWOf/MelI+rSclIiIi0h/d0/e8VuJDP60pJSIiUngUSqVT90ip5J33dh0pJSIiIiJ9ZyUXOvdYAQwsjZQSEREpQAql0ik1UqoY0J33RERERPZX90gpgBJCGiklIiJSgBRKpVMkGUo5EyOl2jRSSkRERGT/OL2JdTpJLHauu++JiIgUHoVS6RQPJ/50eoCdty7WmlIiIiIi/dc9WqrUCGj6noiISAFSKJVOsUQoZZuJEKp7mHm5RkqJiIiI9Fv3HfhKCNKhUEpERKTgKJRKp3hyrQNHIpTqXlOqTGtKiYiIiPSblQylyowAgWicaNzKcUUiIiKSTgql0ikeAcA2E9P3ukdKVfg0UkpERESkv7qn75UQBNAUPhERkQKjUCqdkqFUaqRUqHuklEIpERERkf6yPeUADHCHAIVSIiIihUahVDqlRkp1T99Lrimlhc5FRERE+s3yVgJwmLMT2DkKXURERAqDQql0So2USty+WCOlRERERPaf7asBYIDZAezsW4mIiEhhUCiVTsmFzm3TRSgaJxxLLMZZroXORURERPrNKkqEUjXJUEp34BMRESksCqXSqXv6nsOdWvPAYRoUux25rEpEREQkL1neKgCqaQOgTdP3RERECopCqXSKhRN/7hJKlXudGIaRw6JERESk0L344ouMHDmyx9fMmTN3e+ybb77J2WefTX19PZdeeikbNmzIcrV91z1SqtxOhFJa6FxERKSwaLGjdEpO38N0pT7J03pSIiIikmlr167ltNNO42c/+1lqm8fj6XVcU1MT1157LTNmzODUU0/l7rvv5pprruHpp58+KD9Es5JrSpXGFUqJiIgUIiUm6ZSavudKLcSp9aREREQk0xobGxkxYgS1tbV7PW7x4sWccMIJXHHFFQDMnTuXU045hXfffZcJEyZko9R+sX2J6XtFsVYMLFoCkRxXJCIiIumU0+l74XCY2bNnM378eCZOnMjChQv3+ZyNGzfS0NDAO++8k4UK+ykVSnloC2qklIiIiGRHY2MjRx555D6PW7ZsGePHj0899vl81NXVsXTp0swVdwC615Qysaigk20dCqVEREQKSU4Tk/nz57NixQoWLVpEU1MT119/PYMHD2bq1Kl7fM6cOXMIBAJZrLIfkqEUpmvnmlI+jZQSERGRzLFtm48//pg33niDe++9l3g8ztSpU5k5cyZut7vHsX6/nwEDBvTYVl1dzZYtW/p1zkzN9Ot+3dTrO91YnnLMcBvVRjv+znDGzi0HplfbSV5R++UvtV1+K+T26+s15SyUCgQCLF68mPvvv5+6ujrq6upYs2YNjz322B5Dqaeffpqurq4sV9oPu07fa9dIKREREcm8pqYmgsEgbrebO+64g40bN3LzzTcTCoX48Y9/3OPY7uN25Xa7iUT6NwKpurr0gOvu8+uXDIBwG9V08ElXhKqqEkyzAHvvBSLTvxuSWWq//KW2y2+HcvvlLDFZtWoVsViMhoaG1LZx48Zxzz33YFkWptlzZuGOHTtYsGABCxcu5Oyzz852uX2TWujcnVrovEIjpURERCSDDj/8cN555x3Ky8sxDINRo0ZhWRY//OEPmTVrFg6HI3Wsx+PpFUBFIhHKysr6dc7t2zuw7bSU34NhJDrmu75+ubsSF1BjthGL23y0oYWaYvdeX0eyb3dtJ/lD7Ze/1Hb5rZDbr/va9iVnoZTf76eysrLHp3U1NTWEw2FaW1upqqrqcfzPf/5zzj33XI499thsl9p3qZFS7tT0PY2UEhERkUyrqKjo8fjoo48mHA7T1tbWo081cOBAmpubexzb3NzMqFGj+nU+2yajneddX9/yVQMwzNMFAdjaHqa6SKHUwSrTvxuSWWq//KW2y2+HcvvlLDHZ0/BxoNcneG+++SZLlizh2WefPeDzZnQNhGQoZThcPUZKFeL80EJSyPN4DwVqv/yltstvhdx++XZNr7/+Ov/5n//Ja6+9hs/nA+DDDz+koqKi14d89fX1LFmyJPU4GAyycuVKpk+fntWa+8Py1QBwuCuxhIO/MwwcutMcRERECknOQqk9DR8H8Hq9qW2hUIif/OQn3HjjjT2276+MztWMJeovrSijK2oBMHRgGTU16jjlg0N5Hm8hUPvlL7VdflP75V5DQwMej4cf//jHXHvttWzYsIH58+dz5ZVXEo/HaWlpoby8HLfbzfnnn8+DDz7Ifffdx2mnncbdd9/NkCFDmDBhQq4vY48sXyJYG+TsBGBbp+7AJyIiUihyFkoNHDiQHTt2EIvFcDoTZfj9frxeb491DZYvX86GDRuYOXNmj+d/+9vfZtq0afz0pz/t13kzugZCcqRUeyDO9u4OUyRKc3NH+k8oaVPI83gPBWq//KW2y2+F3H59XQPhYFFSUsKDDz7Irbfeyvnnn09xcTEXXnghV155JZs2beL000/nkUceYcKECQwZMoQ777yTW2+9lbvvvpuGhgbuvvtujIN4eFj3SKlasx2AbR3hXJYjIiIiaZSzUGrUqFE4nU6WLl3K+PHjAViyZAmjR4/uscj5iSeeyAsvvNDjuVOmTOHmm2/mlFNO6fd5MzpXMxlKWYaL9uT0vTKPs+A664XqUJ7HWwjUfvlLbZff1H4Hh2OPPZaHHnqo1/YhQ4awevXqHtsmTZrEpEmTslXaAbOToVSlnQilEtP3REREpBDkLJTy+XxMmzaNOXPmcOutt7Jt2zYWLlzI3LlzgcSoqdLSUrxeL8OGDev1/IEDB1JdXZ3tsvcuefe9kO0kGk/00Mu8uvueiIiIyP7qnr5XarUCsFXT90RERAqGue9DMmfWrFnU1dVx2WWXcdNNNzFjxgymTJkCwMSJE3nuuedyWV7/xROf3LVHEkPgvU4TnyunP2IRERGRvNY9fa8ougMAv6bviYiIFIycjZSCxGipefPmMW/evF77PjvUvK/7cio5Uqo1+QFedbH7oF6jQURERORgZxUlQil3tA0nMbZ1hrFtW30sERGRAqBhPOmUXFOqJfkBXk2xO4fFiIiIiOQ/21OBTSKAqqSDYNSiKxLPcVUiIiKSDgql0ikZSm0PJTpO1QqlRERERA6M6cBOrit1hDcAwFZN4RMRESkICqXSKZYIpZpDiYcaKSUiIiJy4LrXlRqeDKV0Bz4REZHCoFAqnZIjpZqDiTvv1ZQolBIRERE5UJYvccflIzxdAGzTHfhEREQKgkKpdLFtsBILnfuToVR1kUIpERERkQPVHUod7kqGUpq+JyIiUhAUSqVLMpAC2BJIhlIaKSUiIiJywOxkKDXA0Q6AXyOlRERECoJCqTQx4js7R1u7LEBrSomIiIikQ/eaUjW0AbBNa0qJiIgUBIVS6RLfOVLKn1zoXHffExERETlw8dLDAaiObQF09z0REZFCoVAqXazESCnbMInZJqYBlT5XjosSERERyX/xsmEAlAU3AbCpNYRt27ksSURERNJAoVSaGMmRUpaZGB1VWeTGYRq5LElERESkIMTLjwTAHdyM14wTiMY1WkpERKQAKJRKl+RIKctIjI7SelIiIiIi6WEX1WI7izBsi/FliXWlPm0J5rgqEREROVAKpdKke6HzmOEEFEqJiIiIpI1hEC8/AoCxxTsA+LglkMuKREREJA0USqVLcvpejEQoVV2s9aRERERE0qV7Ct9xnmYAPlEoJSIikvcUSqWJkZy+F0HT90RERETSrXux8yPNbQB8vF2hlIiISL5TKJUuyZFSYdsBQLVCKREREZG06R4pNTC2GdBIKRERkUKgUCpNuteU6g6lNFJKREREJH26Q6nS0AYAWgJR2kPRHFYkIiIiB0qhVLpYiU5RyNJIKREREZF0i5cnpu+5OjYysDixhqem8ImIiOQ3hVJp0j1SKmB1L3SuUEpEREQkXaySwdimEyMeZlxlENAUPhERkXynUCpdkiOlwpam74mIiIiknekkXjoEgPqiFgA+3h7MZUUiIiJygBRKpUn3SKkoTordDrwuR44rEhERESksVnJdqZHuZgA+3aGRUiIiIvlMoVS6JEOpCE6NkhIRERHJgO51pY5gC6A1pURERPKdQqk0MZLT96I4Kfe5clyNiIiISOGJlx0JQE1sMwBNbSFC0XgOKxIREZEDoVAqXeKJUCqCkyK3pu6JiIiIpFs8OX3P17memmI3NvDBlo6c1iQiIiL7L6ehVDgcZvbs2YwfP56JEyeycOHCPR779NNPc+aZZ3LiiSdy4YUXsnz58ixWum9GPAxA1HZSpPWkRERERNIuVnUsAM6Wjxh/eBEA721sy2VJIiIicgByGkrNnz+fFStWsGjRIm688Ubuuusunn/++V7H/eMf/+CGG27gmmuu4U9/+hMNDQ18+9vfpqurKwdV78Eu0/d8GiklIiIiknZW2TAsTwWGFeFLFdsAhVIiIiL5LGehVCAQYPHixdxwww3U1dVxxhlncOWVV/LYY4/1Otbv93PNNdfw1a9+laFDh3LttdfS2tpKY2NjDirfPWPX6XsaKSUiIiJZsnXrVmbOnMlJJ53Eqaeeyty5cwmHw7s99rvf/S4jR47s8fXqq69mueIDYBjEBtYDMM71MQD/bGonGrdyWZWIiIjsJ2euTrxq1SpisRgNDQ2pbePGjeOee+7BsixMc2dedtZZZ6W+D4VCPPzww1RXV3P00Udntea9srrvvufCp1BKREREssC2bWbOnElZWRmPPfYYbW1tzJ49G9M0uf7663sd39jYyIIFC/j85z+f2lZeXp7Nkg9YdMAY3Ov/l0FdK6nwnUBrMMrKLR3UH55f1yEiIiI5DKX8fj+VlZW43e7UtpqaGsLhMK2trVRVVfV6zltvvcUVV1yBbdv893//N8XFxf0+r2EcUNl7ft3U9D0HRW4zY+eR9OtuK7VZflL75S+1XX4r5PbLp2tat24dS5cu5W9/+xs1NTUAzJw5k3nz5vUKpSKRCBs3bmT06NHU1tbmoty0iA0YA4Br23IahnybV9c0897GNoVSIiIieShnoVQwGOwRSAGpx5FIZLfPOfbYY/nd737Hq6++yo9+9COGDBnCmDFj+nXe6urS/ap3n5w2kFhTqraymJqaDJ1HMiZjvxuSFWq//KW2y29qv9yqra3lgQceSAVS3To7O3sdu27dOgzDYOjQodkqLyOiAxLT9xwtHzHhGCevroH3N7bxzQk5LkxERET6LWehlMfj6RU+dT/2er27fU5NTQ01NTWMGjWKZcuW8cQTT/Q7lNq+vQPb3q+S96ok0IUXiNhO7EiU5mbdnjhfGEbiP1WZ+t2QzFL75S+1XX4r5PbrvrZ8UFZWxqmnnpp6bFkWjz76KCeffHKvY9etW0dJSQnXXXcd7777LoMGDWLGjBlMmjQpmyUfMLt4APGSwTg6mzjFtwFwsWxTOzHLxmnm0TA3ERERyV0oNXDgQHbs2EEsFsPpTJTh9/vxer2UlZX1OHb58uU4HA7q6upS244++uj9WujctslM5zm10HliTalC66AfCjL2uyFZofbLX2q7/Kb2O7gsWLCAlStX8tRTT/Xat27dOkKhEBMnTuSqq67ixRdf5Lvf/S5PPvkko0eP7td5MrYcQh+nhcYGjsHR2cSRkY8o9dTTEY7x0bZO6g7LjzCxEBXylN5Dgdovf6nt8lsht19frylnodSoUaNwOp0sXbqU8ePHA7BkyRJGjx7dY5FzgKeeeopNmzbx4IMPprZ98MEHHH/88VmteW92rinl1ELnIiIiknULFixg0aJF3H777YwYMaLX/muuuYZLLrkktbD5cccdxwcffMBvfvObfodSmR5Jts/XP+okaHyO0rYVfP7oybywcivLt3UxafTgjNYl+5Yvowxl99R++Uttl98O5fbLWSjl8/mYNm0ac+bM4dZbb2Xbtm0sXLiQuXPnAolRU6WlpXi9Xr7+9a/zb//2byxatIhJkybx9NNPs3z5cubPn5+r8nuLJ269HMVJkVuhlIiIiGTPz372Mx5//HEWLFjAmWeeudtjTNPsdae94cOHs3bt2n6fL1PTNvs6LdRVMopyIL7hH3yuoYwXVm7lLys28436QekvSvqkkKf0HgrUfvlLbZffCrn9+rocQs5CKYBZs2YxZ84cLrvsMkpKSpgxYwZTpkwBYOLEicydO5fzzjuPuro67rrrLm677TZ+8YtfcOyxx/Lggw8ycODAXJbfgxHfefc9jZQSERGRbLnrrrt44oknuO2225g6deoej/vRj36EYRipDwABVq1atdtRVfuS6Wmb+3r9aO2J2Bg4OjbxhQExbgE+2NzB9q4IVUXuPT9RMk5TevOb2i9/qe3y26HcfjkNpXw+H/PmzWPevHm99q1evbrH49NOO43TTjstW6X1X3L6Xth2UaRQSkRERLKgsbGRX/3qV1x11VWMGzcOv9+f2ldbW9tj5PnkyZP5/ve/z4QJE2hoaOCZZ55hyZIl/PSnP83hFewf211KvHoUzu0rGdSxjJEDBrN6WydvftzC2XUaLSUiIpIvzH0fIn1hxBN3DozixKfpeyIiIpIFL7/8MvF4nF//+tdMnDixxxckRp4/99xzAEyZMoUbb7yRX//615x99tm88sorPPDAAwwZMiSXl7DfIoMnAOBueptThlcB8Ld1LbksSURERPoppyOlCokd2xlKaaSUiIiIZMNVV13FVVddtcf9nx15fsEFF3DBBRdkuqysiA6eAP98CFfTO0z8wnUsfHs9b32yg1jcwunQ564iIiL5QP9ip4m960gpl36sIiIiIpkUTY6UcmxfRV1FjAqfi65InKWb2nNcmYiIiPSV0pM06Q6l4qZLn86JiIiIZJhdVEus4mgMbLxbl3DKUZUA/GXVthxXJiIiIn2l9CRdknffczh1xxcRERGRbOgeLeVqeptpow8D4NkPtrKlPZTLskRERKSPFEqlSzwMgOnw5LgQERERkUPDzlDqHcYMKWf80HJils2idzfkuDIRERHpC4VS6dI9UsqlkVIiIiIi2RAdfDIATv8/IdLFlZ8fBsAfV2xhW0c4l6WJiIhIHyiUShPDSoRSTpdGSomIiIhkg1V6OPHSIRh2HPf6Vxk7pJyGw8uIxm0e1mgpERGRg55CqTQxk6GUw61QSkRERCRbwsdOA6D4nQUYVoxv/0titNTvljWxtrkrh5WJiIjIviiUShPTStx9TyOlRERERLInMPYaLG8VztZGvCsf43NHVPLFY6qJ27Dg5bXYtp3rEkVERGQPFEqlg23jsGMAuN1aU0pEREQkW2xPGV0n/QCA4ndvwwi38x9fPBqP0+S9jW28sMqf4wpFRERkTxRKpUNy6h6A0+XNYSEiIiIih55Q3UXEKo/BDLXgW3ovg8u9fHPCUABue62RDTuCOa5QREREdkehVDrEd4ZSbrdCKREREZGsMp10TfghAL5/PgyRLi4eP5Rja4tpCUS59qnlbNXd+ERERA46CqXSwEiuJwXg8WhNKREREZFsixw1lVj5kZjhNnwfPoHHafLL80cztMLL5vYw1y5ezjYFUyIiIgcVhVJpYMQToVTcNvBqTSkRERGR7DMdBMdcDYBv2QNgxagpdnP3BScysNTDpzuCXPnEUj5pCeS4UBEREemmUCodktP3ojgpcjtyXIyIiIjIoSl03NewfNU4OjbgafwTAIeVebnv6/UcUeljc3uYKx9fym/e30RnOJbjakVEREShVBp0T9+L4MLnUiglIiIikhNOH8HR3wSg5K8/xrn57wAMLvfywIX1jBpYQlsoxoJXGjnrnre5+YWPWLW1I5cVi4iIHNKcuS6gICRHSkU0UkpEREQkp4L138L9yYu4ti2j4g9fJ3TcBZjBZkpdxdx//q384cM2frt8Mx9vD/DHf27hj//cwvGDSjn/xMOYclwtXn3AKCIikjUKpdKge6RUFKdGSomIiIjkkO0upXXaYspemoln3fP4Vj6W2ldRewJfH3sV/9YwmKWb2vntsiZeWdPMyi0drNzSwfxX1nLCYaWMG1LBmaMGcESlL4dXIiIiUvgUSqVDcqHzqO3QSCkRERGRXHMV0T71Prwr/wdH68cYkXZ8Kx/Ht/Q+gqMvx3C4aRhSTsOQcnYEIjyzYiu/W76ZTW0hlmxoY8mGNu5761PGDiln0jHVnHREJcNrijANI9dXJiIiUlAUSqWBYXVP33NRpJFSIiIiIrlnmITqLk58Hwvh/uRlHF1b8Hz0B8Kj/i11WGWRm0tPGsolnxvCJy1B3t/Uxv+ubebtT3bw3sY23tvYBkCx2+SmoqcocsS4w7iMiAUnHFbGqIGltIeibOsMc1R1MacdU82gMm8urlhERCTvKJRKh13uvudzae14ERERkYOK00uw/luUvDWXovfvIXzc18Do2WczDIOjqos4qrqI8048jC3tIV5c7efd9a0s3djG1NhrfC30FAB/ihzBn6yT+aQlyLMfbO3xOre92siAEjcuh0l1sZvJx9Zw+ogaBpR6NNJKRETkM3IaSoXDYW666SZeeOEFvF4vV1xxBVdcccVuj33ttde4/fbbWb9+PUOGDOH//J//w+mnn57linfPjoUBLXQuIiIicrAK1V1C0ZK7cO74iPJnLyU8/CwiR56OVTwIYkE8jX/GCLcSOepMrNLDGVTm5ZLPDeWSzw3F6thC9RNXQ2LFBuZV/pEpEy9j6ZYAa/xdVBe5qSp28f7GNpZtamdbZ+LATW0hlje1c8f/rsMAij0OhpT7OLq2mBMGlXLykZUMqUisWxWIxFmxuZ3G7QGGVxdRP7hMi66LiEjBy2koNX/+fFasWMGiRYtoamri+uuvZ/DgwUydOrXHcatWrWL69Olcd911TJo0iTfeeIPvfe97PPXUUxx33HE5qn6naCQRSmmhcxEREZGDk+0pIzD+e5S8eTPu9a/hXv8aALHq4zE7N2GGE9P0eP0nRAafTOcXbiFePRJsm4q//QRnpI1oTR2Ozs2UdH3K5PBL/MvEi3qcw7XpLZz/uJ8O9wBWnvhjVvuD/OXDbSxrascGOsNxVm3rZNW2Tv6UHGFV7HZg2xCKxbHsna/ldhgMKvNS4nEyckAxV548jAGlniz8pERERLInZ6FUIBBg8eLF3H///dTV1VFXV8eaNWt47LHHeoVSzz77LCeffDKXXnopAMOGDeOVV17hz3/+80ERSkWiO0Mpj1PT90REREQORsGG7xAZ+gU8n7yE+5OXcG59H+f2lQDEy44gXnwYrs3v4m56m4rffoWuU36Cp/FPuDf8Fdt00nH67bg3vUnJG3Mo+vtthI86E7uoBkfLR5T89b9wb/obACVAWe0RnDj+e1wwZjDhmEVnOEZ7KMYnLQHW+Dv5x4Y2lje10xWJp+obWOrh2NpiPtrWybbOCOt3BAFYuaWD5z/cxiWfG8rpI2o4qqoIQ1MBRUSkAOQslFq1ahWxWIyGhobUtnHjxnHPPfdgWRamuTPcOffcc4lGo71eo6OjIyu17kssOVIqbrgwDAPb3scTRERERCQn4jXHE6g5nsD4mRiBZtyb/oblrSI65BQwTMyOTZS+/B+4N71J6WvXA2A7PHSe+lPiNccTrDwa37IHcHRspOqxLxAZPhXPR7/HsKLYppvI0FPxfPoyRe/+guhhJxE9/PN4nCYep5vqYjdHVRdx2rE1XAV0hmM0d0Uo3bGC6i1/xRh7BbavCtu22dQWwt8ZYUcwyqN/38g/N7dz35ufct+bn1JV5GLkgBKOqSlmaKWPQWUefE4HkbhFsdvByIGlOE2FViIicvDLWSjl9/uprKzE7XanttXU1BAOh2ltbaWqqiq1/eijj+7x3DVr1vDWW29x4YUXZq3evYlGQgBYpivHlYiIiIhIX9lFNYSP/WqPbVbp4bSd8xjFf/spRf98iMjhp9D5xbnEK4YnDnB4aDvrQUpf+QGu5hV4V/0GgPCRX6Lz1JuxyoZQ+vL38a76DeXPXoLlKcew4mDFwDAIH3MOXSdfj+0pp8TjpGrD85S99D2MeJj42ifpOOMujICfUWufZWRRLeHjLuC0C0/k+dV+nl2xlWVN7bQEorz1yQ7e+mTHbq+r1OPkhMNKCUXjtIZiVBe5GFLhw+Uw6QzHsGwbr8tBkcuBz+3A6zRTi7AXux1UFbuxLJstHWE6wjG8TpNit4NhVUUcW1tMpS/xQWw4ZrF6WyddkRjHDyylJnNNJSIiBSpnoVQwGOwRSAGpx5FIZI/Pa2lpYcaMGYwdO3a/FjrPxEjneHL6nm26MvL6klndbaa2y09qv/yltstvhdx+hXhN0k8OF11f+BmBCf+J7S7r9UsRr62j9YI/4V31GzxrnyE06uuEj/lK6riOL9yMc9synC2rccRCPZ7rW/EI7nV/IXz0lzFDLXjX/BEA21mEo3MzFb8/v8fxRf98iOiAMZw9+RecdcGJRJJB0JrmLhr9XTS1h9jcHiIat3GaBs1dEdpDsR6B1cfb4R8b2tL243E5DMq9LlqDUWK7LIQ1uNwLto1lw6AyD8OqiqgtdlPscTK4zMOEIyspduvm3yIislPO/lXweDy9wqfux16vd7fPaW5u5pvf/Ca2bfPLX/6yxxS/vqquLu1/sfvQ6rAAsB3ujLy+ZIfaLr+p/fKX2i6/qf2kkNme8j3vNB2Ejv8GoeO/0Xufq4gd5z+Ns3UtGA5s0wGGE0fHBorfmIOzdR1F/3wodXhg9DcJTPhPSl/9IZ7G54gXDSA88nzMziY8657HtW0plYu/TGDsNXhLBvM5K8qEwBZMYxuxEWMIjTgfXIm7+MUtm5VbOvjI38kxXe9zQtPjxCMh2i0vm4qOY82grxDxVBGKWgSicYKROMFonO5oqTMcY3tXFNNIrHFV7nMRjsVpD8VobO5iY2siAGvuSvTbq4pclHicrN8RpKltZwC3pSPM0k3tPX8sDoMxh5czoNRDudeJwzAwDIPq4sRIriKXg45wjLhlU1vipqbEjdth4jQNHN1fhoHTYWJZNoFInKhlUelz4XSYtAairPZ3gg1VxS6qitxU+Fw49jKV0bZtNreHaQ1GGVjqoarIpfW6RESyKGeh1MCBA9mxYwexWAynM1GG3+/H6/VSVlbW6/itW7emFjp/5JFHekzv64/t2zvSvuZTZ2cXkAilMvH6klmGkfhPldouP6n98pfaLr8Vcvt1X5vIAXEXExtQ32NTvOpYIof/C96Vj+Po2ortKiJWczyRYaeDYdB+5r042j4mXjoUHIllIbq6tlLyyn/iWf8qxX+/vfd5PnyC4rfnERl2OlbxQCxPBSeZTv5lyxK8jc+mDjscGNX2Gqdve4jw0V8mfOxXiZcNw7vyf3D7Xyc6aDzB+ivBcOHa9D5GuB3bW4Xlq8LyVmEV1WCVjSIUs2lr246r8c/4nAZFx0zCLh9GRzhKaxziG5ZTvvVtWiIOPomU8ZF5FE1WJSu3dLB+R5C/r29N+4/aAEq9TtpDsV77TAOK3U6icYuoZVPidlCeDKps28bfGemx2LzLYSSmNSa/vK5EKGYDtk3yz51/6ZV6nFQWuQjHLLZ3RYnELYrciamRRW4HxW4Hh5d7OaKqCMuy2d4VwQLKvU6qi92MrC2hoijR1pGYhWmAwzQUjInIISNnodSoUaNwOp0sXbqU8ePHA7BkyRJGjx7dawRUIBDgyiuvxDRNHnnkEWpra/f7vLZN2jvP3dP3cLgz8vqSHWq7/Kb2y19qu/ym9js4hMNhbrrpJl544QW8Xi9XXHEFV1xxxW6PXblyJTfeeCMfffQRxxxzDDfddBMnnHBClis+hDm9hE785u73GcbOtauSrOKBtJ/9CN4Pn8T98QuABYYDq3ggtrsMz9qncbSvx7v6qV4vZxsmobpLiA4cgxnageejP+DyL8f70e/xfvT7nmVtX4Xvg0f3WrrlLiNefRxDti3DiCf7v2+D5auh0lfFcCsMrZ+mjj8ZsDGIDj6J2Ih6Am1baesKsK5oDB94xmBZ4Im14w8ZrOr04Y8XU+T1Yhrg74qwvSuCGQ9RZbcRw0EnPgJ4sNn5fwXTAMsmFUgdUenD5TBo6YrSGoxi2dAR3hlWtYVitH0mvHKaBhU+Fy2BCNG4TVu89zH9Mdb4iMPNTbwSH0szexltl1RV5CIYjROMWqlrcjlMPE4Tt8PE7TTxJP9MPDZw77LfMCASt7FtG18yDItbNrHur7idel3TNHAYYBoGpmlgGhCN20TiFpgmHYEIhpEI28q8Lkq9Tko9TiwrcUw0bhGJ2ziSPzOvM7FOWSAax2EYuJ0mLoeJyzRwOU3cjkStLoeZ/NPA5TCJWRaRmI3DJFWzz+XAaRp0hGN0hGOUuJ1UFbvxOk0cpoFpJGqPWTatwSidkTgVXieVRe7UaLhwzOKTlgDtoSiVPjcVPidOM/EzCkbjBKJxTAyK3A5cDoOYZWMAFcnRdrZtE45ZuBzmXkfY7Spm2QQiMWwbyrxOBYoi/ZSzUMrn8zFt2jTmzJnDrbfeyrZt21i4cCFz584FEqOmSktL8Xq93Hvvvaxfv57/9//+X2ofJKb5lZbm/pNMK5b4R9lwuPdxpIiIiEj6zZ8/nxUrVrBo0SKampq4/vrrGTx4MFOnTu1xXCAQ4KqrruKcc87h5z//OY8//jhXX301L774IkVFRTmqXvbJMAgdfyGh43vf5Kdrwg9xr38Vx/ZVmIFtmOE2sOLYLh/BEy4nXluXOjZY/22cW9/H89Hv8TQ+hyOwlfCwyYSPPhvPx39JhF6mi+igsVilh2OEdmAGWzBDOzC7tmBG2jE3vwtArGoktqcM59b3MYPNmMFmIHGnwsjh/wKmE0fHRpzbP8Td9A7upncoAmqAo3mRM/ZwqZZdie0uBWcYoyiIGek5BdDGwHYVE/eUY5cchuWrIWJBLBqlKLYDV3AbWFEodmJVFBHxVBNyVWKaJoZhErNsopaNK7QdX3ALhtONUTMSu3QwdriDaKANIu0YkU6ihpegs5yow4dtOrENB7bhTHxvOrFtAyPYgjPkJ+7wEi8ZzIC2ZVTveB+AuNvFBxWT2WxVEAwGMU0Dt8uFZTgJxE2aww4+DJSyLVSJiYXLjNNh+2ihjHDMhRm3MLFxYGFiYWETxmaLXcJ2yrFS4VziGCdxTCwcWATxEMeR2p/8RQLATZRyOnETwzQsYraTIG5CuAnj6hH67U4JAbxE2UFJ8hyJ8+88X3aZBnidiZCpIxzD2s8PSko9TkKxONF4Iqgq8zopdjsSo+qAznBiqqvHaeJzOYjGLboiccIxK/UaHqfJgBI3RW4nbodBeyjG9kCEWDLI23UqqnOXKakO06DY7aDMm1ijuCsSJxSNYxiJ4NDAwDAS12oYBj6XSaXPhdtp0hGKETMMDMvC43RgsHNE366j+wzDSI3g6x7NF4zG2RGM0hKI0hqIEIxaFHsSAWEsbhOKJa4vFLPwOk1qSzxUFrkocTsp9iRGAha7HYSiVipM7AzH6AjH6QjHCEctPE4DrytxQwWP08RpmpgmyZBxZzhqYNAVidEViWMaJO9a6kgEsK5EMAuwpSPE1o4wXqeDcp8Tt6PvS/q4HCalHiemAds6I7QGo1T4XNQUu3E7E69j0HMZQcMwEttSjxNHpdbWTG4zdj4Bg8Soy0RobBCNW8SS4a9l23idiVGYnZE4rcEoLo+LcDCSCIs/Exp3/4wSU6sTwXn3KMxijxOPw2BHMEpzZwSP00F1sYvqYjdVRS6K3U7CMYtgNE4oZhHa5U/DMPA4TUbUFlN/+L7D80wybDt3n28Gg0HmzJnDCy+8QElJCd/61re4/PLLARg5ciRz587lvPPOY+rUqXz88ce9nn/uuefy85//vF/nbG5O/zSD5U//N6dvuIM/DfwOEy74sT4xzjOGATU1pRn53ZDMU/vlL7Vdfivk9uu+tnwRCAQ4+eSTuf/++5kwYQIAv/rVr3jrrbdSH+h1e+qpp/j1r3/NSy+9hGEkpi+deeaZfOc73+G8887r8zkz1e6F/Ht10LEtiIVTa1EBGKFWbIenx7YUK4Zz+4c4mlcSrxqZmJpoGBiRThxtH2OG2ygvdbG9qA7LVZJ6mtmxCc/aZzG7NmMV1WJYMVzr/4pr6xJshwfbU4ERD2EEWzDYfaPbphvsOIYd3+3+g41tuolXHIWzZXXGzmFhYhlOTDuOye5/LlHDjQE47Cg2JlFHYt1eT7xrr68dM9zETA9Rw00UJxHDRRwnlumkPNZCWbwlWYNB2PDiscOYWATNYtocNUQMV/L9mxi9ZWEQtZ1EcRDBQcR2YhlOLMOVuBIrAlaciGUSxcRhOjBMBxHLIJi8NDMRR2JgJ4I6w8Zl2sTjNgaJ8C7xZeF0GHhMiFtxLMtK/V4F8NJllhLCTSQOcdvANgwcWFTQQRkB2ijGb1cQYveDDWyM3Wzbvd0da2LjJI4DizAuwraLMC5CuIl/Jgzc3fM/u/2zx9h9OO6z9fbpPHbfnr+n8/Q+x76PS8e15eY8ffsZ9OW4PbXNvvb1x3prIIu+e3ZqGnE69bU/ldPbX/h8PubNm8e8efN67Vu9eudf4s8//3w2y+q3dyq/ygONxRx/2BlMyHUxIiIickhZtWoVsViMhoaG1LZx48Zxzz33YFlWj2URli1bxrhx41LTSwzDYOzYsSxdurRfoZQUAMPsFT7Z3oo9H286idWOJlY7uudz3CXEakcnRg3UlGI3d/T4H5ZVejjBhqt7vtb47yXm/e46HMGKJ0dmNWNEu8DhwXZ4sIpqdy44HwthRDsxIx2JYzs3Ywa3p67H8lVhFQ0EpxesGEakEzPoxwjtwLBtoHu+sZ1YJ6vkMIxYCEfLR5gBP7a7BNtThuUpx3YVY8QCmMEdGLEgWLFEMGZFwYqDFcWw41ieSqziARjRIGbHRmxvBaG6i7CKB+HcsgRP43OJc3bPqLCiidqsGEakA7NrM2ZgO5gObNOJGW7HCG5PnMcwsA0HGI5EexnJkRzhVkzbwrT3fMdyANcu+w3iPcIo2zDBdCVe24omzpfktCM44xF2f+up5PMxMLHx2cHUNp/Vhc/ae+C1V7tmMt2Dj/Y1+GpP+7ufv6dBNLkZ1CVy0IkZLnYYk4D9W7M7HXRP1jT4wsjDWLrlFL4y7qhclyIiIiKHGL/fT2VlJW73zk/2a2pqCIfDtLa29rg5jN/v55hjjunx/OrqatasWZO1ekWAnoEUJEKZohriRTV7fo7Lh+3yES/a//Vld+vI09P7ekmxQeOIDRqX/he2YonpklY88XMznGA6wHQmvjcMjFgQI9IJholtujDseCJcs20sX3Ui6NulDQw7Tk25k+3bmiEawoiFEiGgFYF4FCMewbAiWN4q4hXDE6FdaAdGtBOcPmzThRncjhlITp/chWFbiVAvHsGwYokQLB5JHGeYYLqxTQeGFQc7+WUlR8bZFmAkazUSYVoqoEtuN8yd23fZlnhsYiev04x0YoRbE+e2reRXPBFoeiux3WWY4TbMgB/iuwv8djcmag/jpPY01NMwwHQmaotHMGLhxPpssVDyWpOHfWbszO5fd+f3HreTcCS6hxL38FqfPXhPx+2yvfdoRnu33+655r3t25/adn6799r68vPsa22ffUr/2gpIhuQ79zmdDmKx+D5q68M590O8agTs7U6zWaBQKg1GDijhV/92Ymq4uYiIiEi2BIPBHoEUkHociUT6dOxnj9uXTK3jm1qjQ+sE5x21XRY5nNglg3a7K/Xjd3mxfZWp7clxYr2P635sOsBTAkU71yHaGwOguAabnSGiVVSFVX1sny5B0scwwFNdSmcB3g33UNB9x9+2HLZfpv7a7uu/BwqlRERERPKYx+PpFSp1P/Z6vX069rPH7Ut1dWbX3Mr060vmqO3ym9ovf6nt8tuh3H4KpURERETy2MCBA9mxYwexWAynM9G18/v9eL1eysrKeh3b3NzcY1tzczMDBgzo1zm3Z+gT3e5PjDP1+pI5arv8pvbLX2q7/FbI7dd9bfuiUEpEREQkj40aNQqn08nSpUsZP348AEuWLGH06NE9FjkHqK+v5/7770/dHty2bd577z2+853v9Ouctr335S4OVKZfXzJHbZff1H75S22X3w7l9tvT/QhEREREJA/4fD6mTZvGnDlzWL58OS+99BILFy7k0ksvBRKjpkKhEABTp06lvb2dW265hbVr13LLLbcQDAY566yzcnkJIiIicohSKCUiIiKS52bNmkVdXR2XXXYZN910EzNmzGDKlCkATJw4keeeew6AkpIS7r33XpYsWcJ5553HsmXLuO+++ygqKspl+SIiInKI0vQ9ERERkTzn8/mYN28e8+bN67Vv9erVPR6feOKJ/P73v89WaSIiIiJ7pJFSIiIiIiIiIiKSdQqlREREREREREQk6xRKiYiIiIiIiIhI1imUEhERERERERGRrFMoJSIiIiIiIiIiWXfI3X3PMDL7upl6fckctV1+U/vlL7Vdfivk9ivEa0o39afks9R2+U3tl7/UdvmtkNuvr9dk2LZtZ7YUERERERERERGRnjR9T0REREREREREsk6hlIiIiIiIiIiIZJ1CKRERERERERERyTqFUiIiIiIiIiIiknUKpUREREREREREJOsUSomIiIiIiIiISNYplBIRERERERERkaxTKCUiIiIiIiIiIlmnUOoAhcNhZs+ezfjx45k4cSILFy7MdUmyFy+++CIjR47s8TVz5kwAVq5cyQUXXEB9fT3nn38+K1asyHG1AhCJRDj77LN55513Uts2bNjA5ZdfzpgxY/jyl7/MG2+80eM5b775JmeffTb19fVceumlbNiwIdtlS9Lu2u/mm2/u9T589NFHU/ufffZZvvSlL1FfX8+1115LS0tLLko/ZG3dupWZM2dy0kknceqppzJ37lzC4TCg955klvpU+UP9qfykPlX+Un8qP6lP1TcKpQ7Q/PnzWbFiBYsWLeLGG2/krrvu4vnnn891WbIHa9eu5bTTTuONN95Ifd18880EAgGuuuoqxo8fz+9+9zsaGhq4+uqrCQQCuS75kBYOh/n+97/PmjVrUtts2+baa6+lpqaG3/72t3z1q19l+vTpNDU1AdDU1MS1117Leeedx1NPPUVVVRXXXHMNtm3n6jIOWbtrP4DGxkZ+8IMf9Hgfnn/++QAsX76cG264genTp/Pkk0/S3t7OrFmzclH+Icm2bWbOnEkwGOSxxx7j9ttv59VXX+WOO+7Qe08yTn2q/KH+VP5Rnyp/qT+Vn9Sn6gdb9ltXV5c9evRo++23305tu/vuu+2LL744h1XJ3vzgBz+wf/GLX/TavnjxYnvy5Mm2ZVm2bdu2ZVn2GWecYf/2t7/NdomStGbNGvsrX/mKfc4559gjRoxIvc/efPNNe8yYMXZXV1fq2Msuu8z+5S9/adu2bd9xxx093oOBQMBuaGjo8T6VzNtT+9m2bZ966qn266+/vtvn/fCHP7Svv/761OOmpiZ75MiR9vr16zNes9j22rVr7REjRth+vz+17ZlnnrEnTpyo955klPpU+UX9qfyiPlX+Un8qf6lP1XcaKXUAVq1aRSwWo6GhIbVt3LhxLFu2DMuycliZ7EljYyNHHnlkr+3Lli1j3LhxGIYBgGEYjB07lqVLl2a3QEl59913mTBhAk8++WSP7cuWLeP444+nqKgotW3cuHGptlq2bBnjx49P7fP5fNTV1akts2xP7dfZ2cnWrVt3+z6E3u132GGHMXjwYJYtW5bJciWptraWBx54gJqamh7bOzs79d6TjFKfKr+oP5Vf1KfKX+pP5S/1qfrOmesC8pnf76eyshK3253aVlNTQzgcprW1laqqqhxWJ59l2zYff/wxb7zxBvfeey/xeJypU6cyc+ZM/H4/xxxzTI/jq6urew2Tlez593//991u9/v9DBgwoMe26upqtmzZ0qf9kh17ar/GxkYMw+Cee+7hr3/9KxUVFXzzm9/k3HPPBWDbtm1qvxwqKyvj1FNPTT22LItHH32Uk08+We89ySj1qfKH+lP5R32q/KX+VP5Sn6rvFEodgGAw2KPzBKQeRyKRXJQke9HU1JRqszvuuIONGzdy8803EwqF9tiWaseDz77aSm15cFu3bh2GYTB8+HAuvvhi/v73v/Nf//VflJSUcMYZZxAKhdR+B5EFCxawcuVKnnrqKR5++GG99yRj1KfKH+pPFQ71qfKX+lP5R32qPVModQA8Hk+vX4zux16vNxclyV4cfvjhvPPOO5SXl2MYBqNGjcKyLH74wx9y0kkn7bYt1Y4HH4/HQ2tra49tu7bVnt6XZWVl2SpR9mLatGmcdtppVFRUAHDcccfxySef8Pjjj3PGGWfssf18Pl8Oqj20LViwgEWLFnH77bczYsQIvfcko9Snyh/qTxUO/b2ev9Sfyi/qU+2d1pQ6AAMHDmTHjh3EYrHUNr/fj9frLdhfmHxXUVGRWucA4OijjyYcDlNbW0tzc3OPY5ubm3sNm5TcGzhw4F7bak/7a2trs1aj7JlhGKkOVLfhw4ezdetWQO13sPjZz37GQw89xIIFCzjzzDMBvfcks9Snyi/qTxUG/b2ev9Sfyh/qU+2bQqkDMGrUKJxOZ48Fx5YsWcLo0aMxTf1oDzavv/46EyZMIBgMprZ9+OGHVFRUMG7cON5///3UbTZt2+a9996jvr4+V+XKHtTX1/PBBx8QCoVS25YsWZJqq/r6epYsWZLaFwwGWblypdryIPF//+//5fLLL++xbdWqVQwfPhzo3X6bN29m8+bNar8suuuuu3jiiSe47bbb+Nd//dfUdr33JJPUp8of6k8VDv29nr/Un8oP6lP1jf6VPwA+n49p06YxZ84cli9fzksvvcTChQu59NJLc12a7EZDQwMej4cf//jHrFu3jv/93/9l/vz5XHnllUydOpX29nZuueUW1q5dyy233EIwGOSss87KddnyGSeddBKHHXYYs2bNYs2aNdx3330sX76cr33tawCcf/75vPfee9x3332sWbOGWbNmMWTIECZMmJDjygXgtNNO4+9//zsPPvgg69ev53/+53/4wx/+wBVXXAHAN77xDf74xz+yePFiVq1axXXXXccXv/hFhg4dmuPKDw2NjY386le/4tvf/jbjxo3D7/envvTek0xSnyp/qD9VOPT3ev5Sf+rgpz5VP9hyQAKBgH3dddfZY8aMsSdOnGg/9NBDuS5J9uKjjz6yL7/8cnvMmDH2KaecYt955522ZVm2bdv2smXL7GnTptmjR4+2v/a1r9kffPBBjquVbiNGjLDffvvt1ONPPvnEvuiii+wTTjjB/td//Vf7b3/7W4/jX3vtNXvKlCn2iSeeaF922WX2+vXrs12y7OKz7ffiiy/a55xzjj169Gh76tSp9l/+8pcex//2t7+1J02aZI8ZM8a+9tpr7ZaWlmyXfMi699577REjRuz2y7b13pPMUp8qf6g/lb/Up8pf6k/lF/Wp+s6w7eT4WhERERERERERkSzR9D0REREREREREck6hVIiIiIiIiIiIpJ1CqVERERERERERCTrFEqJiIiIiIiIiEjWKZQSEREREREREZGsUyglIiIiIiIiIiJZp1BKRERERERERESyTqGUiIiIiIiIiIhknTPXBYiI7I/JkyezadOm3e575JFHmDBhQkbO+6Mf/QiAn//85xl5fREREZFsUX9KRHJNoZSI5K3Zs2fz5S9/udf28vLyHFQjIiIikn/UnxKRXFIoJSJ5q7S0lNra2lyXISIiIpK31J8SkVzSmlIiUpAmT57Mww8/zDnnnMOYMWO46qqr8Pv9qf2NjY1861vfYuzYsZx66qncddddWJaV2v/HP/6RqVOnUl9fz4UXXsjKlStT+zo7O/mP//gP6uvr+eIXv8gzzzyT1WsTERERyQb1p0Qk0xRKiUjBuvPOO7nyyit58sknCQaDzJgxA4CWlhb+/d//nQEDBrB48WJuvPFGHn30UR555BEAXn/9dW644QYuu+wynn76aU444QSuvvpqIpEIAC+++CJ1dXU8++yznHXWWcyePZuOjo6cXaeIiIhIpqg/JSKZZNi2bee6CBGR/po8eTJ+vx+ns+cs5MGDB/OnP/2JyZMn86UvfYnZs2cDsGHDBr70pS/xzDPP8Pbbb7Nw4UJeeuml1PMff/xx7r77bt544w2mT59OSUlJavHNSCTC7bffzhVXXMEvfvELPvnkE5544gkAOjo6GD9+PL/5zW+or6/P4k9ARERE5MCoPyUiuaY1pUQkb82cOZMpU6b02LZrp2rs2LGp74cOHUpFRQWNjY00NjZSV1fX49iGhgb8fj/t7e18/PHHXHjhhal9breb66+/vsdrdSstLQUgHA6n78JEREREskT9KRHJJYVSIpK3qqurGTZs2B73f/ZTv3g8jmmaeDyeXsd2r38Qj8d7Pe+zHA5Hr20adCoiIiL5SP0pEcklrSklIgVr1apVqe8//fRTOjo6GDlyJEcddRQffPAB0Wg0tf/999+nqqqKiooKhg0b1uO58XicyZMns2TJkqzWLyIiIpJr6k+JSCYplBKRvNXR0YHf7+/1FQgEAHjkkUd4+eWXWbVqFbNnz+aUU07hyCOP5JxzziESifCTn/yExsZGXnrpJe68806+8Y1vYBgGl1xyCU8//TS///3v+fTTT5k7dy62bVNXV5fjKxYRERFJL/WnRCSXNH1PRPLWrbfeyq233tpr+/e+9z0Azj33XG677TaampqYNGkSN910EwAlJSU88MAD3HLLLUybNo2qqiouu+wyrr76agA+97nPceONN3L33Xfj9/s54YQTuOeee/B6vdm7OBEREZEsUH9KRHJJd98TkYI0efJkpk+fznnnnZfrUkRERETykvpTIpJpmr4nIiIiIiIiIiJZp1BKRERERERERESyTtP3REREREREREQk6zRSSkREREREREREsk6hlIiIiIiIiIiIZJ1CKRERERERERERyTqFUiIiIiIiIiIiknUKpUREREREREREJOsUSomIiIiIiIiISNYplBIRERERERERkaxTKCUiIiIiIiIiIlmnUEpERERERERERLLu/wPvfknSgGFnmQAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "##          SINGLE VIDEO PREDICTION TEST         ##\n",
    "def predict_handsign(video_path, model):\n",
    "    # Process the video to extract landmarks\n",
    "    video_data = process_video(video_path)\n",
    "    \n",
    "    # Reshape the data to match the model input shape (add batch dimension for a single video)\n",
    "    video_data = video_data.reshape(1, frames_per_video, num_landmarks, num_coordinates)\n",
    "        # Make the prediction\n",
    "    prediction = model.predict(video_data)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    confidence = prediction[0][predicted_class]\n",
    "    \n",
    "    return predicted_class, confidence  \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the trained model\n",
    "    model = tf.keras.models.load_model('best_handsigns_model.keras')\n",
    "    \n",
    "    test_directory = \"Model testing videos\"\n",
    "    for video_file in os.listdir(test_directory):\n",
    "        if video_file.endswith(('.mp4', '.avi', '.mov')):\n",
    "            video_path = os.path.join(test_directory, video_file)\n",
    "            predicted_class, confidence = predict_handsign(video_path, model)\n",
    "            \n",
    "            # Get hand sign name\n",
    "            predicted_handsign = handsign_names.get(predicted_class, f\"HandSign_{predicted_class}\")\n",
    "            \n",
    "            # Apply confidence threshold\n",
    "            if confidence < 0.7:\n",
    "                predicted_handsign = \"Inseguro (\"+predicted_handsign+\")\"\n",
    "                \n",
    "            print(f\"Video: {video_file}\")\n",
    "            print(f\"Predicted Hand Sign: {predicted_handsign}\")\n",
    "            print(f\"Confidence: {confidence:.2f}\")\n",
    "            print(\"--------------------\")\n"
   ],
   "id": "4f9ed1d4df51b1c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T01:26:16.187817Z",
     "start_time": "2024-10-20T01:25:16.955170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          CONTINUOUS PREDICTION WITH SLIDING WINDOW           ##\n",
    "\n",
    "import collections\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from scipy.interpolate import interp1d  # For smoothing landmarks\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('best_handsigns_model.keras')\n",
    "\n",
    "# Initialize Mediapipe solutions outside the loop for efficiency\n",
    "mp_hands = mp.solutions.hands.Hands(static_image_mode=False, \n",
    "                                    max_num_hands=2, \n",
    "                                    min_detection_confidence=0.4,  # Lowered confidence to allow for fast movement detection\n",
    "                                    min_tracking_confidence=0.4)   # Lowered tracking confidence\n",
    "mp_pose = mp.solutions.pose.Pose(static_image_mode=False, \n",
    "                                 min_detection_confidence=0.4, \n",
    "                                 min_tracking_confidence=0.4)\n",
    "\n",
    "# Open webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Sliding window buffer for frames\n",
    "frame_buffer = collections.deque(maxlen=frames_per_video)\n",
    "\n",
    "# To smooth predictions, keep track of recent predictions\n",
    "prediction_buffer = collections.deque(maxlen=3)\n",
    "\n",
    "# Store the last prediction\n",
    "last_prediction = \"No Prediction\"\n",
    "last_confidence = 0.0\n",
    "\n",
    "# Track missing hands to reset landmarks if missing for too long\n",
    "hand_missing_threshold = 5\n",
    "left_hand_missing_count = 0\n",
    "right_hand_missing_count = 0\n",
    "\n",
    "# Store the last known hand landmarks to compare in future frames\n",
    "last_left_hand_landmarks = None\n",
    "last_right_hand_landmarks = None\n",
    "\n",
    "# Movement delta threshold for fast movements\n",
    "movement_threshold = 0.9  # Adjusted threshold for movement delta\n",
    "\n",
    "# Smoothing factor for missing landmarks\n",
    "smoothing_factor = 0.8  # Weight to smooth landmarks during quick movements\n",
    "\n",
    "# Draw landmarks on the frame\n",
    "def draw_landmarks(frame, landmarks):\n",
    "    \"\"\"Draw landmarks on the frame.\"\"\"\n",
    "    for i, (x, y, z) in enumerate(landmarks):\n",
    "        h, w, _ = frame.shape\n",
    "        x = int(x * w + 325)\n",
    "        y = int(y * h + 250)\n",
    "        \n",
    "        if i < 21:  # Left hand landmarks\n",
    "            color = (0, 255, 0)\n",
    "        elif i < 42:  # Right hand landmarks\n",
    "            color = (0, 0, 255)\n",
    "        else:  # Body landmarks\n",
    "            color = (255, 0, 0)\n",
    "        \n",
    "        cv2.circle(frame, (x, y), 5, color, -1)\n",
    "\n",
    "def smooth_landmarks(new_landmarks, old_landmarks):\n",
    "    \"\"\"Smooth landmarks by interpolating between old and new.\"\"\"\n",
    "    if old_landmarks is None:\n",
    "        return new_landmarks\n",
    "\n",
    "    return old_landmarks * (1 - smoothing_factor) + new_landmarks * smoothing_factor\n",
    "\n",
    "def process_frame(frame):\n",
    "    global last_left_hand_landmarks, last_right_hand_landmarks\n",
    "    global left_hand_missing_count, right_hand_missing_count\n",
    "\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    hands_results = mp_hands.process(image)\n",
    "    pose_results = mp_pose.process(image)\n",
    "\n",
    "    # Initialize a zero-filled array for landmarks (51 landmarks, each with x, y, z)\n",
    "    landmarks = np.zeros((num_landmarks, num_coordinates))\n",
    "\n",
    "    # Detect nose for relative normalization\n",
    "    try:\n",
    "        nose_landmark = pose_results.pose_landmarks.landmark[0]\n",
    "    except:\n",
    "        class nose_landmark:\n",
    "            x = 0\n",
    "            y = 0\n",
    "            z = 0\n",
    "        nose_landmark = nose_landmark()\n",
    "\n",
    "    if hands_results.multi_hand_landmarks:\n",
    "        handedness_labels = [hand.classification[0].label for hand in hands_results.multi_handedness]\n",
    "\n",
    "        for i, hand_landmarks in enumerate(hands_results.multi_hand_landmarks):\n",
    "            if handedness_labels[i] == 'Left':\n",
    "                left_hand = np.array([(lm.x - nose_landmark.x, lm.y - nose_landmark.y, lm.z - nose_landmark.z) \n",
    "                                      for lm in hand_landmarks.landmark])\n",
    "\n",
    "                # Smooth the transition between frames\n",
    "                left_hand = smooth_landmarks(left_hand, last_left_hand_landmarks)\n",
    "                \n",
    "                # Update the stored landmarks for the next frame\n",
    "                last_left_hand_landmarks = left_hand\n",
    "                landmarks[:21] = left_hand  # Insert the left hand landmarks into the first 21 slots\n",
    "\n",
    "            elif handedness_labels[i] == 'Right':\n",
    "                right_hand = np.array([(lm.x - nose_landmark.x, lm.y - nose_landmark.y, lm.z - nose_landmark.z) \n",
    "                                       for lm in hand_landmarks.landmark])\n",
    "\n",
    "                # Smooth the transition between frames\n",
    "                right_hand = smooth_landmarks(right_hand, last_right_hand_landmarks)\n",
    "                \n",
    "                # Update the stored landmarks for the next frame\n",
    "                last_right_hand_landmarks = right_hand\n",
    "                landmarks[21:42] = right_hand  # Insert the right hand landmarks into slots 21-42\n",
    "\n",
    "        # Reset the missing counts if hands are detected\n",
    "        left_hand_missing_count = 0\n",
    "        right_hand_missing_count = 0\n",
    "    else:\n",
    "        # Increment missing count when hands are not detected\n",
    "        left_hand_missing_count += 1\n",
    "        right_hand_missing_count += 1\n",
    "\n",
    "        # Reuse last known landmarks if available and hands are missing for too long\n",
    "        if last_left_hand_landmarks is not None:\n",
    "            landmarks[:21] = last_left_hand_landmarks\n",
    "        if last_right_hand_landmarks is not None:\n",
    "            landmarks[21:42] = last_right_hand_landmarks\n",
    "\n",
    "        # Reset landmarks if hands are missing for too long\n",
    "        if left_hand_missing_count > hand_missing_threshold:\n",
    "            last_left_hand_landmarks = None\n",
    "        if right_hand_missing_count > hand_missing_threshold:\n",
    "            last_right_hand_landmarks = None\n",
    "\n",
    "    # Fill in body landmarks (9 selected)\n",
    "    selected_body_landmarks = [0, 11, 12, 13, 14, 15, 16, 23, 24]\n",
    "    if pose_results.pose_landmarks:\n",
    "        for idx, landmark_idx in enumerate(selected_body_landmarks):\n",
    "            lm = pose_results.pose_landmarks.landmark[landmark_idx]\n",
    "            landmarks[42 + idx] = (lm.x - nose_landmark.x, lm.y - nose_landmark.y, lm.z - nose_landmark.z)\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "# Make a prediction based on the buffer\n",
    "def predict_handsign(buffer):\n",
    "    \"\"\"Make a prediction based on a buffer of frames.\"\"\"    \n",
    "    video_data = np.array(buffer)\n",
    "    video_data = video_data.reshape(1, frames_per_video, num_landmarks, num_coordinates)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(video_data, verbose=0)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    confidence = prediction[0][predicted_class]\n",
    "\n",
    "    return predicted_class, confidence\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Process every frame (no skipping)\n",
    "    landmarks = process_frame(frame)\n",
    "\n",
    "    # Add the landmarks to the frame buffer\n",
    "    frame_buffer.append(landmarks)\n",
    "\n",
    "    # Draw the landmarks on the frame\n",
    "    draw_landmarks(frame, landmarks)\n",
    "\n",
    "    # Once the buffer is full, make a prediction using the sliding window\n",
    "    if len(frame_buffer) == frames_per_video:\n",
    "        predicted_class, confidence = predict_handsign(frame_buffer)\n",
    "        predicted_handsign = handsign_names.get(predicted_class, f\"HandSign_{predicted_class}\")\n",
    "\n",
    "        # Update the last prediction\n",
    "        last_prediction = predicted_handsign\n",
    "        last_confidence = confidence\n",
    "\n",
    "        # Store prediction and confidence in the buffer for smoothing\n",
    "        #print((predicted_class, confidence))\n",
    "        prediction_buffer.append((predicted_class, confidence))\n",
    "\n",
    "        # Average the last N predictions to smooth the output\n",
    "        avg_pred_class = np.argmax(np.bincount([p[0] for p in prediction_buffer]))\n",
    "        avg_confidence = np.mean([p[1] for p in prediction_buffer if p[0] == avg_pred_class])\n",
    "\n",
    "        if avg_confidence > 0.8:\n",
    "            last_prediction = handsign_names.get(avg_pred_class, f\"HandSign_{avg_pred_class}\")\n",
    "            last_confidence = avg_confidence\n",
    "        elif 0.45 < avg_confidence < 0.8:\n",
    "            last_prediction = \"deteccion insegura: \"+handsign_names.get(avg_pred_class, f\"HandSign_{avg_pred_class}\")\n",
    "            last_confidence = avg_confidence\n",
    "        else:\n",
    "            last_prediction = \"deteccion nula\"\n",
    "            last_confidence = avg_confidence\n",
    "            \n",
    "    # Display the last prediction on the frame\n",
    "    cv2.putText(frame, f\"Predicted: {last_prediction} Conf: {last_confidence:.2f}\", \n",
    "                (10, 30), cv2.FONT_ITALIC, 0.7, (0, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow('Hands Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "ee106e95cdc75678",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:45:00.067152Z",
     "start_time": "2024-10-20T16:44:30.673810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##         V2 CONTINUOUS PREDICTION WITH SLIDING WINDOW           ##\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Parameters for video feed and predictions\n",
    "show_landmarks = True  # Toggle to show hand landmarks\n",
    "show_connections = True  # Toggle to show landmark connections\n",
    "show_prediction_text = True  # Toggle to show prediction and accuracy on screen\n",
    "accuracy_threshold = 0.7  # Threshold for highlighting low accuracy\n",
    "sliding_window_size = frames_per_video  # Size of the sliding window for prediction\n",
    "smoothing_buffer_size = 10  # Buffer size for prediction smoothing (larger = smoother)\n",
    "mediapipe_confidence = 0.5  # Confidence threshold for mediapipe detection\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('handsigns_model.h5')\n",
    "\n",
    "# Initialize MediaPipe hands and pose solutions\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "hands = mp_hands.Hands(min_detection_confidence=mediapipe_confidence, min_tracking_confidence=mediapipe_confidence)\n",
    "pose = mp_pose.Pose(min_detection_confidence=mediapipe_confidence, min_tracking_confidence=mediapipe_confidence)\n",
    "\n",
    "# Set up sliding window and smoothing buffer\n",
    "sliding_window = deque(maxlen=sliding_window_size)\n",
    "smoothing_buffer = deque(maxlen=smoothing_buffer_size)\n",
    "\n",
    "# Initialize video capture (webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "last_handedness = {'Left': False, 'Right': False}\n",
    "\n",
    "# Function to smooth predictions\n",
    "def smooth_predictions(predictions):\n",
    "    if len(predictions) > 0:\n",
    "        return np.mean(predictions, axis=0)\n",
    "    return np.zeros(num_handsigns)\n",
    "\n",
    "# Main loop for video feed and prediction\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    #frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Process frame to extract landmarks and results\n",
    "    landmarks, last_handedness = process_frame(frame, hands, pose, last_handedness)\n",
    "    hands_results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    pose_results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Append the landmarks to the sliding window\n",
    "    sliding_window.append(landmarks)\n",
    "    \n",
    "    # Check if the sliding window is full\n",
    "    if len(sliding_window) == sliding_window_size:\n",
    "        # Prepare input for the model (reshape and normalize)\n",
    "        input_data = np.array(sliding_window).reshape(1, sliding_window_size, num_landmarks, num_coordinates)\n",
    "        \n",
    "        # Predict the hand sign\n",
    "        predictions = model.predict(input_data)\n",
    "        prediction = np.argmax(predictions)\n",
    "        confidence = np.max(predictions)\n",
    "        \n",
    "        # Add the prediction to the smoothing buffer\n",
    "        smoothing_buffer.append(predictions)\n",
    "        smoothed_predictions = smooth_predictions(smoothing_buffer)\n",
    "        smoothed_prediction = np.argmax(smoothed_predictions)\n",
    "        smoothed_confidence = np.max(smoothed_predictions)\n",
    "\n",
    "        # Display prediction on screen if enabled\n",
    "        if show_prediction_text:\n",
    "            text = f\"Sign: {handsign_names[smoothed_prediction]}, Accuracy: {smoothed_confidence:.2f}\"\n",
    "            if smoothed_confidence < accuracy_threshold:\n",
    "                text += \" (Low accuracy)\"\n",
    "            cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    # Draw landmarks if enabled\n",
    "    if show_landmarks and hands_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "            if show_connections:\n",
    "                mp.solutions.drawing_utils.draw_landmarks(\n",
    "                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            else:\n",
    "                mp.solutions.drawing_utils.draw_landmarks(frame, hand_landmarks)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Hand Sign Recognition', frame)\n",
    "\n",
    "    # Break the loop with 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ],
   "id": "41e6691dae6fe336",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 157ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 32ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 25ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "819a4814bfc8aec8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
