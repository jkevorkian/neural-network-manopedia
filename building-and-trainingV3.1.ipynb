{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-11T04:14:49.129258Z",
     "start_time": "2024-09-11T04:14:49.104171Z"
    }
   },
   "source": [
    "##          DATA SHAPE DEFINITION           ##\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameters for the data shape\n",
    "num_handsigns = 4\n",
    "videos_per_handsign = 50\n",
    "frames_per_video = 30\n",
    "num_landmarks = 51\n",
    "num_coordinates = 3\n",
    "\n",
    "# Generate dummy data\n",
    "data = [np.random.rand(videos_per_handsign, frames_per_video, num_landmarks, num_coordinates) for _ in range(num_handsigns)]\n",
    "\n",
    "# Convert the list to a numpy array with shape (num_handsigns, videos_per_handsign, frames_per_video, num_landmarks, num_coordinates)\n",
    "data_array = np.array(data)\n",
    "\n",
    "# Save the data array to a .npy file\n",
    "np.save('handsigns_data.npy', data_array)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:14:56.170012Z",
     "start_time": "2024-09-11T04:14:56.157377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          PROCESS VIDEO DATASET FUNC DEFINITIONS         ##\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_landmarks(landmarks, head_idx=0):\n",
    "    # Normalize based on the head as the origin (using the head_idx landmark as the reference point)\n",
    "    origin = landmarks[head_idx]\n",
    "    normalized_landmarks = landmarks - origin\n",
    "    return normalized_landmarks\n",
    "\n",
    "def rotate_landmarks(landmarks, angle):\n",
    "    # Apply rotation to the 3D landmarks\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(angle), -np.sin(angle), 0],\n",
    "        [np.sin(angle), np.cos(angle), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    rotated_landmarks = np.dot(landmarks, rotation_matrix)\n",
    "    return rotated_landmarks\n",
    "\n",
    "def extract_landmarks(image, hands_results, pose_results, apply_rotation=False, rotation_angle=0.1, head_idx=0):\n",
    "    landmarks = []\n",
    "    \n",
    "    # Extract left hand landmarks (21 landmarks)\n",
    "    if hands_results.multi_hand_landmarks and len(hands_results.multi_hand_landmarks) > 0:\n",
    "        hand_landmarks = [(lm.x, lm.y, lm.z) for lm in hands_results.multi_hand_landmarks[0].landmark]\n",
    "        if apply_rotation:\n",
    "            hand_landmarks = rotate_landmarks(np.array(hand_landmarks), rotation_angle)\n",
    "        landmarks.extend(hand_landmarks)\n",
    "    else:\n",
    "        landmarks.extend([(0, 0, 0)] * 21)\n",
    "    \n",
    "    # Extract right hand landmarks (21 landmarks)\n",
    "    if hands_results.multi_hand_landmarks and len(hands_results.multi_hand_landmarks) > 1:\n",
    "        hand_landmarks = [(lm.x, lm.y, lm.z) for lm in hands_results.multi_hand_landmarks[1].landmark]\n",
    "        if apply_rotation:\n",
    "            hand_landmarks = rotate_landmarks(np.array(hand_landmarks), rotation_angle)\n",
    "        landmarks.extend(hand_landmarks)\n",
    "    else:\n",
    "        landmarks.extend([(0, 0, 0)] * 21)\n",
    "    \n",
    "    # Extract selected body landmarks (9 landmarks)\n",
    "    selected_body_landmarks = [0, 11, 12, 13, 14, 15, 16, 23, 24]  # Landmarks for nose, arms, and shoulders\n",
    "    if pose_results.pose_landmarks:\n",
    "        for idx in selected_body_landmarks:\n",
    "            lm = pose_results.pose_landmarks.landmark[idx]\n",
    "            landmarks.append((lm.x, lm.y, lm.z))\n",
    "    else:\n",
    "        landmarks.extend([(0, 0, 0)] * 9)\n",
    "\n",
    "    landmarks = np.array(landmarks)\n",
    "    landmarks = normalize_landmarks(landmarks, head_idx)  # Normalize landmarks with the head as the origin\n",
    "    \n",
    "    return landmarks\n",
    "\n",
    "def process_video(video_path, apply_rotation=False, rotation_angle=0.1, head_idx=0):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    indices = np.linspace(0, total_frames - 1, frames_per_video, dtype=int)\n",
    "    frame_set = set(indices)\n",
    "    frame_count = 0\n",
    "    \n",
    "    with mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5) as hands, \\\n",
    "         mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        \n",
    "        while cap.isOpened() and len(frames) < frames_per_video:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            if frame_count in frame_set:\n",
    "                # Convert the BGR image to RGB\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Process the image and extract landmarks\n",
    "                hands_results = hands.process(image)\n",
    "                pose_results = pose.process(image)\n",
    "                \n",
    "                # Extract landmarks\n",
    "                landmarks = extract_landmarks(image, hands_results, pose_results, apply_rotation, rotation_angle, head_idx)\n",
    "                \n",
    "                frames.append(landmarks)\n",
    "            \n",
    "            frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Pad if we don't have enough frames\n",
    "    if len(frames) < frames_per_video:\n",
    "        frames.extend([np.zeros((51, 3))] * (frames_per_video - len(frames)))\n",
    "    \n",
    "    return np.array(frames)\n",
    "\n",
    "def process_dataset(root_path, apply_rotation=False, rotation_angle=0.1, head_idx=0):\n",
    "    data = []\n",
    "    \n",
    "    for handsign in tqdm(range(num_handsigns), desc=\"Processing handsigns\"):\n",
    "        handsign_path = os.path.join(root_path, f\"handsign_{handsign+1}\")  # Changed to match your folder naming\n",
    "        if not os.path.exists(handsign_path):\n",
    "            print(f\"Warning: Directory {handsign_path} does not exist. Skipping.\")\n",
    "            data.append(np.zeros((videos_per_handsign, frames_per_video, 51, 3)))  # 51 landmarks total\n",
    "            continue\n",
    "        \n",
    "        videos = [f for f in os.listdir(handsign_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        videos = videos[:videos_per_handsign]  # Limit to videos_per_handsign\n",
    "        \n",
    "        handsign_data = []\n",
    "        for video in tqdm(videos, desc=f\"Processing videos for handsign {handsign}\", leave=False):\n",
    "            video_path = os.path.join(handsign_path, video)\n",
    "            video_data = process_video(video_path, apply_rotation, rotation_angle, head_idx)\n",
    "            handsign_data.append(video_data)\n",
    "        \n",
    "        # Pad if we don't have enough videos\n",
    "        if len(handsign_data) < videos_per_handsign:\n",
    "            handsign_data.extend([np.zeros((frames_per_video, 51, 3))] * (videos_per_handsign - len(handsign_data)))\n",
    "        \n",
    "        data.append(np.array(handsign_data))\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "664337e413404390",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:17:31.197211Z",
     "start_time": "2024-09-11T04:15:08.677873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          PROCESS VIDEOS DATASET FUNC CALLING         ##\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_path = \"TestDataset\"  # Replace with your dataset root path\n",
    "    apply_rotation_augmentation = False  # Set this to False to skip rotation augmentation\n",
    "    rotation_angle = 0.1  # Set the desired rotation angle\n",
    "    head_idx = 0  # Set the index of the head landmark (change as per dataset)\n",
    "    \n",
    "    data_array = process_dataset(root_path, apply_rotation_augmentation, rotation_angle, head_idx)\n",
    "    \n",
    "    # Save the data array to a .npy file\n",
    "    np.save('handsigns_data.npy', data_array)\n",
    "    print(\"Data saved to handsigns_data.npy\")\n",
    "\n",
    "\n"
   ],
   "id": "a73a3c5493ed9527",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing handsigns:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Processing videos for handsign 0:   0%|          | 0/50 [00:00<?, ?it/s]\u001B[AC:\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "\n",
      "Processing videos for handsign 0:   2%|▏         | 1/50 [00:02<02:25,  2.98s/it]\u001B[A\n",
      "Processing videos for handsign 0:   4%|▍         | 2/50 [00:05<02:24,  3.00s/it]\u001B[A\n",
      "Processing videos for handsign 0:   6%|▌         | 3/50 [00:08<02:17,  2.92s/it]\u001B[A\n",
      "Processing videos for handsign 0:   8%|▊         | 4/50 [00:11<02:13,  2.89s/it]\u001B[A\n",
      "Processing videos for handsign 0:  10%|█         | 5/50 [00:14<02:06,  2.81s/it]\u001B[A\n",
      "Processing videos for handsign 0:  12%|█▏        | 6/50 [00:17<02:03,  2.81s/it]\u001B[A\n",
      "Processing videos for handsign 0:  14%|█▍        | 7/50 [00:19<02:00,  2.80s/it]\u001B[A\n",
      "Processing videos for handsign 0:  16%|█▌        | 8/50 [00:22<01:56,  2.77s/it]\u001B[A\n",
      "Processing videos for handsign 0:  18%|█▊        | 9/50 [00:25<01:53,  2.76s/it]\u001B[A\n",
      "Processing videos for handsign 0:  20%|██        | 10/50 [00:28<01:49,  2.75s/it]\u001B[A\n",
      "Processing videos for handsign 0:  22%|██▏       | 11/50 [00:30<01:46,  2.72s/it]\u001B[A\n",
      "Processing videos for handsign 0:  24%|██▍       | 12/50 [00:33<01:45,  2.77s/it]\u001B[A\n",
      "Processing videos for handsign 0:  26%|██▌       | 13/50 [00:36<01:41,  2.74s/it]\u001B[A\n",
      "Processing videos for handsign 0:  28%|██▊       | 14/50 [00:38<01:37,  2.72s/it]\u001B[A\n",
      "Processing videos for handsign 0:  30%|███       | 15/50 [00:41<01:34,  2.71s/it]\u001B[A\n",
      "Processing videos for handsign 0:  32%|███▏      | 16/50 [00:44<01:34,  2.77s/it]\u001B[A\n",
      "Processing videos for handsign 0:  34%|███▍      | 17/50 [00:47<01:32,  2.81s/it]\u001B[A\n",
      "Processing videos for handsign 0:  36%|███▌      | 18/50 [00:50<01:30,  2.84s/it]\u001B[A\n",
      "Processing videos for handsign 0:  38%|███▊      | 19/50 [00:53<01:29,  2.87s/it]\u001B[A\n",
      "Processing videos for handsign 0:  40%|████      | 20/50 [00:56<01:25,  2.86s/it]\u001B[A\n",
      "Processing videos for handsign 0:  42%|████▏     | 21/50 [00:59<01:23,  2.88s/it]\u001B[A\n",
      "Processing videos for handsign 0:  44%|████▍     | 22/50 [01:01<01:20,  2.89s/it]\u001B[A\n",
      "Processing videos for handsign 0:  46%|████▌     | 23/50 [01:05<01:21,  3.01s/it]\u001B[A\n",
      "Processing videos for handsign 0:  48%|████▊     | 24/50 [01:08<01:21,  3.12s/it]\u001B[A\n",
      "Processing videos for handsign 0:  50%|█████     | 25/50 [01:11<01:18,  3.13s/it]\u001B[A\n",
      "Processing videos for handsign 0:  52%|█████▏    | 26/50 [01:14<01:12,  3.04s/it]\u001B[A\n",
      "Processing videos for handsign 0:  54%|█████▍    | 27/50 [01:17<01:08,  2.99s/it]\u001B[A\n",
      "Processing videos for handsign 0:  56%|█████▌    | 28/50 [01:20<01:04,  2.94s/it]\u001B[A\n",
      "Processing videos for handsign 0:  58%|█████▊    | 29/50 [01:23<01:01,  2.91s/it]\u001B[A\n",
      "Processing videos for handsign 0:  60%|██████    | 30/50 [01:26<00:58,  2.92s/it]\u001B[A\n",
      "Processing videos for handsign 0:  62%|██████▏   | 31/50 [01:28<00:54,  2.86s/it]\u001B[A\n",
      "Processing videos for handsign 0:  64%|██████▍   | 32/50 [01:31<00:50,  2.81s/it]\u001B[A\n",
      "Processing videos for handsign 0:  66%|██████▌   | 33/50 [01:34<00:48,  2.86s/it]\u001B[A\n",
      "Processing videos for handsign 0:  68%|██████▊   | 34/50 [01:37<00:46,  2.93s/it]\u001B[A\n",
      "Processing videos for handsign 0:  70%|███████   | 35/50 [01:40<00:43,  2.87s/it]\u001B[A\n",
      "Processing videos for handsign 0:  72%|███████▏  | 36/50 [01:43<00:39,  2.83s/it]\u001B[A\n",
      "Processing videos for handsign 0:  74%|███████▍  | 37/50 [01:45<00:36,  2.78s/it]\u001B[A\n",
      "Processing videos for handsign 0:  76%|███████▌  | 38/50 [01:48<00:32,  2.73s/it]\u001B[A\n",
      "Processing videos for handsign 0:  78%|███████▊  | 39/50 [01:51<00:31,  2.82s/it]\u001B[A\n",
      "Processing videos for handsign 0:  80%|████████  | 40/50 [01:54<00:27,  2.78s/it]\u001B[A\n",
      "Processing videos for handsign 0:  82%|████████▏ | 41/50 [01:57<00:25,  2.89s/it]\u001B[A\n",
      "Processing videos for handsign 0:  84%|████████▍ | 42/50 [02:00<00:22,  2.87s/it]\u001B[A\n",
      "Processing videos for handsign 0:  86%|████████▌ | 43/50 [02:02<00:19,  2.81s/it]\u001B[A\n",
      "Processing videos for handsign 0:  88%|████████▊ | 44/50 [02:05<00:16,  2.76s/it]\u001B[A\n",
      "Processing videos for handsign 0:  90%|█████████ | 45/50 [02:08<00:14,  2.85s/it]\u001B[A\n",
      "Processing videos for handsign 0:  92%|█████████▏| 46/50 [02:11<00:11,  2.90s/it]\u001B[A\n",
      "Processing videos for handsign 0:  94%|█████████▍| 47/50 [02:14<00:08,  2.90s/it]\u001B[A\n",
      "Processing videos for handsign 0:  96%|█████████▌| 48/50 [02:17<00:05,  2.87s/it]\u001B[A\n",
      "Processing videos for handsign 0:  98%|█████████▊| 49/50 [02:19<00:02,  2.80s/it]\u001B[A\n",
      "Processing videos for handsign 0: 100%|██████████| 50/50 [02:22<00:00,  2.78s/it]\u001B[A\n",
      "Processing handsigns: 100%|██████████| 4/4 [02:22<00:00, 35.63s/it]              \u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Directory TestDataset\\handsign_2 does not exist. Skipping.\n",
      "Warning: Directory TestDataset\\handsign_3 does not exist. Skipping.\n",
      "Warning: Directory TestDataset\\handsign_4 does not exist. Skipping.\n",
      "Data saved to handsigns_data.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:25:18.617511Z",
     "start_time": "2024-09-11T04:25:18.521855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          MODEL DEFINITION            ##\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, Dense, Reshape, Dropout, BatchNormalization, Input\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    # Input layer with the original shape\n",
    "    Input(shape=(frames_per_video, num_landmarks, num_coordinates)),\n",
    "    \n",
    "    # Reshape layer to convert input shape from (30, 51, 3) to (30, 153)\n",
    "    Reshape((frames_per_video, num_landmarks * num_coordinates)),\n",
    "    \n",
    "    # Convolution layers for feature extraction\n",
    "    Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # No flatten layer here; keeping the output 3D for the LSTM\n",
    "    # Output shape at this point will be (frames_per_video, features)\n",
    "    \n",
    "    # LSTM layer to process temporal information\n",
    "    LSTM(128, return_sequences=False),  # No need to return sequences as this is the final LSTM layer\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Output layer with softmax activation for classification\n",
    "    Dense(num_handsigns, activation='softmax')  # Assuming num_handsigns is defined globally\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ],
   "id": "9483a42ae20bbfb3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_1 (\u001B[38;5;33mReshape\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m153\u001B[0m)        │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001B[38;5;33mConv1D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m64\u001B[0m)         │        \u001B[38;5;34m29,440\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m64\u001B[0m)         │           \u001B[38;5;34m256\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001B[38;5;33mConv1D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m128\u001B[0m)        │        \u001B[38;5;34m24,704\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m128\u001B[0m)        │           \u001B[38;5;34m512\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001B[38;5;33mLSTM\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │       \u001B[38;5;34m131,584\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │        \u001B[38;5;34m16,512\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m4\u001B[0m)              │           \u001B[38;5;34m516\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">153</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">29,440</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m203,524\u001B[0m (795.02 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">203,524</span> (795.02 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m203,140\u001B[0m (793.52 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">203,140</span> (793.52 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m384\u001B[0m (1.50 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:25:34.098985Z",
     "start_time": "2024-09-11T04:25:33.404301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          MODEL TRAINING          ##\n",
    "\n",
    "# Load the data from the .npy file\n",
    "data_array = np.load('handsigns_data.npy')\n",
    "\n",
    "# X remains unchanged\n",
    "X = data_array  # Shape: (4, 50, 30, 51, 3)\n",
    "\n",
    "# Assuming you have labels for your handsigns, define y as follows:\n",
    "y = np.array([i for i in range(num_handsigns) for _ in range(videos_per_handsign)])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the data for training\n",
    "X_train_reshaped = X_train.reshape(-1, frames_per_video, num_landmarks, num_coordinates)\n",
    "X_test_reshaped = X_test.reshape(-1, frames_per_video, num_landmarks, num_coordinates)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test), epochs=10, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ],
   "id": "3b9dce4cc0ddf1e8",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4, 200]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Split the data into training and testing sets\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m---> 14\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Reshape the data for training\u001B[39;00m\n\u001B[0;32m     17\u001B[0m X_train_reshaped \u001B[38;5;241m=\u001B[39m X_train\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, frames_per_video, num_landmarks, num_coordinates)\n",
      "File \u001B[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2657\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2654\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_arrays \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   2655\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt least one array required as input\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2657\u001B[0m arrays \u001B[38;5;241m=\u001B[39m \u001B[43mindexable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2659\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m   2660\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m _validate_shuffle_split(\n\u001B[0;32m   2661\u001B[0m     n_samples, test_size, train_size, default_test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m\n\u001B[0;32m   2662\u001B[0m )\n",
      "File \u001B[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:514\u001B[0m, in \u001B[0;36mindexable\u001B[1;34m(*iterables)\u001B[0m\n\u001B[0;32m    484\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001B[39;00m\n\u001B[0;32m    485\u001B[0m \n\u001B[0;32m    486\u001B[0m \u001B[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    510\u001B[0m \u001B[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <3x1 sparse matrix ...>]\u001B[39;00m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    513\u001B[0m result \u001B[38;5;241m=\u001B[39m [_make_indexable(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m iterables]\n\u001B[1;32m--> 514\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    455\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 457\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    458\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    459\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    460\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [4, 200]"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
