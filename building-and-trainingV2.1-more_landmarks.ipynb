{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T22:03:55.815950Z",
     "start_time": "2024-11-13T22:03:55.786850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          DATA SHAPE DEFINITION           ##\n",
    "import os, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define the parameters for the data shape\n",
    "videos_per_handsign = 150  # Adjust if necessary\n",
    "frames_per_video = 8\n",
    "num_landmarks = 65\n",
    "num_coordinates = 3\n",
    "\n",
    "num_additional_samples = 1       #how many different samples of frames_per_video frames will be taken from each video (careful to not add too many if vids are short)\n",
    "\n",
    "num_augmented_versions = 1      #Add noise and transformation to generate, from each video extracted, a num_augmented_versions number of new ones (also keep between 1-4)\n",
    "if num_augmented_versions>0:\n",
    "    data_augmentation = True\n",
    "else: data_augmentation = False\n",
    "\n",
    "#redefine videos per handsign to accomodate for additional samples to be taken before generating the dummy array (this is done so there's no shaping/dimensions size incompatibility issue)\n",
    "videos_per_handsign = videos_per_handsign * (1+num_additional_samples)\n",
    "\n",
    "# Define the root directory containing handsign folders\n",
    "root_path = \"NomenclatedDataset - Copy\"\n",
    "\n",
    "def get_handsign_folders(root_path):\n",
    "    handsign_names = {}\n",
    "    handsign_video_counts = {}  # To store the video count for each handsign\n",
    "    handsign_count = 0\n",
    "    \n",
    "    # Walk through the root directory\n",
    "    for folder in os.listdir(root_path):\n",
    "        folder_path = os.path.join(root_path, folder)\n",
    "        \n",
    "        # Ignore non-directories\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        # If the folder starts with \"#\", check its subdirectories\n",
    "        if folder.startswith(\"#\"):\n",
    "            for subfolder in os.listdir(folder_path):\n",
    "                subfolder_path = os.path.join(folder_path, subfolder)\n",
    "                if os.path.isdir(subfolder_path):\n",
    "                    handsign_names[handsign_count] = subfolder\n",
    "                    # Count videos in the subfolder\n",
    "                    videos = [f for f in os.listdir(subfolder_path) if f.endswith(('.mp4', '.avi', '.MOV'))]\n",
    "                    handsign_video_counts[subfolder] = len(videos)\n",
    "                    handsign_count += 1\n",
    "        else:\n",
    "            # Directly add the folder as a handsign\n",
    "            handsign_names[handsign_count] = folder\n",
    "            # Count videos in the folder\n",
    "            videos = [f for f in os.listdir(folder_path) if f.endswith(('.mp4', '.avi', '.MOV'))]\n",
    "            handsign_video_counts[folder] = len(videos)\n",
    "            handsign_count += 1\n",
    "    \n",
    "    return handsign_names, handsign_video_counts\n",
    "\n",
    "# Get the handsign names and video counts\n",
    "handsign_names, handsign_video_counts = get_handsign_folders(root_path)\n",
    "num_handsigns = len(handsign_names)\n",
    "\n",
    "# Sort handsigns by the number of available videos in descending order\n",
    "sorted_handsign_video_counts = dict(sorted(handsign_video_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Output the handsign names, number of handsigns, and sorted video counts\n",
    "print(f\"Number of handsigns: {num_handsigns}\")\n",
    "print(\"Handsign names:\")\n",
    "print(json.dumps(handsign_names, indent=4))\n",
    "\n",
    "print(\"\\nVideo counts per handsign (sorted by number of videos):\")\n",
    "print(json.dumps(sorted_handsign_video_counts, indent=4))\n"
   ],
   "id": "823e51359a2141fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of handsigns: 39\n",
      "Handsign names:\n",
      "{\n",
      "    \"0\": \"A\",\n",
      "    \"1\": \"apellido\",\n",
      "    \"2\": \"aprender\",\n",
      "    \"3\": \"argentina\",\n",
      "    \"4\": \"ayuda\",\n",
      "    \"5\": \"B\",\n",
      "    \"6\": \"C\",\n",
      "    \"7\": \"comprar\",\n",
      "    \"8\": \"cumplea\\u00f1os\",\n",
      "    \"9\": \"D\",\n",
      "    \"10\": \"E\",\n",
      "    \"11\": \"F\",\n",
      "    \"12\": \"G\",\n",
      "    \"13\": \"H\",\n",
      "    \"14\": \"hijo\",\n",
      "    \"15\": \"hombre\",\n",
      "    \"16\": \"I\",\n",
      "    \"17\": \"J\",\n",
      "    \"18\": \"K\",\n",
      "    \"19\": \"L\",\n",
      "    \"20\": \"M\",\n",
      "    \"21\": \"mujer\",\n",
      "    \"22\": \"N\",\n",
      "    \"23\": \"nombre\",\n",
      "    \"24\": \"no_sign\",\n",
      "    \"25\": \"O\",\n",
      "    \"26\": \"P\",\n",
      "    \"27\": \"Q\",\n",
      "    \"28\": \"R\",\n",
      "    \"29\": \"S\",\n",
      "    \"30\": \"sordo\",\n",
      "    \"31\": \"T\",\n",
      "    \"32\": \"U\",\n",
      "    \"33\": \"V\",\n",
      "    \"34\": \"W\",\n",
      "    \"35\": \"X\",\n",
      "    \"36\": \"Y\",\n",
      "    \"37\": \"Z\",\n",
      "    \"38\": \"\\u00d1\"\n",
      "}\n",
      "\n",
      "Video counts per handsign (sorted by number of videos):\n",
      "{\n",
      "    \"G\": 278,\n",
      "    \"C\": 266,\n",
      "    \"L\": 258,\n",
      "    \"Y\": 257,\n",
      "    \"I\": 251,\n",
      "    \"U\": 247,\n",
      "    \"O\": 219,\n",
      "    \"J\": 205,\n",
      "    \"A\": 198,\n",
      "    \"V\": 198,\n",
      "    \"H\": 182,\n",
      "    \"D\": 170,\n",
      "    \"K\": 167,\n",
      "    \"W\": 165,\n",
      "    \"S\": 150,\n",
      "    \"B\": 146,\n",
      "    \"cumplea\\u00f1os\": 143,\n",
      "    \"mujer\": 137,\n",
      "    \"apellido\": 136,\n",
      "    \"X\": 133,\n",
      "    \"no_sign\": 125,\n",
      "    \"F\": 113,\n",
      "    \"Z\": 113,\n",
      "    \"M\": 110,\n",
      "    \"ayuda\": 109,\n",
      "    \"N\": 109,\n",
      "    \"E\": 108,\n",
      "    \"Q\": 100,\n",
      "    \"T\": 100,\n",
      "    \"P\": 99,\n",
      "    \"R\": 99,\n",
      "    \"sordo\": 97,\n",
      "    \"\\u00d1\": 94,\n",
      "    \"nombre\": 91,\n",
      "    \"comprar\": 84,\n",
      "    \"aprender\": 83,\n",
      "    \"hijo\": 82,\n",
      "    \"hombre\": 82,\n",
      "    \"argentina\": 81\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T22:49:14.473658Z",
     "start_time": "2024-11-12T22:49:14.019998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate dummy data\n",
    "data = [np.random.rand(videos_per_handsign, frames_per_video, num_landmarks, num_coordinates) for _ in range(num_handsigns)]\n",
    "\n",
    "# Convert the list to a numpy array with shape (num_handsigns, videos_per_handsign, frames_per_video, num_landmarks, num_coordinates)\n",
    "data_array = np.array(data)\n",
    "\n",
    "# Save the data array to a .npy file\n",
    "np.save('handsigns_data.npy', data_array)"
   ],
   "id": "fa59d6b8b575718a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T22:03:57.799183Z",
     "start_time": "2024-11-13T22:03:57.770749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          PROCESS VIDEO DATASET FUNC DEFINITIONS         ##\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_landmarks(hands_results, pose_results):\n",
    "    landmarks = []\n",
    "\n",
    "    # Extract nose landmark or set to default if not available\n",
    "    try:\n",
    "        nose_landmark = pose_results.pose_landmarks.landmark[0]\n",
    "    except AttributeError:\n",
    "        class nose_landmark:\n",
    "            x, y, z = 0, 0, 0\n",
    "        nose_landmark = nose_landmark()\n",
    "\n",
    "    # Default placeholders for hand landmarks\n",
    "    left_hand_landmarks = [(0, 0, 0)] * 21\n",
    "    right_hand_landmarks = [(0, 0, 0)] * 21\n",
    "\n",
    "    # Extract hand landmarks if available\n",
    "    if hands_results.multi_hand_landmarks and hands_results.multi_handedness:\n",
    "        for i, hand_landmarks in enumerate(hands_results.multi_hand_landmarks):\n",
    "            label = hands_results.multi_handedness[i].classification[0].label\n",
    "            # Use the wrist as the reference landmark\n",
    "            wrist_landmark = hand_landmarks.landmark[0]\n",
    "\n",
    "            if label == 'Left':\n",
    "                left_hand_landmarks = [\n",
    "                    (lm.x - wrist_landmark.x, lm.y - wrist_landmark.y, lm.z - wrist_landmark.z)\n",
    "                    for lm in hand_landmarks.landmark\n",
    "                ]\n",
    "            elif label == 'Right':\n",
    "                right_hand_landmarks = [\n",
    "                    (lm.x - wrist_landmark.x, lm.y - wrist_landmark.y, lm.z - wrist_landmark.z)\n",
    "                    for lm in hand_landmarks.landmark\n",
    "                ]\n",
    "\n",
    "    # Add left and right hand landmarks to the landmark list\n",
    "    landmarks.extend(left_hand_landmarks)\n",
    "    landmarks.extend(right_hand_landmarks)\n",
    "\n",
    "    # Add pose landmarks if available, excluding those from the waist down\n",
    "    if pose_results.pose_landmarks:\n",
    "        for i, lm in enumerate(pose_results.pose_landmarks.landmark):\n",
    "            # Exclude landmarks from waist down (indices 23 to 32)\n",
    "            if i < 23:\n",
    "                landmarks.append((lm.x - nose_landmark.x, lm.y - nose_landmark.y, lm.z - nose_landmark.z))\n",
    "    else:\n",
    "        # Use placeholders for upper body landmarks only\n",
    "        landmarks.extend([(0, 0, 0)] * 23)\n",
    "\n",
    "    return np.array(landmarks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_frame(frame, hands, pose):\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    hands_results = hands.process(image)\n",
    "    pose_results = pose.process(image)\n",
    "\n",
    "    landmarks = extract_landmarks(hands_results, pose_results)\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def process_video_with_samples(video_path, frames_per_video, num_landmarks, num_coordinates, num_additional_samples=0):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if the video was opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video {video_path}.\")\n",
    "        return np.zeros((1 + num_additional_samples, frames_per_video, num_landmarks, num_coordinates))\n",
    "    \n",
    "    needs_interpolation = False\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if total_frames < frames_per_video/4 or total_frames < 3:\n",
    "        print(f\"Error: frames on {video_path} are less than 25% of frames_per_video or less than 3. filling with zeros.\")\n",
    "        return np.zeros((1 + num_additional_samples, frames_per_video, num_landmarks, num_coordinates))\n",
    "    elif total_frames < frames_per_video * (1 + num_additional_samples):\n",
    "        needs_interpolation = True\n",
    "        print(f\"Warning: Video {video_path} has fewer frames ({total_frames}) than required ({frames_per_video} * {num_additional_samples+1}). Interpolating frames.\")\n",
    "        \n",
    "    # Buffer frames\n",
    "    frame_buffer = {}\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_buffer[frame_count] = frame\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "\n",
    "    # Apply interpolation if needed\n",
    "    if needs_interpolation:\n",
    "        interpolated_frames = {}\n",
    "        for i in range(total_frames - 1):\n",
    "            interpolated_frames[i] = frame_buffer[i]\n",
    "            # Create additional interpolated frames between i and i+1\n",
    "            num_interp_frames = max(1, frames_per_video // total_frames)\n",
    "            for alpha in np.linspace(0, 1, num_interp_frames + 2)[1:-1]:\n",
    "                interp_frame = cv2.addWeighted(frame_buffer[i], 1 - alpha, frame_buffer[i + 1], alpha, 0)\n",
    "                interpolated_frames[len(interpolated_frames)] = interp_frame\n",
    "        frame_buffer = interpolated_frames\n",
    "\n",
    "    # Frame selection and sample generation remain the same\n",
    "    frames_data = []\n",
    "    frame_sets = [np.linspace(0, len(frame_buffer) - 1, frames_per_video, dtype=int)]\n",
    "\n",
    "    for i in range(1, num_additional_samples + 1):\n",
    "        available_frames = list(set(range(len(frame_buffer))) - set(frame_sets[0]))\n",
    "        if len(available_frames) >= frames_per_video:\n",
    "            additional_indices = [available_frames[idx] for idx in np.linspace(0, len(available_frames) - 1, frames_per_video, dtype=int)]\n",
    "        else:\n",
    "            additional_indices = np.linspace(0, len(frame_buffer) - 1, frames_per_video, dtype=int)\n",
    "        frame_sets.append(additional_indices)\n",
    "\n",
    "    # Process the buffered frames for landmarks\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    with mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5) as hands, \\\n",
    "         mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        \n",
    "        for frame_set in frame_sets:\n",
    "            for frame_idx in frame_set:\n",
    "                if frame_idx in frame_buffer:\n",
    "                    landmarks = process_frame(frame_buffer[frame_idx], hands, pose)\n",
    "                    frames_data.append(landmarks)\n",
    "\n",
    "    # Final reshaping and padding as needed\n",
    "    reshaped_data = np.array(frames_data).reshape((1 + num_additional_samples, frames_per_video, num_landmarks, num_coordinates))\n",
    "    \n",
    "    return reshaped_data\n",
    "\n",
    "def process_dataset_with_samples(root_path, handsign_names, frames_per_video, num_landmarks, num_coordinates, videos_per_handsign, num_additional_samples=0):\n",
    "    data = []\n",
    "\n",
    "    # Calculate how many videos to process based on videos_per_handsign and num_additional_samples (assuming videos_per_handsign was already augmented by num_additional_samples)\n",
    "    videos_to_process = videos_per_handsign // (1 + num_additional_samples)\n",
    "    \n",
    "    for handsign_index in range(len(handsign_names)):\n",
    "        handsign_folder = handsign_names[handsign_index]\n",
    "        handsign_path = os.path.join(root_path, handsign_folder)\n",
    "        \n",
    "        if not os.path.exists(handsign_path):\n",
    "            print(f\"Warning: Directory {handsign_path} does not exist. Skipping.\")\n",
    "            data.append(np.zeros((videos_per_handsign, frames_per_video, num_landmarks, num_coordinates)))\n",
    "            continue\n",
    "\n",
    "        videos = [f for f in os.listdir(handsign_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        handsign_data = []\n",
    "        \n",
    "        # Repeat videos if there aren't enough to reach videos_per_handsign\n",
    "        if len(videos) < videos_to_process:\n",
    "            print(f\"Warning: Handsign '{handsign_folder}' has only {len(videos)} videos. Repeating videos until {videos_to_process} are processed.\")\n",
    "            repeated_videos = (videos * ((videos_to_process // len(videos)) + 1))[:videos_to_process]\n",
    "        else:\n",
    "            repeated_videos = videos[:videos_to_process]\n",
    "\n",
    "        # Process each required video, with repeats if necessary\n",
    "        for video in tqdm(repeated_videos, desc=f\"Processing videos for handsign {handsign_index}\", leave=False):\n",
    "            video_path = os.path.join(handsign_path, video)\n",
    "            \n",
    "            # Call the updated process_video_with_samples function\n",
    "            video_data = process_video_with_samples(video_path, frames_per_video, num_landmarks, num_coordinates, num_additional_samples)\n",
    "\n",
    "            # Append all generated samples (original + additional) from the video\n",
    "            for sample in video_data:\n",
    "                handsign_data.append(sample)\n",
    "\n",
    "        handsign_data = np.array(handsign_data)\n",
    "        total_video_samples = handsign_data.shape[0]  # Total samples generated from the videos processed\n",
    "\n",
    "        # Ensure total number of video samples matches the predefined videos_per_handsign\n",
    "        if total_video_samples < videos_per_handsign:\n",
    "            padding_needed = videos_per_handsign - total_video_samples\n",
    "            print(f\"Warning: Padding with {padding_needed} empty samples for handsign '{handsign_folder}'.\")\n",
    "            handsign_data = np.pad(handsign_data, ((0, padding_needed), (0, 0), (0, 0), (0, 0)), mode='constant')\n",
    "        elif total_video_samples > videos_per_handsign:\n",
    "            handsign_data = handsign_data[:videos_per_handsign]\n",
    "            print(f\"Warning: Trimming excess samples for handsign '{handsign_folder}'.\")\n",
    "\n",
    "        data.append(handsign_data)\n",
    "\n",
    "    final_data = np.array(data)\n",
    "    print(f\"Final dataset shape: {final_data.shape}\")\n",
    "\n",
    "    return final_data\n",
    "\n",
    "    "
   ],
   "id": "c6b1d3592b943838",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T00:39:26.333960Z",
     "start_time": "2024-11-12T22:49:23.242304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          PROCESS VIDEOS DATASET FUNC CALLING         ##\n",
    "data_array = process_dataset_with_samples(root_path, handsign_names, frames_per_video, num_landmarks, num_coordinates, videos_per_handsign, num_additional_samples)\n",
    "\n",
    "# Save the data array to a .npy file\n",
    "np.save('handsigns_data.npy', data_array)\n",
    "print(\"Data saved to handsigns_data.npy\")\n",
    "    "
   ],
   "id": "38b563f5e5342b66",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 0:   9%|▊         | 13/150 [00:24<03:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\A\\split_11.mp4 has fewer frames (15) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 0:  21%|██▏       | 32/150 [00:46<02:17,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\A\\split_16.mp4 has fewer frames (15) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 0:  74%|███████▍  | 111/150 [02:13<00:44,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\A\\split_35 (3).mp4 has fewer frames (6) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 0:  93%|█████████▎| 139/150 [02:44<00:12,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: frames on NomenclatedDataset - Copy\\A\\split_43.mp4 are less than 25% of frames_per_video or less than 3. filling with zeros.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'apellido' has only 136 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'aprender' has only 83 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'argentina' has only 81 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'ayuda' has only 109 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'B' has only 146 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 6:  13%|█▎        | 19/150 [00:21<02:34,  1.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\C\\split_13.mp4 has fewer frames (14) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 6:  61%|██████    | 91/150 [01:41<01:06,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\C\\split_3.mp4 has fewer frames (14) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 6:  67%|██████▋   | 100/150 [01:50<00:43,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to open video NomenclatedDataset - Copy\\C\\split_31.mp4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 6:  75%|███████▍  | 112/150 [02:04<00:45,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\C\\split_36 (2).mp4 has fewer frames (11) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 6:  91%|█████████ | 136/150 [02:31<00:15,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: frames on NomenclatedDataset - Copy\\C\\split_45.mp4 are less than 25% of frames_per_video or less than 3. filling with zeros.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'comprar' has only 84 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'cumpleaños' has only 107 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 8:  33%|███▎      | 50/150 [01:08<02:22,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\cumpleaños\\split_1.mp4 has fewer frames (14) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 8:  45%|████▌     | 68/150 [01:24<01:16,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\cumpleaños\\split_26.mp4 has fewer frames (12) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 8:  48%|████▊     | 72/150 [01:28<01:11,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\cumpleaños\\split_3.mp4 has fewer frames (12) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 8:  55%|█████▍    | 82/150 [01:36<01:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\cumpleaños\\split_9.mp4 has fewer frames (14) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 8:  60%|██████    | 90/150 [01:45<01:04,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\cumpleaños\\WIN_20241004_17_24_35_Pro.mp4 has fewer frames (10) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 8:  61%|██████▏   | 92/150 [01:47<01:01,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\cumpleaños\\WIN_20241004_17_24_40_Pro.mp4 has fewer frames (15) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 9:  47%|████▋     | 70/150 [01:14<01:30,  1.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\D\\split_23 (2).mp4 has fewer frames (10) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'E' has only 108 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 10:  67%|██████▋   | 101/150 [01:51<00:50,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\E\\split_69.mp4 has fewer frames (6) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'F' has only 113 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 11:  53%|█████▎    | 79/150 [01:28<01:16,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\F\\split_45.mp4 has fewer frames (6) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 13:  87%|████████▋ | 131/150 [02:19<00:20,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\H\\split_42.mp4 has fewer frames (5) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'hijo' has only 82 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'hombre' has only 82 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 16:  19%|█▉        | 29/150 [00:29<02:08,  1.06s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\I\\split_14.mp4 has fewer frames (13) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'M' has only 110 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'mujer' has only 137 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'N' has only 109 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'nombre' has only 91 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'no_sign' has only 125 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 24:  19%|█▉        | 29/150 [00:28<02:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\no_sign\\split_125.mp4 has fewer frames (13) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 25:  85%|████████▍ | 127/150 [02:15<00:23,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\O\\split_26 (4).mp4 has fewer frames (12) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 25:  85%|████████▌ | 128/150 [02:16<00:22,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\O\\split_26 (5).mp4 has fewer frames (13) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'P' has only 99 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 26:  65%|██████▌   | 98/150 [01:46<00:55,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: frames on NomenclatedDataset - Copy\\P\\split_99.mp4 are less than 25% of frames_per_video or less than 3. filling with zeros.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'Q' has only 100 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 27:   1%|▏         | 2/150 [00:02<02:31,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\Q\\split_100.mp4 has fewer frames (4) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 27:  68%|██████▊   | 102/150 [01:41<00:46,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\Q\\split_100.mp4 has fewer frames (4) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'R' has only 99 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 29:  63%|██████▎   | 94/150 [01:38<00:56,  1.02s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\S\\split_52.mp4 has fewer frames (10) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'sordo' has only 50 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'T' has only 100 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 31:   1%|▏         | 2/150 [00:01<02:28,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\T\\split_100.mp4 has fewer frames (11) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 31:  68%|██████▊   | 102/150 [01:42<00:48,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\T\\split_100.mp4 has fewer frames (11) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 32:   1%|▏         | 2/150 [00:02<02:32,  1.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\U\\split_1.mp4 has fewer frames (12) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 32:   3%|▎         | 5/150 [00:04<02:25,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\U\\split_10.mp4 has fewer frames (14) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 32:   7%|▋         | 11/150 [00:11<02:22,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\U\\split_12.mp4 has fewer frames (12) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 32:  19%|█▉        | 29/150 [00:29<02:06,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\U\\split_18.mp4 has fewer frames (12) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 32:  23%|██▎       | 35/150 [00:35<01:58,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\U\\split_2.mp4 has fewer frames (14) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 32:  39%|███▉      | 59/150 [00:59<01:37,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\U\\split_27.mp4 has fewer frames (13) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 32:  45%|████▌     | 68/150 [01:08<01:25,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\U\\split_3.mp4 has fewer frames (15) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 32:  51%|█████▏    | 77/150 [01:18<01:18,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\U\\split_32.mp4 has fewer frames (3) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 33:   7%|▋         | 11/150 [00:10<02:10,  1.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\V\\split_12.mp4 has fewer frames (13) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 33:  25%|██▌       | 38/150 [00:35<01:49,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\V\\split_20.mp4 has fewer frames (13) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 33:  37%|███▋      | 56/150 [00:52<01:29,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\V\\split_26.mp4 has fewer frames (6) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 33:  54%|█████▍    | 81/150 [01:15<01:07,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\V\\split_37.mp4 has fewer frames (15) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 34:  43%|████▎     | 65/150 [01:10<01:31,  1.08s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\W\\split_29.mp4 has fewer frames (13) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 34:  56%|█████▌    | 84/150 [01:30<01:10,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\W\\split_37.mp4 has fewer frames (12) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'X' has only 133 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 35:  64%|██████▍   | 96/150 [01:41<00:56,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\X\\split_97.mp4 has fewer frames (13) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 36:   5%|▌         | 8/150 [00:08<02:21,  1.01it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: frames on NomenclatedDataset - Copy\\Y\\split_100.mp4 are less than 25% of frames_per_video or less than 3. filling with zeros.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 36:  16%|█▌        | 24/150 [00:23<02:12,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\Y\\split_14.mp4 has fewer frames (12) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 36:  24%|██▍       | 36/150 [00:36<01:57,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\Y\\split_17.mp4 has fewer frames (15) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 36:  43%|████▎     | 64/150 [01:04<01:29,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\Y\\split_23.mp4 has fewer frames (14) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 36:  45%|████▌     | 68/150 [01:09<01:24,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\Y\\split_24.mp4 has fewer frames (12) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 36:  59%|█████▊    | 88/150 [01:29<01:07,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\Y\\split_29.mp4 has fewer frames (12) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 36:  85%|████████▌ | 128/150 [02:11<00:23,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\Y\\split_4.mp4 has fewer frames (13) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'Z' has only 113 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Handsign 'Ñ' has only 94 videos. Repeating videos until 150 are processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for handsign 38:  43%|████▎     | 65/150 [01:08<01:28,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Video NomenclatedDataset - Copy\\Ñ\\split_24.mp4 has fewer frames (9) than required (8 * 2). Interpolating frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (39, 300, 8, 65, 3)\n",
      "Data saved to handsigns_data.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T03:14:00.980665Z",
     "start_time": "2024-11-13T03:13:58.055270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##        DATA AUGMENTATION (OPTIONAL)        ##\n",
    "\n",
    "def apply_rotation(landmarks, angle_degrees_x, angle_degrees_y, angle_degrees_z):\n",
    "    \"\"\"Rotate the landmarks in 3D space by given angles around X, Y, and Z axes.\"\"\"\n",
    "    angle_radians_x = np.radians(angle_degrees_x)\n",
    "    angle_radians_y = np.radians(angle_degrees_y)\n",
    "    angle_radians_z = np.radians(angle_degrees_z)\n",
    "\n",
    "    # Rotation matrix around the X-axis\n",
    "    rotation_matrix_x = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(angle_radians_x), -np.sin(angle_radians_x)],\n",
    "        [0, np.sin(angle_radians_x), np.cos(angle_radians_x)]\n",
    "    ])\n",
    "\n",
    "    # Rotation matrix around the Y-axis\n",
    "    rotation_matrix_y = np.array([\n",
    "        [np.cos(angle_radians_y), 0, np.sin(angle_radians_y)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(angle_radians_y), 0, np.cos(angle_radians_y)]\n",
    "    ])\n",
    "\n",
    "    # Rotation matrix around the Z-axis\n",
    "    rotation_matrix_z = np.array([\n",
    "        [np.cos(angle_radians_z), -np.sin(angle_radians_z), 0],\n",
    "        [np.sin(angle_radians_z), np.cos(angle_radians_z), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    # Combine rotations by multiplying the matrices (Z * Y * X)\n",
    "    combined_rotation_matrix = np.dot(np.dot(rotation_matrix_z, rotation_matrix_y), rotation_matrix_x)\n",
    "\n",
    "    return np.dot(landmarks, combined_rotation_matrix)\n",
    "\n",
    "def apply_scaling(landmarks, scale_factor):\n",
    "    \"\"\"Scale the landmarks by a given factor.\"\"\"\n",
    "    return landmarks * scale_factor\n",
    "\n",
    "def apply_translation(landmarks, translation_vector):\n",
    "    \"\"\"Translate the landmarks by a given vector (x, y, z).\"\"\"\n",
    "    return landmarks + translation_vector\n",
    "\n",
    "def add_noise(landmarks, noise_level=0.001):\n",
    "    \"\"\"Add random noise to the landmarks.\"\"\"\n",
    "    noise = np.random.normal(0, noise_level, landmarks.shape)\n",
    "    return landmarks + noise\n",
    "\n",
    "def augment_data(data_array, num_augmented_versions=5):\n",
    "    \"\"\"\n",
    "    Augment the data array by applying transformations.\n",
    "    Creates `num_augmented_versions` augmented copies of each handsign video.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Augmentation parameters\n",
    "    noise_level = 0.005  # for the add_noise() function\n",
    "    translation_vector = np.random.uniform(-0.05, 0.05, 3)  # for the apply_translation() function\n",
    "    scale_factor = np.random.uniform(0.6, 1.6)  # for the apply_scaling() function\n",
    "    angle_degrees_x = np.random.uniform(-20, 20)  # for rotation around X-axis\n",
    "    angle_degrees_y = np.random.uniform(-20, 20)  # for rotation around Y-axis\n",
    "    angle_degrees_z = np.random.uniform(-20, 20)  # for rotation around Z-axis\n",
    "\n",
    "    augmented_data = []\n",
    "    for handsign_data in data_array:\n",
    "        augmented_handsign_data = []\n",
    "        for video_data in handsign_data:\n",
    "            augmented_videos = [video_data]  # Start with the original video data\n",
    "\n",
    "            for _ in range(num_augmented_versions):\n",
    "                augmented_video = []\n",
    "                for frame in video_data:\n",
    "                    # Apply a combination of augmentations\n",
    "                    rotated_frame = apply_rotation(frame, angle_degrees_x, angle_degrees_y, angle_degrees_z)\n",
    "                    scaled_frame = apply_scaling(rotated_frame, scale_factor)\n",
    "                    translated_frame = apply_translation(scaled_frame, translation_vector)\n",
    "                    noisy_frame = add_noise(translated_frame, noise_level)\n",
    "\n",
    "                    augmented_video.append(noisy_frame)\n",
    "                \n",
    "                augmented_videos.append(np.array(augmented_video))\n",
    "\n",
    "            # Flatten the augmented videos for each original video\n",
    "            augmented_handsign_data.extend(augmented_videos)\n",
    "\n",
    "        augmented_data.append(np.array(augmented_handsign_data))\n",
    "    \n",
    "    return np.array(augmented_data)\n",
    "\n",
    "# Load original handsigns data\n",
    "handsigns_data = np.load('handsigns_data.npy')\n",
    "\n",
    "# Apply augmentation\n",
    "if data_augmentation:\n",
    "    augmented_data = augment_data(handsigns_data, num_augmented_versions)\n",
    "    # Save the augmented data to a new .npy file\n",
    "    np.save('handsigns_data_augmented.npy', augmented_data)\n",
    "    # Update the videos per handsign value to match the videos generated by the augmentation\n",
    "    data_array = np.load('handsigns_data_augmented.npy')\n",
    "    videos_per_handsign = data_array.shape[1]\n",
    "    \n",
    "    print(\"Augmented data saved to handsigns_data_augmented.npy, videos_per_handsign augmented by \"+str(num_augmented_versions)+\" per existing video for a total of \"+str(videos_per_handsign)+\" videos per handsign\")\n",
    "else:\n",
    "    print(\"no data augmentation was performed, check the flag on the first cell\")"
   ],
   "id": "d0c17cfc27c8ccac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented data saved to handsigns_data_augmented.npy, videos_per_handsign augmented by 1 per existing video for a total of 600 videos per handsign\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T03:14:05.802956Z",
     "start_time": "2024-11-13T03:14:05.685309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          MODEL DEFINITION            ##\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Reshape, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Reshape input to (frames_per_video, num_landmarks * num_coordinates)\n",
    "new_input_shape = (frames_per_video, num_landmarks * num_coordinates)\n",
    "\n",
    "model = Sequential([\n",
    "    # Reshape layer\n",
    "    Reshape((frames_per_video, num_landmarks * num_coordinates), input_shape=(frames_per_video, num_landmarks, num_coordinates)),\n",
    "    \n",
    "    # LSTM layers with Dropout and Batch Normalization to reduce overfitting\n",
    "    Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001))),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    # LSTM layers with Dropout and Batch Normalization to reduce overfitting\n",
    "    Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001))),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Bidirectional(LSTM(128, return_sequences=False, kernel_regularizer=l2(0.001))),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Output layer for multi-class classification\n",
    "    Dense(num_handsigns, activation='softmax')  # Softmax for multi-class classification\n",
    "])\n",
    "\n",
    "# Specify a learning rate\n",
    "learning_rate = 0.005\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#model.summary() # Uncomment if you want to see the model summary\n"
   ],
   "id": "c1f9d72562ce0728",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-13T03:37:47.847400Z",
     "start_time": "2024-11-13T03:37:47.436882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##        DATA PREPROCESSING FOR TRAINING            ##\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "# Load the data from the .npy file, making a copy to use for training as to not modify the original extracted data\n",
    "if data_augmentation:\n",
    "    shutil.copy('handsigns_data_augmented.npy', 'handsigns_data_training_copy.npy')\n",
    "    data_array = np.load('handsigns_data_training_copy.npy')\n",
    "    \n",
    "    # Update videos_per_handsign based on augmentation\n",
    "    #videos_per_handsign = data_array.shape[1]  # Dynamically update based on the new augmented shape\n",
    "    print(\"using handsigns_data_augmented.npy. After augmentation videos per handsign updated to: \" + str(data_array.shape[1]))\n",
    "else:\n",
    "    shutil.copy('handsigns_data.npy', 'handsigns_data_training_copy.npy')\n",
    "    data_array = np.load('handsigns_data_training_copy.npy')\n",
    "\n",
    "print('Array used shape: ',data_array.shape)\n",
    "# X remains unchanged\n",
    "X = data_array \n",
    "\n",
    "# Create labels for each handsign (0 to num_handsigns-1)\n",
    "# This creates a label for each hand sign, repeated for each video\n",
    "y = np.repeat(np.arange(num_handsigns), videos_per_handsign)\n",
    "y = y.reshape(num_handsigns, videos_per_handsign)\n",
    "\n",
    "# Initialize lists to hold training and validation data\n",
    "X_train_list = []\n",
    "X_val_list = []\n",
    "y_train_list = []\n",
    "y_val_list = []\n",
    "\n",
    "# Split videos and labels for each handsign\n",
    "for handsign_index in range(num_handsigns):\n",
    "    # Split the videos within each handsign\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(videos_per_handsign), \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Select training and validation data for this handsign\n",
    "    X_train_list.append(data_array[handsign_index, train_indices])\n",
    "    X_val_list.append(data_array[handsign_index, val_indices])\n",
    "    \n",
    "    # Select corresponding labels\n",
    "    y_train_list.append(y[handsign_index, train_indices])\n",
    "    y_val_list.append(y[handsign_index, val_indices])\n",
    "\n",
    "# Concatenate lists to form the final training and validation sets\n",
    "X_train = np.concatenate(X_train_list, axis=0)\n",
    "X_val = np.concatenate(X_val_list, axis=0)\n",
    "y_train = np.concatenate(y_train_list, axis=0)\n",
    "y_val = np.concatenate(y_val_list, axis=0)\n",
    "\n",
    "# Reshape X_train and X_val to fit the model's expected input shape\n",
    "X_train = X_train.reshape(-1, frames_per_video, num_landmarks, num_coordinates)\n",
    "X_val = X_val.reshape(-1, frames_per_video, num_landmarks, num_coordinates)\n",
    "\n",
    "# Flatten y_train and y_val\n",
    "y_train = y_train.flatten()\n",
    "y_val = y_val.flatten()"
   ],
   "id": "664f881d08e116f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using handsigns_data_augmented.npy. After augmentation videos per handsign updated to: 600\n",
      "Array used shape:  (39, 600, 8, 65, 3)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T03:37:24.066074Z",
     "start_time": "2024-11-13T03:37:24.037064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          MODEL TRAINING          ##\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Enable ITEX optimization\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Callback helpers for model training #\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "# Model checkpointing to save the best model during training\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)  \n",
    "checkpoint = ModelCheckpoint('best_handsigns_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)  \n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val), \n",
    "    callbacks=[early_stopping, checkpoint, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model.save('handsigns_model.h5')\n",
    "\n",
    "# Optionally, save the training history\n",
    "import pickle\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "    \n",
    "\n",
    "    \n",
    "##          TRAINING HISTORY ANALYSIS           ##\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# If you want to save the plot instead of displaying it:\n",
    "# plt.savefig('training_history.png')"
   ],
   "id": "febb1178fa7e41a7",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 18\u001B[0m\n\u001B[0;32m     14\u001B[0m reduce_lr \u001B[38;5;241m=\u001B[39m ReduceLROnPlateau(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, min_lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-6\u001B[39m)  \n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m     17\u001B[0m history \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfit(\n\u001B[1;32m---> 18\u001B[0m     \u001B[43mX_train\u001B[49m, y_train, \n\u001B[0;32m     19\u001B[0m     epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m,\n\u001B[0;32m     20\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m,\n\u001B[0;32m     21\u001B[0m     validation_data\u001B[38;5;241m=\u001B[39m(X_val, y_val), \n\u001B[0;32m     22\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39m[early_stopping, checkpoint, reduce_lr],\n\u001B[0;32m     23\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     24\u001B[0m )\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Save the trained model\u001B[39;00m\n\u001B[0;32m     28\u001B[0m model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhandsigns_model.h5\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T22:54:20.808570Z",
     "start_time": "2024-11-13T22:51:02.387232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          CONTINUOUS PREDICTION WITH SLIDING WINDOW           ##\n",
    "\n",
    "import collections\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from scipy.interpolate import interp1d  # For smoothing landmarks\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('best_handsigns_model.keras')\n",
    "\n",
    "# Initialize Mediapipe solutions outside the loop for efficiency\n",
    "mp_hands = mp.solutions.hands.Hands(static_image_mode=False, \n",
    "                                    max_num_hands=2,\n",
    "                                    min_detection_confidence=0.4,  # Lowered confidence to allow for fast movement detection\n",
    "                                    min_tracking_confidence=0.4)   # Lowered tracking confidence\n",
    "mp_pose = mp.solutions.pose.Pose(static_image_mode=False,\n",
    "                                 model_complexity=0,\n",
    "                                 min_detection_confidence=0.4, \n",
    "                                 min_tracking_confidence=0.4)\n",
    "\n",
    "# Open webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Sliding window buffer for frames\n",
    "frame_buffer = collections.deque(maxlen=frames_per_video)\n",
    "\n",
    "# To smooth predictions, keep track of recent predictions\n",
    "prediction_buffer = collections.deque(maxlen=6)\n",
    "\n",
    "# Store the last prediction\n",
    "last_prediction = \"No Prediction\"\n",
    "last_confidence = 0.0\n",
    "\n",
    "# Track missing hands to reset landmarks if missing for too long\n",
    "hand_missing_threshold = 5\n",
    "left_hand_missing_count = 0\n",
    "right_hand_missing_count = 0\n",
    "\n",
    "# Store the last known hand landmarks to compare in future frames\n",
    "last_left_hand_landmarks = None\n",
    "last_right_hand_landmarks = None\n",
    "\n",
    "landmark_drawing = True\n",
    "\n",
    "# Smoothing factor for missing landmarks\n",
    "smoothing_factor = 0.8  # Weight to smooth landmarks during quick movements\n",
    "\n",
    "# Draw landmarks on the frame\n",
    "def draw_landmarks(frame, landmarks):\n",
    "    \"\"\"Draw landmarks on the frame.\"\"\"\n",
    "    for i, (x, y, z) in enumerate(landmarks):\n",
    "        h, w, _ = frame.shape\n",
    "        x = int(x * w + 325)\n",
    "        y = int(y * h + 250)\n",
    "        \n",
    "        if i < 21:  # Left hand landmarks\n",
    "            color = (0, 255, 0)\n",
    "        elif i < 42:  # Right hand landmarks\n",
    "            color = (0, 0, 255)\n",
    "        else:  # Body landmarks\n",
    "            color = (255, 0, 0)\n",
    "        \n",
    "        cv2.circle(frame, (x, y), 5, color, -1)\n",
    "\n",
    "def smooth_landmarks(new_landmarks, old_landmarks):\n",
    "    \"\"\"Smooth landmarks by interpolating between old and new.\"\"\"\n",
    "    if old_landmarks is None:\n",
    "        return new_landmarks\n",
    "\n",
    "    return old_landmarks * (1 - smoothing_factor) + new_landmarks * smoothing_factor\n",
    "\n",
    "def internal_process_frame(frame, mp_hands, mp_pose):\n",
    "    global last_left_hand_landmarks, last_right_hand_landmarks\n",
    "    global left_hand_missing_count, right_hand_missing_count\n",
    "\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    hands_results = mp_hands.process(image)\n",
    "    pose_results = mp_pose.process(image)\n",
    "\n",
    "    # Initialize a zero-filled array for landmarks (51 landmarks, each with x, y, z)\n",
    "    landmarks = np.zeros((num_landmarks, num_coordinates))\n",
    "\n",
    "    # Detect nose for relative normalization\n",
    "    try:\n",
    "        nose_landmark = pose_results.pose_landmarks.landmark[0]\n",
    "    except:\n",
    "        class nose_landmark:\n",
    "            x = 0\n",
    "            y = 0\n",
    "            z = 0\n",
    "        nose_landmark = nose_landmark()\n",
    "\n",
    "    if hands_results.multi_hand_landmarks:\n",
    "        handedness_labels = [hand.classification[0].label for hand in hands_results.multi_handedness]\n",
    "\n",
    "        for i, hand_landmarks in enumerate(hands_results.multi_hand_landmarks):\n",
    "            if handedness_labels[i] == 'Left':\n",
    "                left_hand = np.array([(lm.x - nose_landmark.x, lm.y - nose_landmark.y, lm.z - nose_landmark.z) \n",
    "                                      for lm in hand_landmarks.landmark])\n",
    "\n",
    "                # Smooth the transition between frames\n",
    "                left_hand = smooth_landmarks(left_hand, last_left_hand_landmarks)\n",
    "                \n",
    "                # Update the stored landmarks for the next frame\n",
    "                last_left_hand_landmarks = left_hand\n",
    "                landmarks[:21] = left_hand  # Insert the left hand landmarks into the first 21 slots\n",
    "\n",
    "            elif handedness_labels[i] == 'Right':\n",
    "                right_hand = np.array([(lm.x - nose_landmark.x, lm.y - nose_landmark.y, lm.z - nose_landmark.z) \n",
    "                                       for lm in hand_landmarks.landmark])\n",
    "\n",
    "                # Smooth the transition between frames\n",
    "                right_hand = smooth_landmarks(right_hand, last_right_hand_landmarks)\n",
    "                \n",
    "                # Update the stored landmarks for the next frame\n",
    "                last_right_hand_landmarks = right_hand\n",
    "                landmarks[21:42] = right_hand  # Insert the right hand landmarks into slots 21-42\n",
    "\n",
    "        # Reset the missing counts if hands are detected\n",
    "        left_hand_missing_count = 0\n",
    "        right_hand_missing_count = 0\n",
    "    else:\n",
    "        # Increment missing count when hands are not detected\n",
    "        left_hand_missing_count += 1\n",
    "        right_hand_missing_count += 1\n",
    "\n",
    "        # Reuse last known landmarks if available and hands are missing for too long\n",
    "        if last_left_hand_landmarks is not None:\n",
    "            landmarks[:21] = last_left_hand_landmarks\n",
    "        if last_right_hand_landmarks is not None:\n",
    "            landmarks[21:42] = last_right_hand_landmarks\n",
    "\n",
    "        # Reset landmarks if hands are missing for too long\n",
    "        if left_hand_missing_count > hand_missing_threshold:\n",
    "            last_left_hand_landmarks = None\n",
    "        if right_hand_missing_count > hand_missing_threshold:\n",
    "            last_right_hand_landmarks = None\n",
    "\n",
    "    # Fill in body landmarks (9 selected)\n",
    "    selected_body_landmarks = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "    if pose_results.pose_landmarks:\n",
    "        for idx, landmark_idx in enumerate(selected_body_landmarks):\n",
    "            lm = pose_results.pose_landmarks.landmark[landmark_idx]\n",
    "            landmarks[42 + idx] = (lm.x - nose_landmark.x, lm.y - nose_landmark.y, lm.z - nose_landmark.z)\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "# Make a prediction based on the buffer\n",
    "def predict_handsign(buffer):\n",
    "    \"\"\"Make a prediction based on a buffer of frames.\"\"\"    \n",
    "    video_data = np.array(buffer)\n",
    "    video_data = video_data.reshape(1, frames_per_video, num_landmarks, num_coordinates)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(video_data, verbose=0)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    confidence = prediction[0][predicted_class]\n",
    "\n",
    "    return predicted_class, confidence\n",
    "scalar = 1\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.resize(frame, (int(640*scalar), int(480*scalar)))\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Process every frame (no skipping)\n",
    "    landmarks = process_frame(frame, mp_hands, mp_pose)\n",
    "\n",
    "    # Add the landmarks to the frame buffer\n",
    "    frame_buffer.append(landmarks)\n",
    "\n",
    "    # Draw the landmarks on the frame\n",
    "    if landmark_drawing:\n",
    "        draw_landmarks(frame, landmarks)\n",
    "\n",
    "    # Once the buffer is full, make a prediction using the sliding window\n",
    "    if len(frame_buffer) == frames_per_video:\n",
    "        predicted_class, confidence = predict_handsign(frame_buffer)\n",
    "        predicted_handsign = handsign_names.get(predicted_class, f\"HandSign_{predicted_class}\")\n",
    "\n",
    "        # Update the last prediction\n",
    "        last_prediction = predicted_handsign\n",
    "        last_confidence = confidence\n",
    "\n",
    "        # Store prediction and confidence in the buffer for smoothing\n",
    "        #print((predicted_class, confidence))\n",
    "        prediction_buffer.append((predicted_class, confidence))\n",
    "\n",
    "        # Average the last N predictions to smooth the output\n",
    "        avg_pred_class = np.argmax(np.bincount([p[0] for p in prediction_buffer]))\n",
    "        avg_confidence = np.mean([p[1] for p in prediction_buffer if p[0] == avg_pred_class])\n",
    "\n",
    "        if avg_confidence > 0.8:\n",
    "            last_prediction = handsign_names.get(avg_pred_class, f\"HandSign_{avg_pred_class}\")\n",
    "            last_confidence = avg_confidence\n",
    "        elif 0.45 < avg_confidence < 0.8:\n",
    "            last_prediction = \"deteccion insegura: \"+handsign_names.get(avg_pred_class, f\"HandSign_{avg_pred_class}\")\n",
    "            last_confidence = avg_confidence\n",
    "        else:\n",
    "            last_prediction = \"deteccion nula\"\n",
    "            last_confidence = avg_confidence\n",
    "            \n",
    "    # Display the last prediction on the frame\n",
    "    cv2.putText(frame, f\"Predicted: {last_prediction} Conf: {last_confidence:.2f}\", \n",
    "                (10, 30), cv2.FONT_ITALIC, 0.7, (0, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow('Hands Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "ee106e95cdc75678",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T23:27:01.565289Z",
     "start_time": "2024-11-13T23:26:43.092789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##         V2 CONTINUOUS PREDICTION WITH SLIDING WINDOW           ##\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Parameters for video feed and predictions\n",
    "show_landmarks = False  # Toggle to show hand landmarks\n",
    "show_connections = True  # Toggle to show landmark connections\n",
    "show_prediction_text = True  # Toggle to show prediction and accuracy on screen\n",
    "accuracy_threshold = 0.80  # Threshold for highlighting low accuracy \n",
    "sliding_window_size = frames_per_video  # Size of the sliding window for prediction\n",
    "smoothing_buffer_size = 4  # Buffer size for prediction smoothing (larger = smoother)\n",
    "mediapipe_confidence = 0.2  # Confidence threshold for mediapipe detection\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('best_handsigns_model.keras')\n",
    "\n",
    "# Initialize MediaPipe hands and pose solutions\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "hands = mp_hands.Hands(min_detection_confidence=mediapipe_confidence, min_tracking_confidence=mediapipe_confidence)\n",
    "pose = mp_pose.Pose(min_detection_confidence=mediapipe_confidence, min_tracking_confidence=mediapipe_confidence)\n",
    "\n",
    "# Set up sliding window and smoothing buffer\n",
    "sliding_window = deque(maxlen=sliding_window_size)\n",
    "smoothing_buffer = deque(maxlen=smoothing_buffer_size)\n",
    "\n",
    "# Initialize video capture (webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Function to smooth predictions\n",
    "def smooth_predictions(predictions):\n",
    "    if len(predictions) > 0:\n",
    "        return np.mean(predictions, axis=0)\n",
    "    return np.zeros(num_handsigns)\n",
    "\n",
    "# Main loop for video feed and prediction\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    #frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Process frame to extract landmarks and results\n",
    "    landmarks = process_frame(frame, hands, pose)\n",
    "    hands_results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    pose_results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Append the landmarks to the sliding window\n",
    "    sliding_window.append(landmarks)\n",
    "    \n",
    "    # Check if the sliding window is full\n",
    "    if len(sliding_window) == sliding_window_size:\n",
    "        # Prepare input for the model (reshape and normalize)\n",
    "        input_data = np.array(sliding_window).reshape(1, sliding_window_size, num_landmarks, num_coordinates)\n",
    "        \n",
    "        # Predict the hand sign\n",
    "        predictions = model.predict(input_data)\n",
    "        prediction = np.argmax(predictions)\n",
    "        confidence = np.max(predictions)\n",
    "        \n",
    "        # Add the prediction to the smoothing buffer\n",
    "        smoothing_buffer.append(predictions)\n",
    "        smoothed_predictions = smooth_predictions(smoothing_buffer)\n",
    "        smoothed_prediction = np.argmax(smoothed_predictions)\n",
    "        smoothed_confidence = np.max(smoothed_predictions)\n",
    "\n",
    "        # Display prediction on screen if enabled\n",
    "        if show_prediction_text:\n",
    "            text = f\"Sign: {handsign_names[smoothed_prediction]}, Accuracy: {smoothed_confidence:.2f}\"\n",
    "            if smoothed_confidence < accuracy_threshold:\n",
    "                text = f\"Low accuracy ({handsign_names[smoothed_prediction]})\"\n",
    "            cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    # Draw landmarks if enabled\n",
    "    if show_landmarks and hands_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "            if show_connections:\n",
    "                mp.solutions.drawing_utils.draw_landmarks(\n",
    "                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            else:\n",
    "                mp.solutions.drawing_utils.draw_landmarks(frame, hand_landmarks)\n",
    "            mp.solutions.drawing_utils.draw_landmarks(frame, pose_results.pose_landmarks)\n",
    "            \n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Hand Sign Recognition', frame)\n",
    "\n",
    "    # Break the loop with 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ],
   "id": "41e6691dae6fe336",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 462ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 0s/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 0s/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 0s/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 28ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 0s/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 0s/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 0s/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T23:44:51.653258Z",
     "start_time": "2024-11-13T23:42:20.606396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##         TFLITE V2 CONTINUOUS PREDICTION WITH SLIDING WINDOW           ##\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow.lite as tflite  # Or use `from tensorflow.lite import Interpreter` if using full TensorFlow\n",
    "\n",
    "# Parameters for video feed and predictions\n",
    "show_landmarks = False\n",
    "show_connections = True\n",
    "show_prediction_text = True\n",
    "accuracy_threshold = 0.80\n",
    "sliding_window_size = frames_per_video\n",
    "smoothing_buffer_size = 5\n",
    "mediapipe_confidence = 0.2\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tflite.Interpreter(model_path='model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Initialize MediaPipe hands and pose solutions\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "hands = mp_hands.Hands(min_detection_confidence=mediapipe_confidence, min_tracking_confidence=mediapipe_confidence)\n",
    "pose = mp_pose.Pose(min_detection_confidence=mediapipe_confidence, min_tracking_confidence=mediapipe_confidence)\n",
    "\n",
    "# Set up sliding window and smoothing buffer\n",
    "sliding_window = deque(maxlen=sliding_window_size)\n",
    "smoothing_buffer = deque(maxlen=smoothing_buffer_size)\n",
    "\n",
    "# Initialize video capture (webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Function to smooth predictions\n",
    "def smooth_predictions(predictions):\n",
    "    if len(predictions) > 0:\n",
    "        return np.mean(predictions, axis=0)\n",
    "    return np.zeros(num_handsigns)\n",
    "\n",
    "# Main loop for video feed and prediction\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Process frame to extract landmarks and results\n",
    "    landmarks = process_frame(frame, hands, pose)\n",
    "    hands_results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    pose_results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Append the landmarks to the sliding window\n",
    "    sliding_window.append(landmarks)\n",
    "    \n",
    "    # Check if the sliding window is full\n",
    "    if len(sliding_window) == sliding_window_size:\n",
    "        # Prepare input for the model (reshape and normalize)\n",
    "        input_data = np.array(sliding_window).reshape(1, sliding_window_size, num_landmarks, num_coordinates).astype(np.float32)\n",
    "        \n",
    "        # Set input tensor\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        \n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "        prediction = np.argmax(predictions)\n",
    "        confidence = np.max(predictions)\n",
    "        \n",
    "        # Add the prediction to the smoothing buffer\n",
    "        smoothing_buffer.append(predictions)\n",
    "        smoothed_predictions = smooth_predictions(smoothing_buffer)\n",
    "        smoothed_prediction = np.argmax(smoothed_predictions)\n",
    "        smoothed_confidence = np.max(smoothed_predictions)\n",
    "\n",
    "        # Display prediction on screen if enabled\n",
    "        if show_prediction_text:\n",
    "            text = f\"Sign: {handsign_names[smoothed_prediction]}, Accuracy: {smoothed_confidence:.2f}\"\n",
    "            if smoothed_confidence < accuracy_threshold:\n",
    "                text = f\"Low accuracy ({handsign_names[smoothed_prediction]})\"\n",
    "            cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    # Draw landmarks if enabled\n",
    "    if show_landmarks and hands_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "            if show_connections:\n",
    "                mp.solutions.drawing_utils.draw_landmarks(\n",
    "                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            else:\n",
    "                mp.solutions.drawing_utils.draw_landmarks(frame, hand_landmarks)\n",
    "            mp.solutions.drawing_utils.draw_landmarks(frame, pose_results.pose_landmarks)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Hand Sign Recognition', frame)\n",
    "\n",
    "    # Break the loop with 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "108b38437fb1d388",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T23:35:36.155623Z",
     "start_time": "2024-11-13T23:35:34.631038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##          MODEL METRICS           ##\n",
    "from sklearn.metrics import classification_report, confusion_matrix, top_k_accuracy_score, roc_auc_score, cohen_kappa_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Ensure labels are correctly ordered for the report and confusion matrix\n",
    "hand_sign_labels = [handsign_names[i] for i in sorted(handsign_names.keys())]\n",
    "\n",
    "# Modify the analyze_training_history function to use hand_sign_labels\n",
    "def analyze_training_history(history, model, X_val, y_val, print_graphs=True):\n",
    "    \n",
    "    # Print the final training and validation accuracy\n",
    "    final_train_acc = history['accuracy'][-1]\n",
    "    final_val_acc = history['val_accuracy'][-1]\n",
    "    print(f\"Final Training Accuracy: {final_train_acc:.2f}\")\n",
    "    print(f\"Final Validation Accuracy: {final_val_acc:.2f}\")\n",
    "    \n",
    "    # Evaluate model performance on validation data\n",
    "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Generate predictions and calculate metrics for validation set\n",
    "    y_pred = np.argmax(model.predict(X_val), axis=1)\n",
    "    \n",
    "    # Print Classification Report with handsign names\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_pred, target_names=hand_sign_labels, digits=4))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Compute Top-k Accuracy\n",
    "    k = 3\n",
    "    top_k_accuracy = top_k_accuracy_score(y_val, model.predict(X_val), k=k)\n",
    "    print(f\"Top-{k} Accuracy: {top_k_accuracy:.4f}\")\n",
    "\n",
    "    # Per-class ROC-AUC\n",
    "    y_val_one_hot = tf.keras.utils.to_categorical(y_val, num_classes=len(handsign_names))\n",
    "    y_pred_proba = model.predict(X_val)\n",
    "    roc_auc_scores = roc_auc_score(y_val_one_hot, y_pred_proba, multi_class='ovr')\n",
    "    print(f\"Per-Class ROC-AUC Score: {roc_auc_scores:.4f}\")\n",
    "    \n",
    "    # Cohen's Kappa Score\n",
    "    y_pred_labels = np.argmax(y_pred_proba, axis=1)\n",
    "    kappa_score = cohen_kappa_score(y_val, y_pred_labels)\n",
    "    print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n",
    "\n",
    " # Display the Confusion Matrix graphically with better readability if print_graphs is True\n",
    "    if print_graphs:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))  # Increase figure size\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=hand_sign_labels)\n",
    "        disp.plot(cmap='Blues', ax=ax, xticks_rotation=45)  # Rotate x-axis labels for readability\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "        \n",
    "    # Plot training history if print_graphs is True\n",
    "    if print_graphs:\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Load the training history and analyze it\n",
    "def load_training_history(filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        history = pickle.load(file)\n",
    "    return history\n",
    "\n",
    "history = load_training_history('training_history.pkl')\n",
    "analyze_training_history(history, model, X_val, y_val, print_graphs=True)\n"
   ],
   "id": "1d1f133a1d03b9e5",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 92\u001B[0m\n\u001B[0;32m     89\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m history\n\u001B[0;32m     91\u001B[0m history \u001B[38;5;241m=\u001B[39m load_training_history(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining_history.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 92\u001B[0m analyze_training_history(history, model, \u001B[43mX_val\u001B[49m, y_val, print_graphs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_val' is not defined"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3b84045d2a4584c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
